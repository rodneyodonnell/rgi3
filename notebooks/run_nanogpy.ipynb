{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# nanoGPT example notebook\n",
    "\n",
    "Notebook for running (a fork of) Kaparthy's nanoGPT data-prep, training and sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.nanogpt import wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "wrapper.run_nanogpt_script(\"prepare_shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_train_args = [\n",
    "    \"--max_iters=5000\",\n",
    "    \"--eval_interval=50\",\n",
    "    \"--eval_iters=10\",\n",
    "    \"--log_interval=10\",\n",
    "    \"--batch_size=16\",\n",
    "    \"--block_size=128\",\n",
    "]\n",
    "\n",
    "# Train super-tiny 'femto' GPT model.\n",
    "femto_train_args = baby_train_args + [\n",
    "    # femto GPT model, much smaller than baby model.\n",
    "    \"--n_layer=4\",\n",
    "    \"--n_head=4\",\n",
    "    \"--n_embd=128\",\n",
    "    \"--dropout=0.2\",\n",
    "    \"--warmup_iters=10\",\n",
    "    # Disable compilation and reduce iters to speed up training.\n",
    "    \"--max_iters=100\",\n",
    "    \"--compile=False\",\n",
    "]\n",
    "\n",
    "\n",
    "train_argv = femto_train_args + [\"--max_iters=100\"]\n",
    "# train_argv = baby_train_args + [\"--max_iters=5000\"]\n",
    "\n",
    "train = wrapper.train(config_file=\"train_shakespeare_char.py\", argv=train_argv)\n",
    "\n",
    "## === Femto model benchmark===\n",
    "\n",
    "## 0.9s, femto_train_args + [], femto\n",
    "# step 100: train loss 2.5547, val loss 2.5611\n",
    "# iter 100: loss 2.5668, time 66.49ms, mfu 0.43%\n",
    "\n",
    "## 1.7s, femto_train_args + [\"--compile=True\"], ...  (first run is much slower?)\n",
    "# step 100: train loss 2.5453, val loss 2.5401\n",
    "# iter 100: loss 2.5679, time 64.51ms, mfu 0.81%\n",
    "\n",
    "## 6.3s, femto_train_args + [\"--max_iters=500\"]\n",
    "# step 500: train loss 2.3504, val loss 2.3744\n",
    "# iter 500: loss 2.3742, time 75.51ms, mfu 0.36%\n",
    "\n",
    "## 61.9s, femto_train_args + [\"--max_iters=500\"]\n",
    "# step 5000: train loss 1.6499, val loss 1.8055\n",
    "# iter 5000: loss 1.7146, time 66.23ms, mfu 0.23%\n",
    "\n",
    "## === Baby model benchmark ===\n",
    "\n",
    "## 7.0s, baby_train_args + [\"--max_iters=100\"]\n",
    "# step 100: train loss 2.5385, val loss 2.4948\n",
    "# iter 100: loss 2.5591, time 638.59ms, mfu 0.89%\n",
    "\n",
    "## 27.20s, baby_train_args + [\"--max_iters=500\"]\n",
    "# step 500: train loss 1.9833, val loss 2.0695\n",
    "# iter 500: loss 1.9927, time 629.50ms, mfu 0.81%\n",
    "\n",
    "## 4m.11s, baby_train_args + [\"--max_iters=5000\"]\n",
    "# step 5000: train loss 1.2840, val loss 1.5177\n",
    "# iter 5000: loss 1.3613, time 393.11ms, mfu 0.81%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "### FemtoGPT sample output (training_steps=5000)\n",
    "```\n",
    "Menry, my much sofer, I have no when her the coundes,\n",
    "And take all not you briegn of them sweart,\n",
    "A so tendersing this son hight Warwas which him be\n",
    "I ward be be of him.\n",
    "\n",
    "First Come, and the any in this blament their and sire,\n",
    "Awarn it there cervowin in that stice for Hare\n",
    "Than the her ople the of the word as deest and Maurth?\n",
    "\n",
    "Sechman:\n",
    "Rech not is stay aboles you that the not cherfs beguts\n",
    "To can news from that love boe then was we it.\n",
    "\n",
    "RICHARD III:\n",
    "The you the pity inson eyet,\n",
    "But will lark th\n",
    "```\n",
    "\n",
    "### BabyGPT sample output (training_steps=5000)\n",
    "```\n",
    "MENENIUS:\n",
    "He is the contentent.\n",
    "\n",
    "CORIOLANUS:\n",
    "And, my lord.\n",
    "\n",
    "ANGELO:\n",
    "Alas! I will not be evern'd; that of death\n",
    "Shall be the wretch'd abroad as matchery.\n",
    "\n",
    "CORIOLANUS:\n",
    "What see me?\n",
    "Perily, sir, when sir? What of them some treason\n",
    "To die and a kingdom the cousin of none this servant?\n",
    "\n",
    "Second Gentleman:\n",
    "And with them the very bitter city.\n",
    "\n",
    "COMINIUS:\n",
    "One is consent promised to thee honour'd.\n",
    "\n",
    "EXETER:\n",
    "His brother; I pray you, good marriage?\n",
    "\n",
    "GLOUCESTER:\n",
    "So may him but thee, which I say weep not; it i\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.sample_data(config_file=\"train_shakespeare_char.py\", argv=train_argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
