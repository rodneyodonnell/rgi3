{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_fields: {'n_layer', 'dropout', 'n_embd', 'n_head', 'n_max_context', 'bias'}\n",
      "train_config_fields: {'model_name', 'beta2', 'log_interval', 'eval_interval', 'eval_iters', 'compile', 'grad_clip', 'eval_only', 'learning_rate', 'decay_lr', 'wandb_log', 'weight_decay', 'dtype', 'warmup_iters', 'max_iters', 'device', 'batch_size', 'beta1', 'lr_decay_iters', 'always_save_checkpoint', 'max_epochs', 'min_lr', 'model_version', 'gradient_accumulation_steps'}\n",
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.experiment import ExperimentRunner, ExperimentConfig\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab, print_dataset_stats, TrajectoryDataset\n",
    "from rgi.rgizero.evaluators import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "from rgi.rgizero.models.tuner import create_random_model\n",
    "\n",
    "import notebook_utils\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "device = notebook_utils.detect_device()\n",
    "\n",
    "## Disable for debugger stability?\n",
    "# # Allow asyncio to work with jupyter notebook\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GENERATIONS = True\n",
    "\n",
    "\n",
    "# Create Experiment Config\n",
    "experiment_config = ExperimentConfig(\n",
    "    experiment_name='smoketest-e2e-v2',\n",
    "    parent_experiment_name='smoketest-e2e',\n",
    "    game_name='connect4',\n",
    "    num_generations=20,\n",
    "    num_games_per_gen=10_000,\n",
    "    num_simulations=50,\n",
    "    model_size=\"tiny\",\n",
    "    train_batch_size=10,\n",
    "    max_training_epochs=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Tuned params from connect4 with 23k training games.\n",
    "tuned_params = {\n",
    "    'batch_size': 256,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'bias': True,\n",
    "    'decay_lr': True,\n",
    "    'dropout': 0.0,\n",
    "    'dtype': 'float16',\n",
    "    'grad_clip': 1.0,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 0.002,\n",
    "    'lr_decay_iters': 1000,\n",
    "    'max_epochs': 1000000,\n",
    "    'max_iters': 1000,\n",
    "    'min_lr': 0.0002,\n",
    "    'n_embd': 256,\n",
    "    'n_head': 2,\n",
    "    'n_layer': 3,\n",
    "    'n_max_context': 44,\n",
    "    'warmup_iters': 100,  # TODO: Tuner says this should be 500? That seems high...\n",
    "    'weight_decay': 0.1,\n",
    "    # 'model_name': 'c4-smoketest',\n",
    "    # 'model_version': '0.1',\n",
    "    # 'num_players': 2,\n",
    "    # 'vocab_size': 8,\n",
    "    # 'dataset_paths': (\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5')\n",
    "    #     ),\n",
    "    'eval_iters': 200,\n",
    "    'log_interval': 1000,\n",
    "    'eval_interval': 10000,\n",
    "    # 'device': 'mps',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up game and experiment runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Runner initialized\n",
      "Game: connect4, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Data dir:  /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data\n",
      "Model dir:  /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "# Initialize Experiment Runner\n",
    "experiment_base_dir = Path.cwd().parent / 'experiments'\n",
    "experiment_runner = ExperimentRunner(experiment_config, experiment_base_dir, training_args=tuned_params)\n",
    "game = experiment_runner.game\n",
    "action_vocab = experiment_runner.action_vocab\n",
    "n_max_context = experiment_runner.n_max_context\n",
    "\n",
    "DATA_DIR = experiment_runner.data_dir\n",
    "MODEL_DIR = experiment_runner.models_dir\n",
    "\n",
    "print('✅ Runner initialized')\n",
    "print(f'Game: {experiment_runner.config.game_name}, Players: {experiment_runner.num_players}, Actions: {list(game.base_game.all_actions())}')\n",
    "print('Data dir: ', DATA_DIR)\n",
    "print('Model dir: ', MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment: smoketest-e2e-v2\n",
      "Loading existing Gen 0 model.\n"
     ]
    }
   ],
   "source": [
    "# Initialize (creates Random Gen 0 if needed)\n",
    "model_0 = experiment_runner.initialize()\n",
    "current_model = model_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Dataset for gen 1 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1. Skipping play.\n",
      "Using forked model for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-1.pt\n",
      "Model for gen 1 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-1.pt. Loading.\n",
      "Using forked model for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-1.pt\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 1000\n",
      "  Total actions: 14552\n",
      "  Avg trajectory length: 14.55\n",
      "Prefix Stats:\n",
      "actions=(): 1000 win=618 loss=382 draw=0 win1%=61.80 model-win1%=46.08\n",
      "actions=(1,): 157 win=80 loss=77 draw=0 win1%=50.96 model-win1%=59.98\n",
      "actions=(2,): 124 win=79 loss=45 draw=0 win1%=63.71 model-win1%=58.56\n",
      "actions=(3,): 113 win=71 loss=42 draw=0 win1%=62.83 model-win1%=57.37\n",
      "actions=(4,): 136 win=104 loss=32 draw=0 win1%=76.47 model-win1%=58.82\n",
      "actions=(5,): 178 win=117 loss=61 draw=0 win1%=65.73 model-win1%=59.77\n",
      "actions=(6,): 134 win=80 loss=54 draw=0 win1%=59.70 model-win1%=59.79\n",
      "actions=(7,): 158 win=87 loss=71 draw=0 win1%=55.06 model-win1%=59.88\n",
      "\n",
      "=== Generation 2 ===\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Dataset for gen 2 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2. Skipping play.\n",
      "Using forked model for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-2.pt\n",
      "Model for gen 2 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-2.pt. Loading.\n",
      "Using forked model for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-2.pt\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 28575\n",
      "  Avg trajectory length: 14.29\n",
      "Prefix Stats:\n",
      "actions=(): 2000 win=1245 loss=755 draw=0 win1%=62.25 model-win1%=41.47\n",
      "actions=(1,): 314 win=167 loss=147 draw=0 win1%=53.18 model-win1%=62.91\n",
      "actions=(2,): 264 win=161 loss=103 draw=0 win1%=60.98 model-win1%=62.54\n",
      "actions=(3,): 235 win=160 loss=75 draw=0 win1%=68.09 model-win1%=61.35\n",
      "actions=(4,): 266 win=201 loss=65 draw=0 win1%=75.56 model-win1%=62.52\n",
      "actions=(5,): 331 win=223 loss=108 draw=0 win1%=67.37 model-win1%=63.84\n",
      "actions=(6,): 289 win=165 loss=124 draw=0 win1%=57.09 model-win1%=63.89\n",
      "actions=(7,): 301 win=168 loss=133 draw=0 win1%=55.81 model-win1%=63.32\n",
      "\n",
      "=== Generation 3 ===\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset for gen 3 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3. Skipping play.\n",
      "Using forked model for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-3.pt\n",
      "Model for gen 3 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-3.pt. Loading.\n",
      "Using forked model for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/models/gen-3.pt\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 3000\n",
      "  Total actions: 43386\n",
      "  Avg trajectory length: 14.46\n",
      "Prefix Stats:\n",
      "actions=(): 3000 win=1840 loss=1160 draw=0 win1%=61.33 model-win1%=44.55\n",
      "actions=(1,): 471 win=254 loss=217 draw=0 win1%=53.93 model-win1%=57.67\n",
      "actions=(2,): 375 win=219 loss=156 draw=0 win1%=58.40 model-win1%=56.50\n",
      "actions=(3,): 345 win=224 loss=121 draw=0 win1%=64.93 model-win1%=56.74\n",
      "actions=(4,): 411 win=307 loss=104 draw=0 win1%=74.70 model-win1%=58.14\n",
      "actions=(5,): 519 win=347 loss=172 draw=0 win1%=66.86 model-win1%=58.52\n",
      "actions=(6,): 449 win=254 loss=195 draw=0 win1%=56.57 model-win1%=58.00\n",
      "actions=(7,): 430 win=235 loss=195 draw=0 win1%=54.65 model-win1%=57.32\n",
      "\n",
      "=== Generation 4 ===\n",
      "Dataset for gen 4 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4. Skipping play.\n",
      "Model for gen 4 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-4.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 13000\n",
      "  Total actions: 197884\n",
      "  Avg trajectory length: 15.22\n",
      "Prefix Stats:\n",
      "actions=(): 13000 win=7734 loss=5263 draw=3 win1%=59.49 model-win1%=54.18\n",
      "actions=(1,): 1965 win=1009 loss=954 draw=2 win1%=51.35 model-win1%=56.78\n",
      "actions=(2,): 1585 win=875 loss=710 draw=0 win1%=55.21 model-win1%=55.52\n",
      "actions=(3,): 1551 win=968 loss=583 draw=0 win1%=62.41 model-win1%=56.89\n",
      "actions=(4,): 1785 win=1328 loss=457 draw=0 win1%=74.40 model-win1%=57.89\n",
      "actions=(5,): 2354 win=1520 loss=833 draw=1 win1%=64.57 model-win1%=58.64\n",
      "actions=(6,): 1983 win=1104 loss=879 draw=0 win1%=55.67 model-win1%=57.33\n",
      "actions=(7,): 1777 win=930 loss=847 draw=0 win1%=52.34 model-win1%=57.53\n",
      "\n",
      "=== Generation 5 ===\n",
      "Dataset for gen 5 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5. Skipping play.\n",
      "Model for gen 5 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-5.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 23000\n",
      "  Total actions: 351053\n",
      "  Avg trajectory length: 15.26\n",
      "Prefix Stats:\n",
      "actions=(): 23000 win=14023 loss=8971 draw=6 win1%=60.97 model-win1%=51.98\n",
      "actions=(1,): 3384 win=1795 loss=1587 draw=2 win1%=53.04 model-win1%=58.20\n",
      "actions=(2,): 2408 win=1369 loss=1038 draw=1 win1%=56.85 model-win1%=57.22\n",
      "actions=(3,): 2829 win=1828 loss=1001 draw=0 win1%=64.62 model-win1%=60.18\n",
      "actions=(4,): 3498 win=2569 loss=928 draw=1 win1%=73.44 model-win1%=59.86\n",
      "actions=(5,): 4276 win=2791 loss=1484 draw=1 win1%=65.27 model-win1%=60.25\n",
      "actions=(6,): 3478 win=2019 loss=1459 draw=0 win1%=58.05 model-win1%=59.35\n",
      "actions=(7,): 3127 win=1652 loss=1474 draw=1 win1%=52.83 model-win1%=60.67\n",
      "\n",
      "=== Generation 6 ===\n",
      "Dataset for gen 6 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6. Skipping play.\n",
      "Model for gen 6 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-6.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 33000\n",
      "  Total actions: 502313\n",
      "  Avg trajectory length: 15.22\n",
      "Prefix Stats:\n",
      "actions=(): 33000 win=20377 loss=12615 draw=8 win1%=61.75 model-win1%=50.11\n",
      "actions=(1,): 4882 win=2638 loss=2241 draw=3 win1%=54.04 model-win1%=59.18\n",
      "actions=(2,): 3192 win=1835 loss=1356 draw=1 win1%=57.49 model-win1%=57.50\n",
      "actions=(3,): 4191 win=2716 loss=1475 draw=0 win1%=64.81 model-win1%=60.28\n",
      "actions=(4,): 5496 win=4049 loss=1446 draw=1 win1%=73.67 model-win1%=59.57\n",
      "actions=(5,): 5868 win=3829 loss=2038 draw=1 win1%=65.25 model-win1%=60.52\n",
      "actions=(6,): 4869 win=2851 loss=2018 draw=0 win1%=58.55 model-win1%=59.71\n",
      "actions=(7,): 4502 win=2459 loss=2041 draw=2 win1%=54.62 model-win1%=61.79\n",
      "\n",
      "=== Generation 7 ===\n",
      "Dataset for gen 7 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7. Skipping play.\n",
      "Model for gen 7 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-7.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 43000\n",
      "  Total actions: 652488\n",
      "  Avg trajectory length: 15.17\n",
      "Prefix Stats:\n",
      "actions=(): 43000 win=26550 loss=16441 draw=9 win1%=61.74 model-win1%=49.48\n",
      "actions=(1,): 6267 win=3398 loss=2866 draw=3 win1%=54.22 model-win1%=60.19\n",
      "actions=(2,): 3900 win=2253 loss=1646 draw=1 win1%=57.77 model-win1%=57.20\n",
      "actions=(3,): 5452 win=3554 loss=1897 draw=1 win1%=65.19 model-win1%=59.71\n",
      "actions=(4,): 7540 win=5516 loss=2023 draw=1 win1%=73.16 model-win1%=59.58\n",
      "actions=(5,): 7518 win=4863 loss=2654 draw=1 win1%=64.68 model-win1%=60.17\n",
      "actions=(6,): 6298 win=3684 loss=2614 draw=0 win1%=58.49 model-win1%=60.72\n",
      "actions=(7,): 6025 win=3282 loss=2741 draw=2 win1%=54.47 model-win1%=62.77\n",
      "\n",
      "=== Generation 8 ===\n",
      "Dataset for gen 8 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8. Skipping play.\n",
      "Model for gen 8 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-8.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 53000\n",
      "  Total actions: 801009\n",
      "  Avg trajectory length: 15.11\n",
      "Prefix Stats:\n",
      "actions=(): 53000 win=32775 loss=20216 draw=9 win1%=61.84 model-win1%=49.83\n",
      "actions=(1,): 7644 win=4168 loss=3473 draw=3 win1%=54.53 model-win1%=60.44\n",
      "actions=(2,): 4508 win=2588 loss=1919 draw=1 win1%=57.41 model-win1%=55.76\n",
      "actions=(3,): 6686 win=4362 loss=2323 draw=1 win1%=65.24 model-win1%=57.13\n",
      "actions=(4,): 9757 win=7139 loss=2617 draw=1 win1%=73.17 model-win1%=57.83\n",
      "actions=(5,): 9048 win=5821 loss=3226 draw=1 win1%=64.33 model-win1%=59.31\n",
      "actions=(6,): 7667 win=4475 loss=3192 draw=0 win1%=58.37 model-win1%=60.83\n",
      "actions=(7,): 7690 win=4222 loss=3466 draw=2 win1%=54.90 model-win1%=61.84\n",
      "\n",
      "=== Generation 9 ===\n",
      "Dataset for gen 9 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9. Skipping play.\n",
      "Model for gen 9 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-9.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 63000\n",
      "  Total actions: 952839\n",
      "  Avg trajectory length: 15.12\n",
      "Prefix Stats:\n",
      "actions=(): 63000 win=38773 loss=24218 draw=9 win1%=61.54 model-win1%=51.35\n",
      "actions=(1,): 9272 win=5004 loss=4265 draw=3 win1%=53.97 model-win1%=61.64\n",
      "actions=(2,): 5014 win=2859 loss=2154 draw=1 win1%=57.02 model-win1%=54.63\n",
      "actions=(3,): 7825 win=5077 loss=2747 draw=1 win1%=64.88 model-win1%=56.02\n",
      "actions=(4,): 11669 win=8540 loss=3128 draw=1 win1%=73.19 model-win1%=56.64\n",
      "actions=(5,): 10574 win=6766 loss=3807 draw=1 win1%=63.99 model-win1%=58.76\n",
      "actions=(6,): 9079 win=5286 loss=3793 draw=0 win1%=58.22 model-win1%=59.78\n",
      "actions=(7,): 9567 win=5241 loss=4324 draw=2 win1%=54.78 model-win1%=62.66\n",
      "\n",
      "=== Generation 10 ===\n",
      "Dataset for gen 10 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10. Skipping play.\n",
      "Model for gen 10 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-10.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 73000\n",
      "  Total actions: 1111932\n",
      "  Avg trajectory length: 15.23\n",
      "Prefix Stats:\n",
      "actions=(): 73000 win=44896 loss=28091 draw=13 win1%=61.50 model-win1%=52.98\n",
      "actions=(1,): 12297 win=6756 loss=5537 draw=4 win1%=54.94 model-win1%=64.38\n",
      "actions=(2,): 5493 win=3132 loss=2359 draw=2 win1%=57.02 model-win1%=56.76\n",
      "actions=(3,): 8819 win=5688 loss=3129 draw=2 win1%=64.50 model-win1%=58.84\n",
      "actions=(4,): 12990 win=9537 loss=3452 draw=1 win1%=73.42 model-win1%=55.77\n",
      "actions=(5,): 11746 win=7499 loss=4245 draw=2 win1%=63.84 model-win1%=60.06\n",
      "actions=(6,): 10229 win=5954 loss=4275 draw=0 win1%=58.21 model-win1%=62.39\n",
      "actions=(7,): 11426 win=6330 loss=5094 draw=2 win1%=55.40 model-win1%=66.86\n",
      "\n",
      "=== Generation 11 ===\n",
      "Dataset for gen 11 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11. Skipping play.\n",
      "Model for gen 11 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-11.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 83000\n",
      "  Total actions: 1274553\n",
      "  Avg trajectory length: 15.36\n",
      "Prefix Stats:\n",
      "actions=(): 83000 win=50831 loss=32155 draw=14 win1%=61.24 model-win1%=53.76\n",
      "actions=(1,): 15490 win=8540 loss=6945 draw=5 win1%=55.13 model-win1%=67.42\n",
      "actions=(2,): 5806 win=3312 loss=2492 draw=2 win1%=57.04 model-win1%=58.64\n",
      "actions=(3,): 9623 win=6172 loss=3449 draw=2 win1%=64.14 model-win1%=60.94\n",
      "actions=(4,): 14008 win=10292 loss=3715 draw=1 win1%=73.47 model-win1%=55.17\n",
      "actions=(5,): 12515 win=7967 loss=4546 draw=2 win1%=63.66 model-win1%=61.56\n",
      "actions=(6,): 11159 win=6511 loss=4648 draw=0 win1%=58.35 model-win1%=65.27\n",
      "actions=(7,): 14399 win=8037 loss=6360 draw=2 win1%=55.82 model-win1%=69.71\n",
      "\n",
      "=== Generation 12 ===\n",
      "Dataset for gen 12 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12. Skipping play.\n",
      "Model for gen 12 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-12.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 93000\n",
      "  Total actions: 1434775\n",
      "  Avg trajectory length: 15.43\n",
      "Prefix Stats:\n",
      "actions=(): 93000 win=56479 loss=36503 draw=18 win1%=60.73 model-win1%=53.86\n",
      "actions=(1,): 18846 win=10339 loss=8502 draw=5 win1%=54.86 model-win1%=67.96\n",
      "actions=(1, 1): 4888 win=2780 loss=2108 draw=0 win1%=56.87 model-win1%=61.57\n",
      "actions=(2,): 6111 win=3478 loss=2631 draw=2 win1%=56.91 model-win1%=59.14\n",
      "actions=(3,): 10381 win=6619 loss=3759 draw=3 win1%=63.76 model-win1%=60.97\n",
      "actions=(4,): 14769 win=10841 loss=3927 draw=1 win1%=73.40 model-win1%=49.40\n",
      "actions=(5,): 13165 win=8337 loss=4825 draw=3 win1%=63.33 model-win1%=60.37\n",
      "actions=(6,): 12020 win=7004 loss=5015 draw=1 win1%=58.27 model-win1%=65.56\n",
      "actions=(7,): 17708 win=9861 loss=7844 draw=3 win1%=55.69 model-win1%=69.30\n",
      "\n",
      "=== Generation 13 ===\n",
      "Dataset for gen 13 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13. Skipping play.\n",
      "Model for gen 13 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-13.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 103000\n",
      "  Total actions: 1599621\n",
      "  Avg trajectory length: 15.53\n",
      "Prefix Stats:\n",
      "actions=(): 103000 win=62471 loss=40508 draw=21 win1%=60.65 model-win1%=55.10\n",
      "actions=(1,): 23060 win=12771 loss=10282 draw=7 win1%=55.38 model-win1%=68.29\n",
      "actions=(1, 1): 6118 win=3472 loss=2645 draw=1 win1%=56.75 model-win1%=62.09\n",
      "actions=(2,): 6452 win=3682 loss=2767 draw=3 win1%=57.07 model-win1%=58.16\n",
      "actions=(3,): 11170 win=7118 loss=4049 draw=3 win1%=63.72 model-win1%=60.67\n",
      "actions=(4,): 15464 win=11357 loss=4106 draw=1 win1%=73.44 model-win1%=50.76\n",
      "actions=(5,): 13870 win=8761 loss=5106 draw=3 win1%=63.17 model-win1%=59.66\n",
      "actions=(6,): 12896 win=7517 loss=5378 draw=1 win1%=58.29 model-win1%=65.46\n",
      "actions=(7,): 20088 win=11265 loss=8820 draw=3 win1%=56.08 model-win1%=69.96\n",
      "\n",
      "=== Generation 14 ===\n",
      "Dataset for gen 14 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14. Skipping play.\n",
      "Model for gen 14 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-14.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 113000\n",
      "  Total actions: 1763570\n",
      "  Avg trajectory length: 15.61\n",
      "Prefix Stats:\n",
      "actions=(): 113000 win=68164 loss=44813 draw=23 win1%=60.32 model-win1%=53.87\n",
      "actions=(1,): 27620 win=15313 loss=12298 draw=9 win1%=55.44 model-win1%=69.21\n",
      "actions=(1, 1): 7395 win=4181 loss=3212 draw=2 win1%=56.54 model-win1%=61.99\n",
      "actions=(2,): 6709 win=3843 loss=2863 draw=3 win1%=57.28 model-win1%=60.25\n",
      "actions=(3,): 11856 win=7514 loss=4339 draw=3 win1%=63.38 model-win1%=62.18\n",
      "actions=(4,): 16129 win=11837 loss=4291 draw=1 win1%=73.39 model-win1%=48.43\n",
      "actions=(5,): 14570 win=9186 loss=5381 draw=3 win1%=63.05 model-win1%=58.95\n",
      "actions=(6,): 13569 win=7869 loss=5699 draw=1 win1%=57.99 model-win1%=66.27\n",
      "actions=(7,): 22547 win=12602 loss=9942 draw=3 win1%=55.89 model-win1%=72.40\n",
      "\n",
      "=== Generation 15 ===\n",
      "Dataset for gen 15 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15. Skipping play.\n",
      "Model for gen 15 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-15.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 123000\n",
      "  Total actions: 1931036\n",
      "  Avg trajectory length: 15.70\n",
      "Prefix Stats:\n",
      "actions=(): 123000 win=74118 loss=48854 draw=28 win1%=60.26 model-win1%=55.07\n",
      "actions=(1,): 31291 win=17447 loss=13834 draw=10 win1%=55.76 model-win1%=66.12\n",
      "actions=(1, 1): 8538 win=4837 loss=3698 draw=3 win1%=56.65 model-win1%=60.36\n",
      "actions=(2,): 6980 win=3996 loss=2980 draw=4 win1%=57.25 model-win1%=58.59\n",
      "actions=(3,): 12569 win=7965 loss=4600 draw=4 win1%=63.37 model-win1%=59.33\n",
      "actions=(4,): 16732 win=12279 loss=4452 draw=1 win1%=73.39 model-win1%=48.98\n",
      "actions=(5,): 15142 win=9521 loss=5618 draw=3 win1%=62.88 model-win1%=57.71\n",
      "actions=(6,): 14217 win=8240 loss=5976 draw=1 win1%=57.96 model-win1%=63.83\n",
      "actions=(7,): 26069 win=14670 loss=11394 draw=5 win1%=56.27 model-win1%=69.15\n",
      "actions=(7, 1): 6282 win=3812 loss=2470 draw=0 win1%=60.68 model-win1%=60.85\n",
      "\n",
      "=== Generation 16 ===\n",
      "Dataset for gen 16 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16. Skipping play.\n",
      "Model for gen 16 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-16.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 133000\n",
      "  Total actions: 2098143\n",
      "  Avg trajectory length: 15.78\n",
      "Prefix Stats:\n",
      "actions=(): 133000 win=80175 loss=52795 draw=30 win1%=60.28 model-win1%=54.68\n",
      "actions=(1,): 34636 win=19371 loss=15254 draw=11 win1%=55.93 model-win1%=65.94\n",
      "actions=(1, 1): 9623 win=5458 loss=4162 draw=3 win1%=56.72 model-win1%=58.93\n",
      "actions=(2,): 7311 win=4184 loss=3123 draw=4 win1%=57.23 model-win1%=57.01\n",
      "actions=(3,): 13365 win=8444 loss=4916 draw=5 win1%=63.18 model-win1%=55.87\n",
      "actions=(4,): 17447 win=12793 loss=4653 draw=1 win1%=73.32 model-win1%=41.75\n",
      "actions=(5,): 15741 win=9910 loss=5828 draw=3 win1%=62.96 model-win1%=54.17\n",
      "actions=(6,): 15009 win=8716 loss=6292 draw=1 win1%=58.07 model-win1%=63.08\n",
      "actions=(7,): 29491 win=16757 loss=12729 draw=5 win1%=56.82 model-win1%=70.79\n",
      "actions=(7, 1): 7184 win=4404 loss=2780 draw=0 win1%=61.30 model-win1%=60.48\n",
      "\n",
      "=== Generation 17 ===\n",
      "Dataset for gen 17 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17. Skipping play.\n",
      "Model for gen 17 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-17.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 143000\n",
      "  Total actions: 2264160\n",
      "  Avg trajectory length: 15.83\n",
      "Prefix Stats:\n",
      "actions=(): 143000 win=86380 loss=56588 draw=32 win1%=60.41 model-win1%=56.16\n",
      "actions=(1,): 37855 win=21325 loss=16518 draw=12 win1%=56.33 model-win1%=66.63\n",
      "actions=(1, 1): 10647 win=6081 loss=4563 draw=3 win1%=57.11 model-win1%=57.42\n",
      "actions=(2,): 7574 win=4345 loss=3225 draw=4 win1%=57.37 model-win1%=58.75\n",
      "actions=(3,): 13905 win=8775 loss=5125 draw=5 win1%=63.11 model-win1%=59.86\n",
      "actions=(4,): 17938 win=13165 loss=4772 draw=1 win1%=73.39 model-win1%=43.27\n",
      "actions=(5,): 16265 win=10243 loss=6019 draw=3 win1%=62.98 model-win1%=56.48\n",
      "actions=(6,): 15682 win=9100 loss=6581 draw=1 win1%=58.03 model-win1%=64.90\n",
      "actions=(7,): 33781 win=19427 loss=14348 draw=6 win1%=57.51 model-win1%=70.09\n",
      "actions=(7, 1): 8362 win=5206 loss=3155 draw=1 win1%=62.26 model-win1%=58.45\n",
      "\n",
      "=== Generation 18 ===\n",
      "Dataset for gen 18 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18. Skipping play.\n",
      "Model for gen 18 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-18.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 153000\n",
      "  Total actions: 2430492\n",
      "  Avg trajectory length: 15.89\n",
      "Prefix Stats:\n",
      "actions=(): 153000 win=92278 loss=60687 draw=35 win1%=60.31 model-win1%=55.61\n",
      "actions=(1,): 41152 win=23207 loss=17932 draw=13 win1%=56.39 model-win1%=66.82\n",
      "actions=(1, 1): 11595 win=6610 loss=4982 draw=3 win1%=57.01 model-win1%=59.85\n",
      "actions=(2,): 7937 win=4563 loss=3370 draw=4 win1%=57.49 model-win1%=59.18\n",
      "actions=(3,): 14673 win=9232 loss=5436 draw=5 win1%=62.92 model-win1%=60.11\n",
      "actions=(4,): 18511 win=13557 loss=4953 draw=1 win1%=73.24 model-win1%=44.94\n",
      "actions=(5,): 16982 win=10663 loss=6316 draw=3 win1%=62.79 model-win1%=57.47\n",
      "actions=(6,): 16529 win=9601 loss=6926 draw=2 win1%=58.09 model-win1%=65.86\n",
      "actions=(7,): 37216 win=21455 loss=15754 draw=7 win1%=57.65 model-win1%=71.07\n",
      "actions=(7, 1): 9241 win=5776 loss=3463 draw=2 win1%=62.50 model-win1%=61.88\n",
      "\n",
      "=== Generation 19 ===\n",
      "Dataset for gen 19 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19. Skipping play.\n",
      "Model for gen 19 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-19.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 163000\n",
      "  Total actions: 2593781\n",
      "  Avg trajectory length: 15.91\n",
      "Prefix Stats:\n",
      "actions=(): 163000 win=98313 loss=64652 draw=35 win1%=60.31 model-win1%=56.31\n",
      "actions=(1,): 44079 win=24935 loss=19131 draw=13 win1%=56.57 model-win1%=66.20\n",
      "actions=(1, 1): 12506 win=7143 loss=5360 draw=3 win1%=57.12 model-win1%=58.34\n",
      "actions=(2,): 8252 win=4741 loss=3507 draw=4 win1%=57.45 model-win1%=59.91\n",
      "actions=(3,): 15370 win=9653 loss=5712 draw=5 win1%=62.80 model-win1%=59.93\n",
      "actions=(4,): 19077 win=13952 loss=5124 draw=1 win1%=73.14 model-win1%=45.26\n",
      "actions=(5,): 17490 win=10981 loss=6506 draw=3 win1%=62.78 model-win1%=59.40\n",
      "actions=(6,): 17223 win=10001 loss=7220 draw=2 win1%=58.07 model-win1%=64.88\n",
      "actions=(7,): 41509 win=24050 loss=17452 draw=7 win1%=57.94 model-win1%=70.14\n",
      "actions=(7, 1): 10359 win=6502 loss=3855 draw=2 win1%=62.77 model-win1%=60.14\n",
      "\n",
      "=== Generation 20 ===\n",
      "Dataset for gen 20 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20. Skipping play.\n",
      "Model for gen 20 exists at /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/models/gen-20.pt. Loading.\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 173000\n",
      "  Total actions: 2754358\n",
      "  Avg trajectory length: 15.92\n",
      "Prefix Stats:\n",
      "actions=(): 173000 win=104006 loss=68957 draw=37 win1%=60.12 model-win1%=56.70\n",
      "actions=(1,): 46966 win=26530 loss=20423 draw=13 win1%=56.49 model-win1%=69.02\n",
      "actions=(1, 1): 13319 win=7577 loss=5739 draw=3 win1%=56.89 model-win1%=60.38\n",
      "actions=(2,): 8666 win=4963 loss=3698 draw=5 win1%=57.27 model-win1%=61.35\n",
      "actions=(3,): 16091 win=10062 loss=6024 draw=5 win1%=62.53 model-win1%=60.01\n",
      "actions=(4,): 19604 win=14349 loss=5254 draw=1 win1%=73.19 model-win1%=44.78\n",
      "actions=(5,): 18147 win=11351 loss=6793 draw=3 win1%=62.55 model-win1%=59.57\n",
      "actions=(6,): 17995 win=10412 loss=7581 draw=2 win1%=57.86 model-win1%=65.26\n",
      "actions=(7,): 45531 win=26339 loss=19184 draw=8 win1%=57.85 model-win1%=71.27\n",
      "actions=(7, 1): 11328 win=7096 loss=4230 draw=2 win1%=62.64 model-win1%=61.49\n",
      "actions=(7, 7): 8785 win=5147 loss=3637 draw=1 win1%=58.59 model-win1%=63.15\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, experiment_config.num_generations+1):\n",
    "        current_model = await experiment_runner.run_generation_step_async(generation_id, current_model)\n",
    "        dataset_paths = experiment_runner.get_trajectory_paths(generation_id)\n",
    "        \n",
    "        # print stats for visibility\n",
    "        print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=current_model, game=game)\n",
    "        \n",
    "        model_dict[generation_id] = current_model\n",
    "\n",
    "# 10m to play 2x10k generations... probabilities still very wrong.\n",
    "# Evaluation time: 0.015 seconds, size=574, eval-per-second=37837.60, total-batches=6000, mean-eval-per-second=94963.99, mean-time-per-batch=0.010, mean-batch-size=990.34\n",
    "\n",
    "# >>> log(2) + log(7) -> 2.6390573296152584\n",
    "## Model doesn't seem to improve loss at all?\n",
    "# step.   0: losses: train:2.5971, train_policy_loss:1.9146, train_value_loss:0.6825, val:2.5972, val_policy_loss:1.9147, val_value_loss:0.6825\n",
    "# step 1000: losses: train:2.6036, train_policy_loss:1.9122, train_value_loss:0.6914, val:2.6050, val_policy_loss:1.9132, val_value_loss:0.6917\n",
    "# step 2000: losses: train:2.6056, train_policy_loss:1.9119, train_value_loss:0.6937, val:2.6056, val_policy_loss:1.9123, val_value_loss:0.6933\n",
    "# iter    0/1170/5000: loss 2.5699, policy_loss:1.9129, value_loss:0.6570, time 5.18s, iter_time: 0.00ms\n",
    "# iter 1000/1170/5000: loss 2.5996, policy_loss:1.9068, value_loss:0.6928, time 1.96s, iter_time: 1957.74ms\n",
    "# iter 2339/2340/5000: loss 2.6014, policy_loss:1.9131, value_loss:0.6884, time 0.01s, iter_time: 14.61ms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model (initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_fields: {'n_layer', 'dropout', 'n_embd', 'n_head', 'n_max_context', 'bias'}\n",
      "train_config_fields: {'model_name', 'beta2', 'log_interval', 'eval_interval', 'eval_iters', 'compile', 'grad_clip', 'eval_only', 'learning_rate', 'decay_lr', 'wandb_log', 'weight_decay', 'dtype', 'warmup_iters', 'max_iters', 'device', 'batch_size', 'beta1', 'lr_decay_iters', 'always_save_checkpoint', 'max_epochs', 'min_lr', 'model_version', 'gradient_accumulation_steps'}\n",
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.5129219937324523 elapsed=6.371288776397705s, val_policy=1.8344, val_value=0.6786\n",
      "## Searching generation 0 with 17 candidates, including ['learning_rate: 0.01 -> 0.02', 'learning_rate: 0.01 -> 0.005', 'beta1: 0.9 -> 0.95', 'beta2: 0.99 -> 0.98', 'decay_lr: True -> False']\n",
      "## improved: False, loss=2.5114 elapsed=7.79s, mutation learning_rate: 0.01 -> 0.02\n",
      "## improved: False, loss=2.5170 elapsed=7.30s, mutation learning_rate: 0.01 -> 0.005\n",
      "## improved: False, loss=2.5140 elapsed=7.31s, mutation beta1: 0.9 -> 0.95\n",
      "## improved: False, loss=2.5127 elapsed=7.31s, mutation beta2: 0.99 -> 0.98\n",
      "## improved: False, loss=2.5144 elapsed=7.22s, mutation decay_lr: True -> False\n",
      "## improved: False, loss=2.5131 elapsed=7.51s, mutation dropout: 0.02 -> 0.01\n",
      "## improved: False, loss=2.5166 elapsed=6.76s, mutation batch_size: 64 -> 32\n",
      "## improved: False, loss=2.5206 elapsed=7.23s, mutation n_embd: 16 -> 8\n",
      "## improved: False, loss=2.5060 elapsed=9.29s, mutation max_iters: 100 -> 300\n",
      "## improved: False, loss=2.5130 elapsed=7.34s, mutation dropout: 0.02 -> 0.05\n",
      "## improved: False, loss=2.5125 elapsed=7.99s, mutation n_layer: 1 -> 2\n",
      "## improved: False, loss=2.5130 elapsed=6.37s, mutation weight_decay: 0.01 -> 0.05\n",
      "## improved: False, loss=2.5152 elapsed=7.48s, mutation n_embd: 16 -> 32\n",
      "## improved: False, loss=2.5126 elapsed=7.26s, mutation n_head: 1 -> 2\n",
      "## improved: False, loss=2.5115 elapsed=7.38s, mutation bias: False -> True\n",
      "## improved: False, loss=2.5084 elapsed=7.78s, mutation batch_size: 64 -> 128\n",
      "## improved: False, loss=2.5129 elapsed=7.26s, mutation dtype: bfloat16 -> float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 2.5129219937324523,\n",
       " 6.371288776397705,\n",
       " {'batch_size': 64,\n",
       "  'beta1': 0.9,\n",
       "  'beta2': 0.99,\n",
       "  'bias': False,\n",
       "  'decay_lr': True,\n",
       "  'dropout': 0.02,\n",
       "  'dtype': 'bfloat16',\n",
       "  'grad_clip': 1.0,\n",
       "  'gradient_accumulation_steps': 1,\n",
       "  'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20',\n",
       "  'learning_rate': 0.01,\n",
       "  'lr_decay_iters': 100,\n",
       "  'max_epochs': 1000000,\n",
       "  'max_iters': 100,\n",
       "  'min_lr': 0.001,\n",
       "  'n_embd': 16,\n",
       "  'n_head': 1,\n",
       "  'n_layer': 1,\n",
       "  'n_max_context': 44,\n",
       "  'warmup_iters': 0,\n",
       "  'weight_decay': 0.01,\n",
       "  'model_name': 'c4-smoketest',\n",
       "  'model_version': '0.1',\n",
       "  'num_players': 2,\n",
       "  'vocab_size': 8,\n",
       "  'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')),\n",
       "  'eval_iters': 200,\n",
       "  'log_interval': 1000,\n",
       "  'eval_interval': 10000,\n",
       "  'device': 'mps'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "state_0 = game.initial_state()\n",
    "NUM_GENERATIONS = 5\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Parameters which will never be used for tuning.\n",
    "fixed_params = dict(\n",
    "    model_name='c4-smoketest',\n",
    "    model_version='0.1',\n",
    "    num_players = game.num_players(state_0),\n",
    "    vocab_size = action_vocab.vocab_size,\n",
    "    dataset_paths = tuple(experiment_runner.get_trajectory_paths(experiment_config.num_generations)),\n",
    "\n",
    "\n",
    "    eval_iters = 200,\n",
    "    log_interval = 1000,\n",
    "    eval_interval = 10_000,\n",
    "\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "initial_params = dict(\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_embd=8,  # tiny model\n",
    "\n",
    "    n_max_context=n_max_context,\n",
    "    batch_size = 32,\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    max_iters=100,\n",
    "    max_epochs=1_000_000, # Make max_epoch high, rely on max_iters to stop.\n",
    "        \n",
    "    learning_rate = LEARNING_RATE,    \n",
    "    decay_lr = True,  # whether to decay the learning rate\n",
    "    lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    "\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    grad_clip = 1.0,  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = \"float16\",\n",
    "\n",
    "    dropout = 0.0,\n",
    "    bias = False,  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    last_file = None,   # Used in tuning key only.\n",
    ")\n",
    "\n",
    "tune_options = dict(\n",
    "    n_layer = [1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 32],\n",
    "    # n_head = [1, 2, 4, 8, 16, 32],   # Needs to be calcualted to ensure n_embed % n_head == 0\n",
    "    n_embd = [8, 16, 32, 64, 128, 256, 512, 1024, 2048],\n",
    "\n",
    "    n_max_context = [initial_params['n_max_context']],\n",
    "    batch_size = [16, 32, 64, 128, 256, 512, 1024],\n",
    "    gradient_accumulation_steps = [1],  # TODO: We only support 1 for now. This fails is we don't have an exact multiple of the batch size per epoch.\n",
    "\n",
    "    max_iters = [100, 300, 1_000, 3_000, 5_000, 10_000, 30_000, 100_000, 300_000],\n",
    "    max_epochs = [1_000_000], # Make max_epoch high, rely on max_iters to stop.\n",
    " \n",
    "    learning_rate = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "    decay_lr = [False, True],\n",
    "\n",
    "    # TODO: What is a sensible range here?\n",
    "    beta1 = [0.90, 0.95, 0.99],\n",
    "    beta2 = [0.95, 0.98, 0.99],\n",
    "\n",
    "    weight_decay = [0.01, 0.05, 0.1, 0.2],\n",
    "    grad_clip = [0,0, 1.0],  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = [\"bfloat16\", \"float16\"],\n",
    "    dropout = [0.0, 0.01, 0.02, 0.05, 0.1],\n",
    "    bias = [True, False],    \n",
    ")\n",
    "\n",
    "_n_head_options = [1, 2, 4, 8, 16, 32]\n",
    "computed_tune_options = dict(\n",
    "    min_lr = lambda opt: [opt['learning_rate'] / 10],\n",
    "    lr_decay_iters = lambda opt: [opt['max_iters']],\n",
    "    warmup_iters = lambda opt: [x for x in [0, 100, 500, 1000] if x < opt['lr_decay_iters']] if opt['decay_lr'] else [0],\n",
    "    n_head = lambda opt: [n for n in _n_head_options if opt['n_embd'] % n == 0],\n",
    "    last_file = lambda opt: [str(opt['dataset_paths'][-1])],\n",
    ")\n",
    "\n",
    "TUNER_VERSION = \"0.0.4-smoketest\"\n",
    "\n",
    "from rgi.rgizero.models.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=1.00)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.421036395956488 elapsed=22.771531105041504s, val_policy=1.7834, val_value=0.6376\n",
      "## Searching generation 0 with 22 candidates, including ['learning_rate: 0.02 -> 0.05', 'learning_rate: 0.02 -> 0.01', 'dropout: 0.0 -> 0.01', 'dtype: float16 -> bfloat16', 'n_embd: 64 -> 32']\n",
      "## improved: False, loss=2.4266 elapsed=26.04s, mutation learning_rate: 0.02 -> 0.05\n",
      "## improved: False, loss=2.4231 elapsed=24.46s, mutation learning_rate: 0.02 -> 0.01\n",
      "## improved: False, loss=2.4201 elapsed=23.92s, mutation dropout: 0.0 -> 0.01\n",
      "## improved: False, loss=2.4211 elapsed=25.75s, mutation dtype: float16 -> bfloat16\n",
      "## improved: False, loss=2.4282 elapsed=22.50s, mutation n_embd: 64 -> 32\n",
      "## improved: False, loss=2.4682 elapsed=11.64s, mutation max_iters: 1000 -> 300\n",
      "## improved: False, loss=2.4199 elapsed=26.99s, mutation weight_decay: 0.05 -> 0.01\n",
      "## improved: False, loss=2.4241 elapsed=25.83s, mutation beta1: 0.9 -> 0.95\n",
      "## improved: False, loss=2.4751 elapsed=27.46s, mutation n_embd: 64 -> 128\n",
      "## improved: False, loss=2.4214 elapsed=25.54s, mutation n_head: 8 -> 4\n",
      "## improved: False, loss=2.4863 elapsed=26.17s, mutation decay_lr: True -> False\n",
      "## improved: False, loss=2.4458 elapsed=20.62s, mutation batch_size: 128 -> 64\n",
      "## improved: False, loss=2.4189 elapsed=27.13s, mutation beta2: 0.98 -> 0.95\n",
      "## improved: False, loss=2.4221 elapsed=27.18s, mutation n_layer: 1 -> 2\n",
      "## improved: False, loss=2.4766 elapsed=25.33s, mutation warmup_iters: 100 -> 0\n",
      "## improved: False, loss=2.4220 elapsed=29.48s, mutation beta2: 0.98 -> 0.99\n",
      "## improved: False, loss=2.4253 elapsed=27.97s, mutation bias: False -> True\n",
      "## improved: False, loss=2.4257 elapsed=26.88s, mutation warmup_iters: 100 -> 500\n",
      "## improved: False, loss=2.4209 elapsed=27.04s, mutation weight_decay: 0.05 -> 0.1\n",
      "## improved: False, loss=2.4077 elapsed=117.57s, mutation batch_size: 128 -> 256\n",
      "## improved: False, loss=2.4224 elapsed=28.90s, mutation n_head: 8 -> 16\n",
      "## improved: False, loss=2.3933 elapsed=78.96s, mutation max_iters: 1000 -> 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 2.421036395956488,\n",
       " 22.771531105041504,\n",
       " {'batch_size': 128,\n",
       "  'beta1': 0.9,\n",
       "  'beta2': 0.98,\n",
       "  'bias': False,\n",
       "  'decay_lr': True,\n",
       "  'dropout': 0.0,\n",
       "  'dtype': 'float16',\n",
       "  'grad_clip': 1.0,\n",
       "  'gradient_accumulation_steps': 1,\n",
       "  'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20',\n",
       "  'learning_rate': 0.02,\n",
       "  'lr_decay_iters': 1000,\n",
       "  'max_epochs': 1000000,\n",
       "  'max_iters': 1000,\n",
       "  'min_lr': 0.002,\n",
       "  'n_embd': 64,\n",
       "  'n_head': 8,\n",
       "  'n_layer': 1,\n",
       "  'n_max_context': 44,\n",
       "  'warmup_iters': 100,\n",
       "  'weight_decay': 0.05,\n",
       "  'model_name': 'c4-smoketest',\n",
       "  'model_version': '0.1',\n",
       "  'num_players': 2,\n",
       "  'vocab_size': 8,\n",
       "  'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'),\n",
       "   PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')),\n",
       "  'eval_iters': 200,\n",
       "  'log_interval': 1000,\n",
       "  'eval_interval': 10000,\n",
       "  'device': 'mps'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.1)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.326388120651245 elapsed=172.16889691352844s, val_policy=1.7533, val_value=0.5730\n",
      "## Searching generation 0 with 20 candidates, including ['learning_rate: 0.01 -> 0.005', 'learning_rate: 0.01 -> 0.02', 'max_iters: 3000 -> 5000', 'n_head: 8 -> 16', 'n_layer: 4 -> 5']\n",
      "Training learning_rate: 0.01 -> 0.005\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=4, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-smoketest', model_version='0.1', eval_interval=10000, log_interval=1000, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=512, learning_rate=0.005, max_epochs=1000000, max_iters=3000, weight_decay=0.2, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=1000, lr_decay_iters=3000, min_lr=0.0005, device='mps', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "step 0: losses: train:2.7746, train_policy_loss:2.0602, train_value_loss:0.7143, val:2.7748, val_policy_loss:2.0602, val_value_loss:0.7146\n",
      "iter 0/305/3000: loss 2.7799, policy_loss:2.0592, value_loss:0.7207, time 16.66s, iter_time: 0.00ms\n",
      "iter 1000/1220/3000: loss 2.4193, policy_loss:1.7841, value_loss:0.6351, time 5.75s, iter_time: 67.67ms\n",
      "iter 2000/2135/3000: loss 2.3429, policy_loss:1.7667, value_loss:0.5762, time 10.97s, iter_time: 64.54ms\n",
      "iter 3000/3000/3000: loss 2.3255, policy_loss:1.7530, value_loss:0.5725, time 0.05s, iter_time: 0.00ms\n",
      "## train_loss: 2.3288, val_loss: 2.3365, Time taken: 237.51702904701233s, val_policy_loss: 1.7575, val_value_loss: 0.5790, overrides={'batch_size': 512, 'beta1': 0.9, 'beta2': 0.99, 'bias': False, 'decay_lr': True, 'dropout': 0.0, 'dtype': 'float16', 'grad_clip': 1.0, 'gradient_accumulation_steps': 1, 'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20', 'learning_rate': 0.005, 'lr_decay_iters': 3000, 'max_epochs': 1000000, 'max_iters': 3000, 'min_lr': 0.0005, 'n_embd': 64, 'n_head': 8, 'n_layer': 4, 'warmup_iters': 1000, 'weight_decay': 0.2, 'model_name': 'c4-smoketest', 'model_version': '0.1', 'eval_iters': 200, 'log_interval': 1000, 'eval_interval': 10000}\n",
      "## improved: False, loss=2.3365 elapsed=237.52s, mutation learning_rate: 0.01 -> 0.005\n",
      "Training learning_rate: 0.01 -> 0.02\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=4, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-smoketest', model_version='0.1', eval_interval=10000, log_interval=1000, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=512, learning_rate=0.02, max_epochs=1000000, max_iters=3000, weight_decay=0.2, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=1000, lr_decay_iters=3000, min_lr=0.002, device='mps', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: losses: train:2.7746, train_policy_loss:2.0602, train_value_loss:0.7143, val:2.7748, val_policy_loss:2.0602, val_value_loss:0.7146\n",
      "iter 0/305/3000: loss 2.7799, policy_loss:2.0592, value_loss:0.7207, time 7.32s, iter_time: 0.00ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m tuner = Tuner(\n\u001b[32m      2\u001b[39m     fixed_params=fixed_params.copy(),\n\u001b[32m      3\u001b[39m     initial_params=initial_params.copy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     cache_version=TUNER_VERSION,\n\u001b[32m      7\u001b[39m     target_improvement_per_minute=\u001b[32m0.01\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautotune_smart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:395\u001b[39m, in \u001b[36mTuner.autotune_smart\u001b[39m\u001b[34m(self, max_generations)\u001b[39m\n\u001b[32m    391\u001b[39m candidate_params_list = \u001b[38;5;28mself\u001b[39m.select_candidate_params()\n\u001b[32m    392\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    393\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Searching generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_params_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidates, including \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[k\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39m(k,\u001b[38;5;250m \u001b[39mv)\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcandidate_params_list[:\u001b[32m5\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_find_improvement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    397\u001b[39m generation += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:404\u001b[39m, in \u001b[36mTuner._find_improvement\u001b[39m\u001b[34m(self, candidate_params_list)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_find_improvement\u001b[39m(\u001b[38;5;28mself\u001b[39m, candidate_params_list):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, params \u001b[38;5;129;01min\u001b[39;00m candidate_params_list:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m         loss, elapsed, loss_dict, model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m         is_improved = \u001b[38;5;28mself\u001b[39m.maybe_update_best_param(loss, elapsed, params, loss_dict)\n\u001b[32m    406\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## improved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_improved\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elapsed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, mutation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:230\u001b[39m, in \u001b[36mTuner.train_and_compute_loss\u001b[39m\u001b[34m(self, params, reload_model, name)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     loss_dict, elapsed, model = \u001b[43mtrain_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    231\u001b[39m     loss_dict[\u001b[33m\"\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m\"\u001b[39m] = elapsed\n\u001b[32m    232\u001b[39m     loss_dict[\u001b[33m\"\u001b[39m\u001b[33mparam_hash\u001b[39m\u001b[33m\"\u001b[39m] = param_key_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:109\u001b[39m, in \u001b[36mtrain_with\u001b[39m\u001b[34m(vocab_size, num_players, device, n_max_context, dataset_paths, **overrides)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m model = create_random_model(\n\u001b[32m    106\u001b[39m     model_config, action_vocab_size=vocab_size, num_players=num_players, seed=\u001b[32m42\u001b[39m, device=device\n\u001b[32m    107\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m model, trainer = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_max_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_max_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m loss_dict = trainer.estimate_loss()\n\u001b[32m    111\u001b[39m loss_dict = {k: \u001b[38;5;28mfloat\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:80\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataset_paths, train_config, device, n_max_context, num_workers)\u001b[39m\n\u001b[32m     67\u001b[39m train_loader, val_loader = build_trajectory_loader(\n\u001b[32m     68\u001b[39m     dataset_paths,\n\u001b[32m     69\u001b[39m     block_size=n_max_context,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     76\u001b[39m trainer = Trainer(\n\u001b[32m     77\u001b[39m     model=model, train_config=train_config, train_loader=train_loader, val_loader=val_loader, device=device\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/train.py:136\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_config.max_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m         \u001b[38;5;66;03m# termination conditions\u001b[39;00m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_num > \u001b[38;5;28mself\u001b[39m.train_config.max_iters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/train.py:188\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch_id)\u001b[39m\n\u001b[32m    184\u001b[39m         loss = (\n\u001b[32m    185\u001b[39m             loss / \u001b[38;5;28mself\u001b[39m.train_config.gradient_accumulation_steps\n\u001b[32m    186\u001b[39m         )  \u001b[38;5;66;03m# scale the loss to account for gradient accumulation\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_config.grad_clip != \u001b[32m0.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.319357395172119 elapsed=267.94705629348755s, val_policy=1.7505, val_value=0.5688\n",
      "## Searching generation 0 with 20 candidates, including ['learning_rate: 0.01 -> 0.005', 'learning_rate: 0.01 -> 0.02', 'n_head: 8 -> 16', 'bias: False -> True', 'n_layer: 4 -> 5']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m tuner = Tuner(\n\u001b[32m      2\u001b[39m     fixed_params=fixed_params.copy(),\n\u001b[32m      3\u001b[39m     initial_params=initial_params.copy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     cache_version=TUNER_VERSION,\n\u001b[32m      7\u001b[39m     target_improvement_per_minute=\u001b[32m0.001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautotune_smart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:395\u001b[39m, in \u001b[36mTuner.autotune_smart\u001b[39m\u001b[34m(self, max_generations)\u001b[39m\n\u001b[32m    391\u001b[39m candidate_params_list = \u001b[38;5;28mself\u001b[39m.select_candidate_params()\n\u001b[32m    392\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    393\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Searching generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_params_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidates, including \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[k\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39m(k,\u001b[38;5;250m \u001b[39mv)\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcandidate_params_list[:\u001b[32m5\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_find_improvement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    397\u001b[39m generation += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:403\u001b[39m, in \u001b[36m_find_improvement\u001b[39m\u001b[34m(self, candidate_params_list)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model_for_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, params):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     loss, elapsed, loss_dict, model = \u001b[38;5;28mself\u001b[39m.train_and_compute_loss(params, name=\u001b[33m\"\u001b[39m\u001b[33mget_model_for_params\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1481\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1564\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1961\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.3160123053719017 elapsed=547.5916638374329s, val_policy=1.7481, val_value=0.5679\n",
      "## Searching generation 0 with 20 candidates, including ['learning_rate: 0.01 -> 0.005', 'learning_rate: 0.01 -> 0.02', 'dtype: bfloat16 -> float16', 'n_head: 8 -> 16', 'bias: False -> True']\n",
      "## improved: False, loss=2.3345 elapsed=545.90s, mutation learning_rate: 0.01 -> 0.005\n",
      "## improved: False, loss=2.3318 elapsed=546.38s, mutation learning_rate: 0.01 -> 0.02\n",
      "## improved: False, loss=2.3171 elapsed=529.62s, mutation dtype: bfloat16 -> float16\n",
      "## improved: False, loss=2.3215 elapsed=711.62s, mutation n_head: 8 -> 16\n",
      "## improved: False, loss=2.3247 elapsed=574.86s, mutation bias: False -> True\n",
      "Training n_layer: 4 -> 5\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=5, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-smoketest', model_version='0.1', eval_interval=10000, log_interval=1000, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=512, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.2, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=1000, lr_decay_iters=10000, min_lr=0.001, device='mps', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 23, with 249,216 parameters\n",
      "num non-decayed parameter tensors: 13, with 714 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m tuner = Tuner(\n\u001b[32m      2\u001b[39m     fixed_params=fixed_params.copy(),\n\u001b[32m      3\u001b[39m     initial_params=initial_params.copy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     cache_version=TUNER_VERSION,\n\u001b[32m      7\u001b[39m     target_improvement_per_minute=\u001b[32m0.0001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautotune_smart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:395\u001b[39m, in \u001b[36mTuner.autotune_smart\u001b[39m\u001b[34m(self, max_generations)\u001b[39m\n\u001b[32m    391\u001b[39m candidate_params_list = \u001b[38;5;28mself\u001b[39m.select_candidate_params()\n\u001b[32m    392\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    393\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Searching generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_params_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidates, including \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[k\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39m(k,\u001b[38;5;250m \u001b[39mv)\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcandidate_params_list[:\u001b[32m5\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_find_improvement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    397\u001b[39m generation += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:404\u001b[39m, in \u001b[36mTuner._find_improvement\u001b[39m\u001b[34m(self, candidate_params_list)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_find_improvement\u001b[39m(\u001b[38;5;28mself\u001b[39m, candidate_params_list):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, params \u001b[38;5;129;01min\u001b[39;00m candidate_params_list:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m         loss, elapsed, loss_dict, model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m         is_improved = \u001b[38;5;28mself\u001b[39m.maybe_update_best_param(loss, elapsed, params, loss_dict)\n\u001b[32m    406\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## improved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_improved\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elapsed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, mutation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:230\u001b[39m, in \u001b[36mTuner.train_and_compute_loss\u001b[39m\u001b[34m(self, params, reload_model, name)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     loss_dict, elapsed, model = \u001b[43mtrain_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    231\u001b[39m     loss_dict[\u001b[33m\"\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m\"\u001b[39m] = elapsed\n\u001b[32m    232\u001b[39m     loss_dict[\u001b[33m\"\u001b[39m\u001b[33mparam_hash\u001b[39m\u001b[33m\"\u001b[39m] = param_key_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:109\u001b[39m, in \u001b[36mtrain_with\u001b[39m\u001b[34m(vocab_size, num_players, device, n_max_context, dataset_paths, **overrides)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m model = create_random_model(\n\u001b[32m    106\u001b[39m     model_config, action_vocab_size=vocab_size, num_players=num_players, seed=\u001b[32m42\u001b[39m, device=device\n\u001b[32m    107\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m model, trainer = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_max_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_max_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m loss_dict = trainer.estimate_loss()\n\u001b[32m    111\u001b[39m loss_dict = {k: \u001b[38;5;28mfloat\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/tuner.py:80\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataset_paths, train_config, device, n_max_context, num_workers)\u001b[39m\n\u001b[32m     67\u001b[39m train_loader, val_loader = build_trajectory_loader(\n\u001b[32m     68\u001b[39m     dataset_paths,\n\u001b[32m     69\u001b[39m     block_size=n_max_context,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     76\u001b[39m trainer = Trainer(\n\u001b[32m     77\u001b[39m     model=model, train_config=train_config, train_loader=train_loader, val_loader=val_loader, device=device\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/train.py:136\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_config.max_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m         \u001b[38;5;66;03m# termination conditions\u001b[39;00m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_num > \u001b[38;5;28mself\u001b[39m.train_config.max_iters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/train.py:157\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch_id)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# evaluate the loss on train/val sets and write checkpoints\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_num % \u001b[38;5;28mself\u001b[39m.train_config.eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     loss_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m losses.items())\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.iter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/train.py:108\u001b[39m, in \u001b[36mTrainer.estimate_loss\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m data_batch = \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     logits, loss_dict, loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m loss_sums[split] += loss.item()\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/action_history_transformer.py:101\u001b[39m, in \u001b[36mActionHistoryTransformer.forward\u001b[39m\u001b[34m(self, idx, policy_target, value_target, padding_mask, encoded_len)\u001b[39m\n\u001b[32m     99\u001b[39m     flat_policy_logits = flat_policy_logits[flat_padding_mask]\n\u001b[32m    100\u001b[39m     flat_policy_target = flat_policy_target[flat_padding_mask]\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43mvalidate_probabilities_or_die\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_policy_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Calcualte the average loss per unpadded tokens.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# note: We may want to experiment with average per batch?\u001b[39;00m\n\u001b[32m    104\u001b[39m policy_loss = F.cross_entropy(flat_policy_logits, flat_policy_target, reduction=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3-sync/src/rgi/rgizero/models/action_history_transformer.py:14\u001b[39m, in \u001b[36mvalidate_probabilities_or_die\u001b[39m\u001b[34m(tensor, dim, tol)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrgi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrgizero\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_probabilities_or_die\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# TODO: Move to util class.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_probabilities_or_die\u001b[39m(tensor: torch.Tensor, dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m, tol: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1e-6\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 1. Check if all values are >= 0 and <= 1\u001b[39;00m\n\u001b[32m     16\u001b[39m     in_range = (tensor >= \u001b[32m0\u001b[39m).all() \u001b[38;5;129;01mand\u001b[39;00m (tensor <= \u001b[32m1\u001b[39m).all()\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_range:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0001)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Skip this...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_fields: {'n_layer', 'dropout', 'n_embd', 'n_head', 'n_max_context', 'bias'}\n",
      "train_config_fields: {'model_name', 'beta2', 'log_interval', 'eval_interval', 'eval_iters', 'compile', 'grad_clip', 'eval_only', 'learning_rate', 'decay_lr', 'wandb_log', 'weight_decay', 'dtype', 'warmup_iters', 'max_iters', 'device', 'batch_size', 'beta1', 'lr_decay_iters', 'always_save_checkpoint', 'max_epochs', 'min_lr', 'model_version', 'gradient_accumulation_steps'}\n",
      "Using initial model as baseline.\n",
      "## Initial Model, loss=2.319357395172119 elapsed=267.94705629348755s, val_policy=1.7505, val_value=0.5688\n",
      "## Searching generation 0 with 20 candidates, including ['learning_rate: 0.01 -> 0.005', 'learning_rate: 0.01 -> 0.02', 'n_head: 8 -> 16', 'n_layer: 4 -> 5', 'bias: False -> True']\n",
      "## improved: False, loss=2.3267 elapsed=267.92s, mutation learning_rate: 0.01 -> 0.005\n",
      "## improved: False, loss=2.3394 elapsed=267.30s, mutation learning_rate: 0.01 -> 0.02\n",
      "## improved: False, loss=2.3256 elapsed=347.61s, mutation n_head: 8 -> 16\n",
      "## improved: False, loss=2.3233 elapsed=316.85s, mutation n_layer: 4 -> 5\n",
      "## improved: False, loss=2.3244 elapsed=291.85s, mutation bias: False -> True\n",
      "## improved: False, loss=2.3171 elapsed=529.62s, mutation max_iters: 5000 -> 10000\n",
      "## improved: False, loss=2.3356 elapsed=894.24s, mutation batch_size: 512 -> 1024\n",
      "## improved: False, loss=2.3221 elapsed=268.85s, mutation beta2: 0.99 -> 0.98\n",
      "## improved: False, loss=2.3221 elapsed=269.51s, mutation weight_decay: 0.2 -> 0.1\n",
      "## improved: False, loss=2.3314 elapsed=166.01s, mutation batch_size: 512 -> 256\n",
      "## improved: False, loss=2.3295 elapsed=269.07s, mutation beta1: 0.9 -> 0.95\n",
      "## improved: False, loss=2.3275 elapsed=388.28s, mutation n_embd: 64 -> 128\n",
      "## improved: False, loss=2.3528 elapsed=229.91s, mutation n_embd: 64 -> 32\n",
      "## improved: False, loss=2.3209 elapsed=273.21s, mutation dtype: float16 -> bfloat16\n",
      "## improved: False, loss=2.3196 elapsed=328.32s, mutation dropout: 0.0 -> 0.01\n",
      "## improved: False, loss=2.3280 elapsed=285.61s, mutation n_head: 8 -> 4\n",
      "## improved: False, loss=2.3253 elapsed=272.66s, mutation warmup_iters: 1000 -> 500\n",
      "## improved: False, loss=2.3284 elapsed=226.62s, mutation n_layer: 4 -> 3\n",
      "## improved: False, loss=2.3710 elapsed=274.61s, mutation decay_lr: True -> False\n",
      "## improved: False, loss=2.3264 elapsed=172.17s, mutation max_iters: 5000 -> 3000\n",
      "\n",
      "## Recalculating with best_params = {'batch_size': 512, 'beta1': 0.9, 'beta2': 0.99, 'bias': False, 'decay_lr': True, 'dropout': 0.0, 'dtype': 'float16', 'grad_clip': 1.0, 'gradient_accumulation_steps': 1, 'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20', 'learning_rate': 0.0005, 'lr_decay_iters': 5000, 'max_epochs': 1000000, 'max_iters': 30000, 'min_lr': 0.001, 'n_embd': 64, 'n_head': 8, 'n_layer': 4, 'n_max_context': 44, 'warmup_iters': 1000, 'weight_decay': 0.2, 'model_name': 'c4-smoketest', 'model_version': '0.1', 'num_players': 2, 'vocab_size': 8, 'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')), 'eval_iters': 200, 'log_interval': 1000, 'eval_interval': 10000, 'device': 'mps'}\n",
      "{'train': 2.2302739822864535, 'train_policy_loss': 1.7535288149118424, 'train_value_loss': 0.47674516707658765, 'val': 2.4826266765594482, 'val_policy_loss': 1.755559900227715, 'val_value_loss': 0.7270667658132666, 'elapsed': 1617.1719007492065, 'param_hash': '4364fb3cfa7a3b4d33fe30f70ec9957a0a14bc7d9a195b5a2de2247d2a72a6d1'}\n"
     ]
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "\n",
    "tuner_result = tuner.autotune_smart()\n",
    "# print(f'tuner_result={tuner_result}')\n",
    "\n",
    "best_params = tuner.best_params.copy()\n",
    "## Recalculating with best_params = {'batch_size': 512, 'beta1': 0.9, 'beta2': 0.99, 'bias': False, 'decay_lr': True, 'dropout': 0.0, 'dtype': 'float16', 'grad_clip': 1.0, 'gradient_accumulation_steps': 1, 'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20', 'learning_rate': 0.01, 'lr_decay_iters': 5000, 'max_epochs': 1000000, 'max_iters': 30000, 'min_lr': 0.001, 'n_embd': 64, 'n_head': 8, 'n_layer': 4, 'n_max_context': 44, 'warmup_iters': 1000, 'weight_decay': 0.2, 'model_name': 'c4-smoketest', 'model_version': '0.1', 'num_players': 2, 'vocab_size': 8, 'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')), 'eval_iters': 200, 'log_interval': 1000, 'eval_interval': 10000, 'device': 'mps'}\n",
    "## {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "\n",
    "best_params['max_iters'] = 30_000 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "# best_params['max_iters'] = 10_000 #{'train': 2.2972692108154296, 'train_policy_loss': 1.7478227978944778, 'train_value_loss': 0.5494464221596718, 'val': 2.3170768583522126, 'val_policy_loss': 1.7486604136579178, 'val_value_loss': 0.5684164271635168, 'elapsed': 529.6186480522156, 'param_hash': 'c81f68b9b00650b83a229a76b93d77d51f7bd1af43148f6c1af75e8f562f2670'}\n",
    "# best_params['max_iters'] = 5000 # {'train': 2.3059648656845093, 'train_policy_loss': 1.749609624147415, 'train_value_loss': 0.5563552376627922, 'val': 2.319357395172119, 'val_policy_loss': 1.7505432332263273, 'val_value_loss': 0.5688141549334806, 'elapsed': 267.94705629348755, 'param_hash': 'a1194e1f289d35c51e5a0f01692109358f947530b50dd54364f01baefdb6bedf'}\n",
    "# best_params['max_iters'] = 3000 # {'train': 2.3192384707927705, 'train_policy_loss': 1.752758464217186, 'train_value_loss': 0.5664800041913987, 'val': 2.326388120651245, 'val_policy_loss': 1.7533490412375505, 'val_value_loss': 0.5730390969444724, 'elapsed': 172.16889691352844, 'param_hash': '9e0204edd0b23fd54025b0c4de66c870a268b95197e0e47c4460ec03875b3dcb'}\n",
    "\n",
    "# best_params['learning_rate'] = 0.01 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "best_params['learning_rate'] = 0.0005 # {'train': 2.2302739822864535, 'train_policy_loss': 1.7535288149118424, 'train_value_loss': 0.47674516707658765, 'val': 2.4826266765594482, 'val_policy_loss': 1.755559900227715, 'val_value_loss': 0.7270667658132666, 'elapsed': 1617.1719007492065, 'param_hash': '4364fb3cfa7a3b4d33fe30f70ec9957a0a14bc7d9a195b5a2de2247d2a72a6d1'}\n",
    "\n",
    "print(f'\\n## Recalculating with best_params = {best_params}')\n",
    "best_params = tuner._recalculate_tunable_params(best_params)\n",
    "best_model = tuner.get_model_for_params(best_params)\n",
    "print(tuner.train_and_compute_loss(best_params, reload_model=True)[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
      "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
      "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 23000\n",
      "  Total actions: 351053\n",
      "  Avg trajectory length: 15.26\n",
      "Prefix Stats:\n",
      "actions=(): 23000 win=14023 loss=8971 draw=6 win1%=60.97 model-win1%=68.75\n",
      "actions=(1,): 3384 win=1795 loss=1587 draw=2 win1%=53.04 model-win1%=98.00\n",
      "actions=(2,): 2408 win=1369 loss=1038 draw=1 win1%=56.85 model-win1%=99.00\n",
      "actions=(3,): 2829 win=1828 loss=1001 draw=0 win1%=64.62 model-win1%=96.93\n",
      "actions=(4,): 3498 win=2569 loss=928 draw=1 win1%=73.44 model-win1%=99.15\n",
      "actions=(5,): 4276 win=2791 loss=1484 draw=1 win1%=65.27 model-win1%=98.74\n",
      "actions=(6,): 3478 win=2019 loss=1459 draw=0 win1%=58.05 model-win1%=98.81\n",
      "actions=(7,): 3127 win=1652 loss=1474 draw=1 win1%=52.83 model-win1%=95.51\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = experiment_runner.get_trajectory_paths(NUM_GENERATIONS)\n",
    "print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=best_model, game=game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionHistoryTransformer(\n",
       "  (action_embedding): Embedding(8, 64)\n",
       "  (transformer): Transformer(\n",
       "    (wpe): Embedding(44, 64)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (policy_value_head): PolicyValueHead(\n",
       "    (policy): Linear(in_features=64, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        # dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:5].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:6].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:7].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:8].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:9].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:10].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=n_max_context, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):    \n",
    "    list(dd.items())[10][1]['*'].sum() > 100\n",
    "    top_k = sorted(dd.items(), key=lambda kv: kv[1]['*'].sum(), reverse=True)[:20]\n",
    "    top_k_keys = sorted(k for k, v in top_k)\n",
    "    \n",
    "    prefix_list = top_k_keys\n",
    "\n",
    "    # prefix_list = [\n",
    "    #     (0,), \n",
    "    #     (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "    #     (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "    #     (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7),\n",
    "    # ]\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            if gen == '*':\n",
    "                print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        # # assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tournament to calcualte ELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from contextlib import asynccontextmanager\n",
    "from rgi.rgizero.tournament import Tournament\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_player_factory(model, simulations, game, device, block_size, action_vocab, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a shared evaluator and returns a factory function that produces \n",
    "    new AlphazeroPlayer instances using that shared evaluator.\n",
    "    \"\"\"\n",
    "    # 1. Setup the shared evaluator\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(\n",
    "        model, \n",
    "        device=device, \n",
    "        block_size=n_max_context, \n",
    "        vocab=action_vocab\n",
    "    )\n",
    "    async_evaluator = AsyncNetworkEvaluator(\n",
    "        base_evaluator=serial_evaluator, \n",
    "        max_batch_size=max_batch_size, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 2. Start the evaluator background task\n",
    "    await async_evaluator.start()\n",
    "    \n",
    "    try:\n",
    "        # 3. Define the factory. This is called by Tournament for every game.\n",
    "        # It creates a NEW player instance but uses the SHARED async_evaluator.\n",
    "        def player_factory():\n",
    "            # Create a fresh RNG for each game/player instance\n",
    "            rng = np.random.default_rng(np.random.randint(0, 2**31))\n",
    "            return AlphazeroPlayer(\n",
    "                game, \n",
    "                async_evaluator, \n",
    "                rng=rng, \n",
    "                add_noise=True, \n",
    "                simulations=simulations\n",
    "            )\n",
    "            \n",
    "        yield player_factory\n",
    "        \n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        await async_evaluator.stop()\n",
    "\n",
    "async def run_tournament_async():\n",
    "    # Use async with to manage the lifecycle of the evaluators\n",
    "    async with (\n",
    "        # create_player_factory(model_dict[0], 100, game, device, block_size, action_vocab, 10) as factory_gen0_100,\n",
    "        # create_player_factory(model_dict[1], 100, game, device, block_size, action_vocab, 10) as factory_gen1_100,\n",
    "        # create_player_factory(model_dict[2], 100, game, device, block_size, action_vocab, 10) as factory_gen2_100,\n",
    "        # create_player_factory(model_dict[3], 100, game, device, block_size, action_vocab, 10) as factory_gen3_100,\n",
    "        # create_player_factory(model_dict[4], 100, game, device, block_size, action_vocab, 10) as factory_gen4_100,\n",
    "        # create_player_factory(model_dict[5], 100, game, device, block_size, action_vocab, 10) as factory_gen5_100,\n",
    "        # create_player_factory(model_dict[10], 100, game, device, block_size, action_vocab, 10) as factory_gen6_100,\n",
    "        # create_player_factory(model_dict[15], 100, game, device, block_size, action_vocab, 10) as factory_gen7_100,\n",
    "        # create_player_factory(model_dict[20], 100, game, device, block_size, action_vocab, 10) as factory_gen8_100,\n",
    "\n",
    "        create_player_factory(model_dict[0], 200, game, device, block_size, action_vocab, 10) as factory_gen0_200,\n",
    "        #create_player_factory(model_dict[1], 200, game, device, block_size, action_vocab, 10) as factory_gen1_200,\n",
    "        #create_player_factory(model_dict[2], 200, game, device, block_size, action_vocab, 10) as factory_gen2_200,\n",
    "        #create_player_factory(model_dict[3], 200, game, device, block_size, action_vocab, 10) as factory_gen3_200,\n",
    "        #create_player_factory(model_dict[4], 200, game, device, block_size, action_vocab, 10) as factory_gen4_200,\n",
    "        create_player_factory(model_dict[5], 200, game, device, block_size, action_vocab, 10) as factory_gen5_200,\n",
    "        #create_player_factory(model_dict[10], 200, game, device, block_size, action_vocab, 10) as factory_gen10_200,\n",
    "        #create_player_factory(model_dict[15], 200, game, device, block_size, action_vocab, 10) as factory_gen15_200,\n",
    "        create_player_factory(model_dict[20], 200, game, device, block_size, action_vocab, 10) as factory_gen20_200,\n",
    "        ):\n",
    "        \n",
    "        # The dictionary now maps names to FACTORIES (Callables), not Player instances\n",
    "        player_factories = {\n",
    "            # \"factory_gen0_100\": factory_gen0_100,\n",
    "            # \"factory_gen1_100\": factory_gen1_100,\n",
    "            # \"factory_gen2_100\": factory_gen2_100,\n",
    "            # \"factory_gen3_100\": factory_gen3_100,\n",
    "            # \"factory_gen4_100\": factory_gen4_100,\n",
    "            # \"factory_gen5_100\": factory_gen5_100,\n",
    "            # \"factory_gen6_100\": factory_gen6_100,\n",
    "            # \"factory_gen7_100\": factory_gen7_100,\n",
    "\n",
    "            \"factory_gen0_200\": factory_gen0_200,\n",
    "            #\"factory_gen1_200\": factory_gen1_200,\n",
    "            #\"factory_gen2_200\": factory_gen2_200,\n",
    "            #\"factory_gen3_200\": factory_gen3_200,\n",
    "            #\"factory_gen4_200\": factory_gen4_200,\n",
    "            \"factory_gen5_200\": factory_gen5_200,\n",
    "            #\"factory_gen10_200\": factory_gen10_200,\n",
    "            #\"factory_gen15_200\": factory_gen15_200,\n",
    "            \"factory_gen20_200\": factory_gen20_200,\n",
    "        }\n",
    "        \n",
    "        tournament = Tournament(game, player_factories, initial_elo=1000)\n",
    "        \n",
    "        print(\"Running tournament...\")\n",
    "        # await tournament.run(num_games=1_000, concurrent_games=2000)\n",
    "        await tournament.run(num_games=100, concurrent_games=2000)\n",
    "        tournament.print_standings()\n",
    "\n",
    "# RUN_TOURNAMENT = True\n",
    "if RUN_TOURNAMENT:\n",
    "    await run_tournament_async()\n",
    "\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 10000/10000 [1:25:59<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen6_200     1140.5     1247     827-419-1      \n",
    "# 2     factory_gen2_200     1100.1     1251     693-554-4      \n",
    "# 3     factory_gen5_100     1074.4     1251     598-652-1      \n",
    "# 4     factory_gen3_200     1029.1     1252     674-573-5      \n",
    "# 5     factory_gen4_200     1027.0     1248     711-536-1      \n",
    "# 6     factory_gen0_200     1020.0     1254     444-810-0      \n",
    "# 7     factory_gen5_200     990.2      1248     742-502-4      \n",
    "# 8     factory_gen7_100     987.5      1250     650-597-3      \n",
    "# 9     factory_gen7_200     979.2      1248     768-476-4      \n",
    "# 10    factory_gen2_100     974.0      1249     522-723-4      \n",
    "# 11    factory_gen6_100     966.6      1248     684-564-0      \n",
    "# 12    factory_gen4_100     964.2      1251     557-693-1      \n",
    "# 13    factory_gen1_100     962.5      1252     547-705-0      \n",
    "# 14    factory_gen3_100     947.0      1251     528-723-0      \n",
    "# 15    factory_gen1_200     941.1      1252     630-620-2      \n",
    "# 16    factory_gen0_100     896.5      1248     410-838-0     \n",
    "\n",
    "\n",
    "## 20 generations.\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 1000/1000 [08:35<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen10_200    1114.2     333      212-120-1      \n",
    "# 2     factory_gen2_200     1032.6     333      190-141-2      \n",
    "# 3     factory_gen1_200     1003.9     334      159-175-0      \n",
    "# 4     factory_gen20_200    1000.9     335      171-164-0      \n",
    "# 5     factory_gen5_200     974.6      331      183-146-2      \n",
    "# 6     factory_gen0_200     873.8      334      82-251-1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model (continued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune_smart()\n",
    "\n",
    "# Using initial model as baseline.\n",
    "# ## Initial Model, loss=2.1298508644104004 elapsed=171.78943705558777s\n",
    "# ## Searching generation 0 with 22 candidates, including ['bias: False -> True', 'learning_rate: 0.005 -> 0.002', 'learning_rate: 0.005 -> 0.002', 'dtype: bfloat16 -> float16', 'weight_decay: 0.1 -> 0.2']\n",
    "# ## improved: False, loss=2.1332 elapsed=178.64s, mutation bias: False -> True\n",
    "# ## improved: False, loss=2.1395 elapsed=172.03s, mutation learning_rate: 0.005 -> 0.002\n",
    "# ## improved: False, loss=2.1395 elapsed=172.03s, mutation learning_rate: 0.005 -> 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune_smart()\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "tiny_config: TransformerConfig = TransformerConfig(n_max_context=100, n_layer=2, n_head=2, n_embd=8)\n",
    "tiny_model = ActionHistoryTransformer(config=tiny_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0))\n",
    "tiny_model.to(device)\n",
    "tiny_evaluator = ActionHistoryTransformerEvaluator(tiny_model, device=device, block_size=5, vocab=action_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0001)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "best_model = tuner.load_best_model()\n",
    "compare_model_vs_data(best_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(f'tuner.best_loss={tuner.best_loss}')\n",
    "print(f'tuner.best_loss_elapsed={int(tuner.best_loss_elapsed)//60}m{tuner.best_loss_elapsed%60:.0f}s')\n",
    "pprint(tuner.best_params)\n",
    "# best_params = tuner.initial_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print tuner stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "# print stats based on cached results.\n",
    "tuner_stats = tuner.print_hparam_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(k,v['mean_val_delta']) for (k,v) in sorted(tuner_stats.items(), key=lambda x: x[1]['mean_val_delta'], reverse=True)]\n",
    "\n",
    "for x in  sorted([(v['mean_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True): print(x)\n",
    "# sorted([(v['mean_val_delta'], k) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted([(v['std_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['std_val_delta'])], reverse=True): print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"xxx STOP HERE xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
