{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_fields: {'bias', 'n_max_context', 'dropout', 'n_layer', 'n_head', 'n_embd'}\n",
      "train_config_fields: {'batch_size', 'min_lr', 'model_version', 'beta2', 'max_epochs', 'learning_rate', 'always_save_checkpoint', 'max_iters', 'beta1', 'warmup_iters', 'eval_iters', 'weight_decay', 'model_name', 'early_stop_patience', 'grad_clip', 'compile', 'wandb_log', 'lr_decay_iters', 'device', 'eval_only', 'log_interval', 'dtype', 'gradient_accumulation_steps', 'eval_interval', 'decay_lr'}\n",
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.experiment import ExperimentRunner, ExperimentConfig\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab, print_dataset_stats, TrajectoryDataset\n",
    "from rgi.rgizero.evaluators import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "from rgi.rgizero.models.tuner import create_random_model\n",
    "\n",
    "import notebook_utils\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "device = notebook_utils.detect_device()\n",
    "\n",
    "## Disable for debugger stability?\n",
    "# # Allow asyncio to work with jupyter notebook\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GENERATIONS = True\n",
    "\n",
    "\n",
    "# Create Experiment Config\n",
    "experiment_config = ExperimentConfig(\n",
    "    experiment_name='smoketest-e2e-v7',   # Use sliding window. Fix game reward bug. Shorted eval interval. Set max-epoch=1. max_epoch=2, less training steps.\n",
    "    # experiment_name='smoketest-e2e-v2',\n",
    "    # parent_experiment_name='smoketest-e2e',\n",
    "    game_name='connect4',\n",
    "    num_generations=10,\n",
    "    num_games_per_gen=2_000,\n",
    "    num_simulations=200,\n",
    "    # model_size=\"tiny\",\n",
    "    # train_batch_size=10,\n",
    "    # max_training_epochs=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Tuned params from connect4 with 23k training games.\n",
    "tuned_params = {\n",
    "    'batch_size': 512,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.99,\n",
    "    'bias': False,\n",
    "    'decay_lr': True,\n",
    "    'dropout': 0.0,\n",
    "    'dtype': 'float16',\n",
    "    'grad_clip': 1.0,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'lr_decay_iters': 5000,\n",
    "    'max_epochs': 2,      # We retrain the same model each generation, so having a low epoch count good. This means we don't overfit during early generations.\n",
    "    'max_iters': 10_000,  # 30_000\n",
    "    'min_lr': 0.0001,\n",
    "    'n_embd': 64,\n",
    "    'n_head': 2,\n",
    "    'n_layer': 4,\n",
    "    'n_max_context': 44,\n",
    "    'warmup_iters': 200,\n",
    "    'weight_decay': 0.2,\n",
    "    'eval_iters': 50,\n",
    "    'log_interval': 250,\n",
    "    'eval_interval': 500,\n",
    "    'early_stop_patience': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up game and experiment runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Runner initialized\n",
      "Game: connect4, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Data dir:  /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/data\n",
      "Model dir:  /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "# Initialize Experiment Runner\n",
    "experiment_base_dir = Path.cwd().parent / 'experiments'\n",
    "experiment_runner = ExperimentRunner(experiment_config, experiment_base_dir, training_args=tuned_params)\n",
    "game = experiment_runner.game\n",
    "action_vocab = experiment_runner.action_vocab\n",
    "n_max_context = experiment_runner.n_max_context\n",
    "\n",
    "DATA_DIR = experiment_runner.data_dir\n",
    "MODEL_DIR = experiment_runner.models_dir\n",
    "\n",
    "print('✅ Runner initialized')\n",
    "print(f'Game: {experiment_runner.config.game_name}, Players: {experiment_runner.num_players}, Actions: {list(game.base_game.all_actions())}')\n",
    "print('Data dir: ', DATA_DIR)\n",
    "print('Model dir: ', MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment: smoketest-e2e-v7\n",
      "Initializing Random Gen 0 model.\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-0.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize (creates Random Gen 0 if needed)\n",
    "model_0 = experiment_runner.initialize()\n",
    "current_model = model_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   4%|▍         | 89/2000 [00:44<04:38,  6.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=1000, eval-per-second=114467.11, total-batches=1000, mean-eval-per-second=89943.26, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  22%|██▏       | 447/2000 [01:36<02:53,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.021 seconds, size=1000, eval-per-second=47472.12, total-batches=2000, mean-eval-per-second=78431.52, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  44%|████▍     | 878/2000 [02:33<01:15, 14.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.019 seconds, size=1000, eval-per-second=53859.44, total-batches=3000, mean-eval-per-second=72124.02, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  66%|██████▋   | 1326/2000 [03:30<00:53, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.020 seconds, size=678, eval-per-second=33158.87, total-batches=4000, mean-eval-per-second=61657.04, mean-time-per-batch=0.016, mean-batch-size=969.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  84%|████████▎ | 1673/2000 [04:07<00:49,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.026 seconds, size=327, eval-per-second=12661.79, total-batches=5000, mean-eval-per-second=52680.10, mean-time-per-batch=0.017, mean-batch-size=873.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  94%|█████████▍| 1890/2000 [04:31<00:10, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.006 seconds, size=110, eval-per-second=17903.51, total-batches=6000, mean-eval-per-second=46058.76, mean-time-per-batch=0.017, mean-batch-size=763.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  99%|█████████▉| 1979/2000 [04:42<00:03,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=22, eval-per-second=9255.23, total-batches=7000, mean-eval-per-second=42685.32, mean-time-per-batch=0.016, mean-batch-size=662.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1999/2000 [04:47<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=2, eval-per-second=1226.23, total-batches=8000, mean-eval-per-second=41379.50, mean-time-per-batch=0.014, mean-batch-size=580.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [04:47<00:00,  6.95it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 1...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.7687, train_policy_loss:2.0614, train_value_loss:0.7072, val:2.7748, val_policy_loss:2.0622, val_value_loss:0.7126\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1/best.pt\n",
      "iter 0/4/10000: loss 2.7673, policy_loss:2.0620, value_loss:0.7053, time 0.65s, iter_time: 0.00ms\n",
      "step 3: losses: train:2.7556, train_policy_loss:2.0551, train_value_loss:0.7006, val:2.7591, val_policy_loss:2.0556, val_value_loss:0.7035\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1\n",
      "step 7: losses: train:2.7197, train_policy_loss:2.0341, train_value_loss:0.6856, val:2.7150, val_policy_loss:2.0352, val_value_loss:0.6798\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-1/best.pt (val_loss=2.7150)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-1.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33621\n",
      "  Avg trajectory length: 16.81\n",
      "Prefix Stats:\n",
      "actions=(): 2000 win=1273 loss=718 draw=9 win1%=63.65 model-win1%=56.65\n",
      "actions=(1,): 93 win=53 loss=40 draw=0 win1%=56.99 model-win1%=48.11\n",
      "actions=(2,): 73 win=45 loss=27 draw=1 win1%=61.64 model-win1%=58.20\n",
      "actions=(3,): 107 win=66 loss=41 draw=0 win1%=61.68 model-win1%=46.66\n",
      "actions=(4,): 827 win=583 loss=243 draw=1 win1%=70.50 model-win1%=55.78\n",
      "actions=(4, 1): 379 win=273 loss=106 draw=0 win1%=72.03 model-win1%=54.16\n",
      "actions=(5,): 228 win=154 loss=73 draw=1 win1%=67.54 model-win1%=54.22\n",
      "actions=(5, 1): 121 win=89 loss=31 draw=1 win1%=73.55 model-win1%=53.02\n",
      "actions=(6,): 468 win=275 loss=188 draw=5 win1%=58.76 model-win1%=55.20\n",
      "actions=(6, 1): 241 win=152 loss=86 draw=3 win1%=63.07 model-win1%=52.52\n",
      "actions=(7,): 204 win=97 loss=106 draw=1 win1%=47.55 model-win1%=52.54\n",
      "\n",
      "=== Generation 2 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   4%|▍         | 84/2000 [00:45<04:09,  7.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.019 seconds, size=1000, eval-per-second=52070.17, total-batches=1000, mean-eval-per-second=87920.44, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  20%|██        | 404/2000 [01:40<05:04,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.021 seconds, size=1000, eval-per-second=47870.35, total-batches=2000, mean-eval-per-second=76821.89, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  42%|████▏     | 834/2000 [02:39<02:07,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.017 seconds, size=1000, eval-per-second=57424.75, total-batches=3000, mean-eval-per-second=71333.05, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  63%|██████▎   | 1256/2000 [03:40<01:49,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.018 seconds, size=748, eval-per-second=42521.17, total-batches=4000, mean-eval-per-second=61299.90, mean-time-per-batch=0.016, mean-batch-size=980.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  80%|████████  | 1600/2000 [04:26<00:33, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.039 seconds, size=400, eval-per-second=10312.95, total-batches=5000, mean-eval-per-second=51443.67, mean-time-per-batch=0.017, mean-batch-size=898.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  92%|█████████▏| 1842/2000 [04:54<00:20,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.008 seconds, size=156, eval-per-second=20131.42, total-batches=6000, mean-eval-per-second=45255.04, mean-time-per-batch=0.018, mean-batch-size=795.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  98%|█████████▊| 1968/2000 [05:08<00:03,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.003 seconds, size=34, eval-per-second=11277.69, total-batches=7000, mean-eval-per-second=41886.33, mean-time-per-batch=0.017, mean-batch-size=693.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1999/2000 [05:11<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=1, eval-per-second=560.81, total-batches=8000, mean-eval-per-second=40859.16, mean-time-per-batch=0.015, mean-batch-size=608.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:12<00:00,  6.39it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 2...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.7138, train_policy_loss:2.0346, train_value_loss:0.6792, val:2.7199, val_policy_loss:2.0335, val_value_loss:0.6864\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2/best.pt\n",
      "iter 0/8/10000: loss 2.7178, policy_loss:2.0350, value_loss:0.6828, time 0.67s, iter_time: 0.00ms\n",
      "step 7: losses: train:2.6847, train_policy_loss:2.0134, train_value_loss:0.6713, val:2.6925, val_policy_loss:2.0113, val_value_loss:0.6812\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2\n",
      "step 15: losses: train:2.6444, train_policy_loss:1.9763, train_value_loss:0.6681, val:2.6537, val_policy_loss:1.9761, val_value_loss:0.6776\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-2/best.pt (val_loss=2.6537)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-2.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 4000\n",
      "  Total actions: 67717\n",
      "  Avg trajectory length: 16.93\n",
      "Prefix Stats:\n",
      "actions=(): 4000 win=2573 loss=1417 draw=10 win1%=64.33 model-win1%=64.95\n",
      "actions=(1,): 197 win=119 loss=78 draw=0 win1%=60.41 model-win1%=57.44\n",
      "actions=(2,): 152 win=89 loss=62 draw=1 win1%=58.55 model-win1%=60.84\n",
      "actions=(3,): 246 win=162 loss=84 draw=0 win1%=65.85 model-win1%=58.39\n",
      "actions=(4,): 1639 win=1144 loss=493 draw=2 win1%=69.80 model-win1%=62.68\n",
      "actions=(4, 1): 561 win=415 loss=146 draw=0 win1%=73.98 model-win1%=62.43\n",
      "actions=(4, 2): 231 win=168 loss=63 draw=0 win1%=72.73 model-win1%=65.42\n",
      "actions=(5,): 598 win=389 loss=208 draw=1 win1%=65.05 model-win1%=61.96\n",
      "actions=(5, 1): 229 win=167 loss=61 draw=1 win1%=72.93 model-win1%=61.76\n",
      "actions=(6,): 824 win=487 loss=332 draw=5 win1%=59.10 model-win1%=59.60\n",
      "actions=(6, 1): 322 win=205 loss=114 draw=3 win1%=63.66 model-win1%=59.71\n",
      "actions=(7,): 344 win=183 loss=160 draw=1 win1%=53.20 model-win1%=58.23\n",
      "\n",
      "=== Generation 3 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   2%|▏         | 49/2000 [00:44<02:46, 11.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.007 seconds, size=1000, eval-per-second=139373.43, total-batches=1000, mean-eval-per-second=90218.53, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  17%|█▋        | 331/2000 [01:41<03:26,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.020 seconds, size=1000, eval-per-second=50717.71, total-batches=2000, mean-eval-per-second=79266.10, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  36%|███▌      | 716/2000 [02:39<03:16,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.010 seconds, size=1000, eval-per-second=96589.54, total-batches=3000, mean-eval-per-second=73243.81, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  57%|█████▋    | 1131/2000 [03:45<03:38,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.047 seconds, size=870, eval-per-second=18698.86, total-batches=4000, mean-eval-per-second=61264.19, mean-time-per-batch=0.016, mean-batch-size=994.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  75%|███████▌  | 1506/2000 [04:42<00:41, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.010 seconds, size=493, eval-per-second=51483.71, total-batches=5000, mean-eval-per-second=49022.93, mean-time-per-batch=0.019, mean-batch-size=933.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  89%|████████▉ | 1779/2000 [05:14<00:38,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.012 seconds, size=222, eval-per-second=18859.59, total-batches=6000, mean-eval-per-second=44024.72, mean-time-per-batch=0.019, mean-batch-size=836.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  97%|█████████▋| 1939/2000 [05:33<00:09,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.004 seconds, size=62, eval-per-second=13995.31, total-batches=7000, mean-eval-per-second=39871.58, mean-time-per-batch=0.018, mean-batch-size=734.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1993/2000 [05:38<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=7, eval-per-second=3662.69, total-batches=8000, mean-eval-per-second=38853.96, mean-time-per-batch=0.017, mean-batch-size=646.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:40<00:00,  5.87it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 3...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.6531, train_policy_loss:1.9795, train_value_loss:0.6737, val:2.6370, val_policy_loss:1.9801, val_value_loss:0.6569\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-3/best.pt\n",
      "iter 0/11/10000: loss 2.6458, policy_loss:1.9796, value_loss:0.6662, time 1.31s, iter_time: 0.00ms\n",
      "step 10: losses: train:2.6426, train_policy_loss:1.9688, train_value_loss:0.6738, val:2.6448, val_policy_loss:1.9695, val_value_loss:0.6752\n",
      "Early stopping triggered! Valid loss hasn't improved for 1 evals.\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-3/best.pt (val_loss=2.6370)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-3.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 6000\n",
      "  Total actions: 103256\n",
      "  Avg trajectory length: 17.21\n",
      "Prefix Stats:\n",
      "actions=(): 6000 win=3816 loss=2167 draw=17 win1%=63.60 model-win1%=64.95\n",
      "actions=(1,): 374 win=218 loss=154 draw=2 win1%=58.29 model-win1%=57.44\n",
      "actions=(2,): 286 win=170 loss=115 draw=1 win1%=59.44 model-win1%=60.84\n",
      "actions=(3,): 412 win=257 loss=154 draw=1 win1%=62.38 model-win1%=58.39\n",
      "actions=(4,): 2293 win=1590 loss=700 draw=3 win1%=69.34 model-win1%=62.68\n",
      "actions=(4, 1): 647 win=473 loss=174 draw=0 win1%=73.11 model-win1%=62.43\n",
      "actions=(4, 2): 342 win=247 loss=94 draw=1 win1%=72.22 model-win1%=65.42\n",
      "actions=(4, 4): 301 win=188 loss=111 draw=2 win1%=62.46 model-win1%=65.11\n",
      "actions=(5,): 1032 win=656 loss=372 draw=4 win1%=63.57 model-win1%=61.96\n",
      "actions=(5, 1): 303 win=215 loss=85 draw=3 win1%=70.96 model-win1%=61.76\n",
      "actions=(6,): 1088 win=638 loss=445 draw=5 win1%=58.64 model-win1%=59.60\n",
      "actions=(6, 1): 377 win=241 loss=133 draw=3 win1%=63.93 model-win1%=59.71\n",
      "actions=(7,): 515 win=287 loss=227 draw=1 win1%=55.73 model-win1%=58.23\n",
      "\n",
      "=== Generation 4 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   1%|▏         | 29/2000 [00:44<05:04,  6.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.019 seconds, size=1000, eval-per-second=52695.57, total-batches=1000, mean-eval-per-second=89903.71, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  16%|█▌        | 319/2000 [01:40<06:56,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.016 seconds, size=1000, eval-per-second=64172.34, total-batches=2000, mean-eval-per-second=77203.77, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  36%|███▌      | 716/2000 [02:39<02:06, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.018 seconds, size=1000, eval-per-second=56895.83, total-batches=3000, mean-eval-per-second=71333.21, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  57%|█████▊    | 1150/2000 [03:46<02:59,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.011 seconds, size=852, eval-per-second=75099.76, total-batches=4000, mean-eval-per-second=60008.93, mean-time-per-batch=0.017, mean-batch-size=992.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  75%|███████▌  | 1503/2000 [04:41<01:11,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.008 seconds, size=498, eval-per-second=61657.27, total-batches=5000, mean-eval-per-second=49587.16, mean-time-per-batch=0.019, mean-batch-size=929.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  88%|████████▊ | 1767/2000 [05:15<00:42,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.005 seconds, size=235, eval-per-second=50440.69, total-batches=6000, mean-eval-per-second=43445.91, mean-time-per-batch=0.019, mean-batch-size=832.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  96%|█████████▋| 1930/2000 [05:33<00:09,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.005 seconds, size=71, eval-per-second=14011.93, total-batches=7000, mean-eval-per-second=40162.74, mean-time-per-batch=0.018, mean-batch-size=734.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1993/2000 [05:40<00:00, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=9, eval-per-second=5211.75, total-batches=8000, mean-eval-per-second=38745.47, mean-time-per-batch=0.017, mean-batch-size=647.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:41<00:00,  5.85it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 4...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.6579, train_policy_loss:1.9810, train_value_loss:0.6769, val:2.6465, val_policy_loss:1.9813, val_value_loss:0.6652\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4/best.pt\n",
      "iter 0/15/10000: loss 2.6432, policy_loss:1.9809, value_loss:0.6623, time 1.40s, iter_time: 0.00ms\n",
      "step 14: losses: train:2.6383, train_policy_loss:1.9655, train_value_loss:0.6728, val:2.6348, val_policy_loss:1.9651, val_value_loss:0.6697\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4\n",
      "step 29: losses: train:2.6231, train_policy_loss:1.9517, train_value_loss:0.6714, val:2.6243, val_policy_loss:1.9522, val_value_loss:0.6720\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-4/best.pt (val_loss=2.6243)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-4.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 8000\n",
      "  Total actions: 138916\n",
      "  Avg trajectory length: 17.36\n",
      "Prefix Stats:\n",
      "actions=(): 8000 win=5064 loss=2907 draw=29 win1%=63.30 model-win1%=62.85\n",
      "actions=(1,): 563 win=319 loss=239 draw=5 win1%=56.66 model-win1%=59.30\n",
      "actions=(2,): 422 win=255 loss=166 draw=1 win1%=60.43 model-win1%=59.21\n",
      "actions=(3,): 592 win=357 loss=232 draw=3 win1%=60.30 model-win1%=59.20\n",
      "actions=(4,): 2928 win=2035 loss=886 draw=7 win1%=69.50 model-win1%=62.70\n",
      "actions=(4, 1): 739 win=544 loss=195 draw=0 win1%=73.61 model-win1%=63.41\n",
      "actions=(4, 2): 448 win=319 loss=126 draw=3 win1%=71.21 model-win1%=63.50\n",
      "actions=(4, 6): 401 win=272 loss=128 draw=1 win1%=67.83 model-win1%=63.30\n",
      "actions=(5,): 1484 win=943 loss=536 draw=5 win1%=63.54 model-win1%=60.57\n",
      "actions=(6,): 1354 win=786 loss=561 draw=7 win1%=58.05 model-win1%=58.58\n",
      "actions=(6, 1): 427 win=265 loss=157 draw=5 win1%=62.06 model-win1%=60.72\n",
      "actions=(7,): 657 win=369 loss=287 draw=1 win1%=56.16 model-win1%=57.70\n",
      "\n",
      "=== Generation 5 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   2%|▏         | 47/2000 [00:44<02:43, 11.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.012 seconds, size=1000, eval-per-second=85977.04, total-batches=1000, mean-eval-per-second=92157.41, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  14%|█▍        | 280/2000 [01:38<07:14,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.021 seconds, size=1000, eval-per-second=47849.05, total-batches=2000, mean-eval-per-second=79669.83, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  35%|███▌      | 706/2000 [02:36<02:17,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.014 seconds, size=1000, eval-per-second=70407.31, total-batches=3000, mean-eval-per-second=73711.06, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  55%|█████▍    | 1092/2000 [03:43<02:17,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.022 seconds, size=908, eval-per-second=41866.04, total-batches=4000, mean-eval-per-second=61174.39, mean-time-per-batch=0.016, mean-batch-size=997.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  73%|███████▎  | 1463/2000 [04:43<01:04,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.065 seconds, size=537, eval-per-second=8223.86, total-batches=5000, mean-eval-per-second=49290.93, mean-time-per-batch=0.019, mean-batch-size=941.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  87%|████████▋ | 1733/2000 [05:18<00:34,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.013 seconds, size=267, eval-per-second=19901.53, total-batches=6000, mean-eval-per-second=43922.74, mean-time-per-batch=0.019, mean-batch-size=850.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  96%|█████████▌| 1916/2000 [05:36<00:07, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.005 seconds, size=83, eval-per-second=15571.98, total-batches=7000, mean-eval-per-second=40789.78, mean-time-per-batch=0.018, mean-batch-size=751.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  99%|█████████▉| 1983/2000 [05:44<00:01,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=18, eval-per-second=8475.24, total-batches=8000, mean-eval-per-second=39329.45, mean-time-per-batch=0.017, mean-batch-size=662.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:47<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.001 seconds, size=1, eval-per-second=882.64, total-batches=9000, mean-eval-per-second=38759.61, mean-time-per-batch=0.015, mean-batch-size=589.93\n",
      "Writing 2000 trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for gen 5...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.6285, train_policy_loss:1.9543, train_value_loss:0.6742, val:2.6236, val_policy_loss:1.9541, val_value_loss:0.6695\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5/best.pt\n",
      "iter 0/18/10000: loss 2.6308, policy_loss:1.9537, value_loss:0.6771, time 1.57s, iter_time: 0.00ms\n",
      "step 17: losses: train:2.6223, train_policy_loss:1.9502, train_value_loss:0.6722, val:2.6187, val_policy_loss:1.9502, val_value_loss:0.6685\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5\n",
      "step 35: losses: train:2.6154, train_policy_loss:1.9420, train_value_loss:0.6734, val:2.6113, val_policy_loss:1.9421, val_value_loss:0.6692\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-5/best.pt (val_loss=2.6113)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-5.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 175257\n",
      "  Avg trajectory length: 17.53\n",
      "Prefix Stats:\n",
      "actions=(): 10000 win=6279 loss=3678 draw=43 win1%=62.79 model-win1%=64.79\n",
      "actions=(1,): 734 win=417 loss=311 draw=6 win1%=56.81 model-win1%=61.77\n",
      "actions=(2,): 595 win=353 loss=239 draw=3 win1%=59.33 model-win1%=65.70\n",
      "actions=(3,): 772 win=458 loss=311 draw=3 win1%=59.33 model-win1%=59.29\n",
      "actions=(4,): 3656 win=2504 loss=1138 draw=14 win1%=68.49 model-win1%=69.14\n",
      "actions=(4, 1): 848 win=613 loss=234 draw=1 win1%=72.29 model-win1%=69.39\n",
      "actions=(4, 2): 552 win=390 loss=157 draw=5 win1%=70.65 model-win1%=70.56\n",
      "actions=(4, 6): 530 win=357 loss=172 draw=1 win1%=67.36 model-win1%=68.48\n",
      "actions=(5,): 1813 win=1153 loss=654 draw=6 win1%=63.60 model-win1%=65.09\n",
      "actions=(6,): 1580 win=924 loss=648 draw=8 win1%=58.48 model-win1%=60.18\n",
      "actions=(7,): 850 win=470 loss=377 draw=3 win1%=55.29 model-win1%=58.49\n",
      "\n",
      "=== Generation 6 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   3%|▎         | 62/2000 [00:46<03:30,  9.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.020 seconds, size=1000, eval-per-second=49798.21, total-batches=1000, mean-eval-per-second=83258.90, mean-time-per-batch=0.012, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  18%|█▊        | 352/2000 [01:40<02:42, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.014 seconds, size=1000, eval-per-second=70472.37, total-batches=2000, mean-eval-per-second=76386.39, mean-time-per-batch=0.013, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  38%|███▊      | 763/2000 [02:36<02:36,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.021 seconds, size=1000, eval-per-second=48398.42, total-batches=3000, mean-eval-per-second=72057.57, mean-time-per-batch=0.014, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  60%|█████▉    | 1198/2000 [03:38<01:40,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.016 seconds, size=804, eval-per-second=48751.23, total-batches=4000, mean-eval-per-second=61844.03, mean-time-per-batch=0.016, mean-batch-size=989.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  79%|███████▊  | 1574/2000 [04:23<00:30, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.022 seconds, size=426, eval-per-second=19608.59, total-batches=5000, mean-eval-per-second=52488.59, mean-time-per-batch=0.017, mean-batch-size=912.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  91%|█████████▏| 1826/2000 [04:48<00:20,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.004 seconds, size=175, eval-per-second=48871.64, total-batches=6000, mean-eval-per-second=47285.35, mean-time-per-batch=0.017, mean-batch-size=808.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  98%|█████████▊| 1965/2000 [05:01<00:02, 14.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.003 seconds, size=40, eval-per-second=12326.22, total-batches=7000, mean-eval-per-second=44204.67, mean-time-per-batch=0.016, mean-batch-size=707.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:06<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.001 seconds, size=1, eval-per-second=892.03, total-batches=8000, mean-eval-per-second=42913.68, mean-time-per-batch=0.014, mean-batch-size=620.62\n",
      "Writing 2000 trajectories...\n",
      "Training model for gen 6...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: losses: train:2.6085, train_policy_loss:1.9409, train_value_loss:0.6676, val:2.6061, val_policy_loss:1.9407, val_value_loss:0.6654\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6/best.pt\n",
      "iter 0/22/10000: loss 2.6046, policy_loss:1.9413, value_loss:0.6633, time 2.37s, iter_time: 0.00ms\n",
      "step 21: losses: train:2.6067, train_policy_loss:1.9374, train_value_loss:0.6693, val:2.6057, val_policy_loss:1.9366, val_value_loss:0.6691\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6\n",
      "step 43: losses: train:2.5972, train_policy_loss:1.9303, train_value_loss:0.6669, val:2.5964, val_policy_loss:1.9290, val_value_loss:0.6675\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-6/best.pt (val_loss=2.5964)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-6.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 12000\n",
      "  Total actions: 209805\n",
      "  Avg trajectory length: 17.48\n",
      "Prefix Stats:\n",
      "actions=(): 12000 win=7635 loss=4316 draw=49 win1%=63.62 model-win1%=63.98\n",
      "actions=(1,): 859 win=497 loss=356 draw=6 win1%=57.86 model-win1%=61.48\n",
      "actions=(2,): 775 win=472 loss=299 draw=4 win1%=60.90 model-win1%=62.95\n",
      "actions=(3,): 848 win=506 loss=339 draw=3 win1%=59.67 model-win1%=58.78\n",
      "actions=(4,): 4763 win=3288 loss=1460 draw=15 win1%=69.03 model-win1%=67.81\n",
      "actions=(4, 1): 1044 win=763 loss=280 draw=1 win1%=73.08 model-win1%=67.82\n",
      "actions=(4, 2): 685 win=485 loss=195 draw=5 win1%=70.80 model-win1%=68.14\n",
      "actions=(4, 3): 658 win=429 loss=227 draw=2 win1%=65.20 model-win1%=65.30\n",
      "actions=(4, 6): 721 win=494 loss=226 draw=1 win1%=68.52 model-win1%=66.99\n",
      "actions=(5,): 2110 win=1353 loss=747 draw=10 win1%=64.12 model-win1%=63.19\n",
      "actions=(6,): 1724 win=1003 loss=713 draw=8 win1%=58.18 model-win1%=59.56\n",
      "actions=(7,): 921 win=516 loss=402 draw=3 win1%=56.03 model-win1%=57.72\n",
      "\n",
      "=== Generation 7 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   4%|▎         | 73/2000 [00:44<04:11,  7.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=1000, eval-per-second=112459.89, total-batches=1000, mean-eval-per-second=91406.72, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  18%|█▊        | 353/2000 [01:39<03:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=1000, eval-per-second=106486.85, total-batches=2000, mean-eval-per-second=87789.98, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  40%|████      | 809/2000 [02:34<01:48, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.014 seconds, size=1000, eval-per-second=72237.12, total-batches=3000, mean-eval-per-second=86189.48, mean-time-per-batch=0.012, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  61%|██████    | 1219/2000 [03:33<03:48,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.014 seconds, size=782, eval-per-second=54911.95, total-batches=4000, mean-eval-per-second=73794.15, mean-time-per-batch=0.013, mean-batch-size=984.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  78%|███████▊  | 1563/2000 [04:18<00:34, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.010 seconds, size=436, eval-per-second=41847.06, total-batches=5000, mean-eval-per-second=60039.84, mean-time-per-batch=0.015, mean-batch-size=907.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  91%|█████████ | 1812/2000 [04:44<00:19,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=192, eval-per-second=20225.70, total-batches=6000, mean-eval-per-second=53450.47, mean-time-per-batch=0.015, mean-batch-size=807.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  98%|█████████▊| 1952/2000 [04:55<00:04, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=49, eval-per-second=22973.50, total-batches=7000, mean-eval-per-second=50945.79, mean-time-per-batch=0.014, mean-batch-size=707.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1997/2000 [04:58<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=4, eval-per-second=2462.89, total-batches=8000, mean-eval-per-second=49788.15, mean-time-per-batch=0.012, mean-batch-size=621.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [04:59<00:00,  6.68it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 7...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.5959, train_policy_loss:1.9287, train_value_loss:0.6672, val:2.5825, val_policy_loss:1.9289, val_value_loss:0.6536\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-7/best.pt\n",
      "iter 0/25/10000: loss 2.6075, policy_loss:1.9292, value_loss:0.6783, time 2.97s, iter_time: 0.00ms\n",
      "step 24: losses: train:2.5924, train_policy_loss:1.9253, train_value_loss:0.6671, val:2.5831, val_policy_loss:1.9259, val_value_loss:0.6572\n",
      "Early stopping triggered! Valid loss hasn't improved for 1 evals.\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-7/best.pt (val_loss=2.5825)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-7.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 14000\n",
      "  Total actions: 244265\n",
      "  Avg trajectory length: 17.45\n",
      "Prefix Stats:\n",
      "actions=(): 14000 win=8959 loss=4985 draw=56 win1%=63.99 model-win1%=63.98\n",
      "actions=(1,): 971 win=558 loss=407 draw=6 win1%=57.47 model-win1%=61.48\n",
      "actions=(2,): 905 win=542 loss=358 draw=5 win1%=59.89 model-win1%=62.95\n",
      "actions=(3,): 919 win=552 loss=364 draw=3 win1%=60.07 model-win1%=58.78\n",
      "actions=(4,): 5925 win=4100 loss=1805 draw=20 win1%=69.20 model-win1%=67.81\n",
      "actions=(4, 1): 1230 win=907 loss=322 draw=1 win1%=73.74 model-win1%=67.82\n",
      "actions=(4, 2): 848 win=609 loss=234 draw=5 win1%=71.82 model-win1%=68.14\n",
      "actions=(4, 3): 870 win=569 loss=298 draw=3 win1%=65.40 model-win1%=65.30\n",
      "actions=(4, 6): 924 win=632 loss=291 draw=1 win1%=68.40 model-win1%=66.99\n",
      "actions=(4, 7): 720 win=523 loss=195 draw=2 win1%=72.64 model-win1%=66.76\n",
      "actions=(5,): 2379 win=1528 loss=840 draw=11 win1%=64.23 model-win1%=63.19\n",
      "actions=(6,): 1896 win=1112 loss=776 draw=8 win1%=58.65 model-win1%=59.56\n",
      "actions=(7,): 1005 win=567 loss=435 draw=3 win1%=56.42 model-win1%=57.72\n",
      "\n",
      "=== Generation 8 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   3%|▎         | 68/2000 [00:45<02:27, 13.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.011 seconds, size=1000, eval-per-second=93022.78, total-batches=1000, mean-eval-per-second=103521.03, mean-time-per-batch=0.010, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  18%|█▊        | 354/2000 [01:39<03:36,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.015 seconds, size=1000, eval-per-second=64998.74, total-batches=2000, mean-eval-per-second=92729.67, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  39%|███▉      | 784/2000 [02:34<01:31, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.012 seconds, size=1000, eval-per-second=81679.11, total-batches=3000, mean-eval-per-second=89958.51, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  60%|██████    | 1201/2000 [03:34<01:51,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.010 seconds, size=800, eval-per-second=82390.69, total-batches=4000, mean-eval-per-second=77172.26, mean-time-per-batch=0.013, mean-batch-size=987.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  79%|███████▉  | 1581/2000 [04:15<00:36, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.059 seconds, size=420, eval-per-second=7114.42, total-batches=5000, mean-eval-per-second=66773.70, mean-time-per-batch=0.014, mean-batch-size=910.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  92%|█████████▏| 1834/2000 [04:44<00:12, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.004 seconds, size=167, eval-per-second=39886.61, total-batches=6000, mean-eval-per-second=55666.89, mean-time-per-batch=0.014, mean-batch-size=805.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  98%|█████████▊| 1965/2000 [04:54<00:02, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.003 seconds, size=36, eval-per-second=11096.86, total-batches=7000, mean-eval-per-second=52838.24, mean-time-per-batch=0.013, mean-batch-size=703.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [04:59<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=1, eval-per-second=526.59, total-batches=8000, mean-eval-per-second=50829.02, mean-time-per-batch=0.012, mean-batch-size=617.83\n",
      "Writing 2000 trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for gen 8...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.5932, train_policy_loss:1.9277, train_value_loss:0.6655, val:2.5819, val_policy_loss:1.9270, val_value_loss:0.6550\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-8/best.pt\n",
      "iter 0/29/10000: loss 2.5857, policy_loss:1.9296, value_loss:0.6561, time 2.85s, iter_time: 0.00ms\n",
      "step 28: losses: train:2.5900, train_policy_loss:1.9230, train_value_loss:0.6669, val:2.5847, val_policy_loss:1.9245, val_value_loss:0.6602\n",
      "Early stopping triggered! Valid loss hasn't improved for 1 evals.\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-8/best.pt (val_loss=2.5819)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-8.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 16000\n",
      "  Total actions: 278600\n",
      "  Avg trajectory length: 17.41\n",
      "Prefix Stats:\n",
      "actions=(): 16000 win=10242 loss=5698 draw=60 win1%=64.01 model-win1%=63.98\n",
      "actions=(1,): 1093 win=630 loss=457 draw=6 win1%=57.64 model-win1%=61.48\n",
      "actions=(2,): 1041 win=620 loss=415 draw=6 win1%=59.56 model-win1%=62.95\n",
      "actions=(3,): 1004 win=607 loss=393 draw=4 win1%=60.46 model-win1%=58.78\n",
      "actions=(4,): 7082 win=4879 loss=2182 draw=21 win1%=68.89 model-win1%=67.81\n",
      "actions=(4, 1): 1438 win=1046 loss=391 draw=1 win1%=72.74 model-win1%=67.82\n",
      "actions=(4, 2): 994 win=706 loss=282 draw=6 win1%=71.03 model-win1%=68.14\n",
      "actions=(4, 3): 1094 win=711 loss=380 draw=3 win1%=64.99 model-win1%=65.30\n",
      "actions=(4, 5): 801 win=532 loss=264 draw=5 win1%=66.42 model-win1%=66.85\n",
      "actions=(4, 6): 1121 win=763 loss=357 draw=1 win1%=68.06 model-win1%=66.99\n",
      "actions=(4, 7): 898 win=647 loss=249 draw=2 win1%=72.05 model-win1%=66.76\n",
      "actions=(5,): 2658 win=1707 loss=939 draw=12 win1%=64.22 model-win1%=63.19\n",
      "actions=(6,): 2033 win=1191 loss=834 draw=8 win1%=58.58 model-win1%=59.56\n",
      "actions=(7,): 1089 win=608 loss=478 draw=3 win1%=55.83 model-win1%=57.72\n",
      "\n",
      "=== Generation 9 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   3%|▎         | 60/2000 [00:44<05:19,  6.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=1000, eval-per-second=106484.15, total-batches=1000, mean-eval-per-second=102975.31, mean-time-per-batch=0.010, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  16%|█▌        | 324/2000 [01:38<02:11, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.015 seconds, size=1000, eval-per-second=67254.13, total-batches=2000, mean-eval-per-second=92737.04, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  38%|███▊      | 762/2000 [02:34<01:45, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.016 seconds, size=1000, eval-per-second=62594.08, total-batches=3000, mean-eval-per-second=88553.27, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  58%|█████▊    | 1166/2000 [03:32<01:39,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.012 seconds, size=838, eval-per-second=71647.81, total-batches=4000, mean-eval-per-second=80901.51, mean-time-per-batch=0.012, mean-batch-size=991.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  76%|███████▌  | 1523/2000 [04:14<00:41, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.009 seconds, size=479, eval-per-second=55107.98, total-batches=5000, mean-eval-per-second=72128.20, mean-time-per-batch=0.013, mean-batch-size=923.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  90%|█████████ | 1802/2000 [04:41<00:19, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.011 seconds, size=197, eval-per-second=18646.40, total-batches=6000, mean-eval-per-second=61336.16, mean-time-per-batch=0.013, mean-batch-size=823.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  97%|█████████▋| 1946/2000 [04:55<00:04, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=55, eval-per-second=24696.15, total-batches=7000, mean-eval-per-second=56396.47, mean-time-per-batch=0.013, mean-batch-size=722.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1990/2000 [04:59<00:01,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=8, eval-per-second=4446.65, total-batches=8000, mean-eval-per-second=54864.01, mean-time-per-batch=0.012, mean-batch-size=635.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [05:01<00:00,  6.64it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 9...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.5923, train_policy_loss:1.9270, train_value_loss:0.6654, val:2.5938, val_policy_loss:1.9267, val_value_loss:0.6671\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9/best.pt\n",
      "iter 0/32/10000: loss 2.5962, policy_loss:1.9283, value_loss:0.6679, time 2.38s, iter_time: 0.00ms\n",
      "step 31: losses: train:2.5863, train_policy_loss:1.9214, train_value_loss:0.6649, val:2.5872, val_policy_loss:1.9210, val_value_loss:0.6662\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9\n",
      "step 63: losses: train:2.5790, train_policy_loss:1.9144, train_value_loss:0.6646, val:2.5816, val_policy_loss:1.9149, val_value_loss:0.6667\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-9/best.pt (val_loss=2.5816)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-9.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 18000\n",
      "  Total actions: 314033\n",
      "  Avg trajectory length: 17.45\n",
      "Prefix Stats:\n",
      "actions=(): 18000 win=11539 loss=6388 draw=73 win1%=64.11 model-win1%=62.56\n",
      "actions=(1,): 1218 win=706 loss=506 draw=6 win1%=57.96 model-win1%=60.67\n",
      "actions=(2,): 1176 win=706 loss=463 draw=7 win1%=60.03 model-win1%=60.34\n",
      "actions=(3,): 1090 win=666 loss=420 draw=4 win1%=61.10 model-win1%=56.57\n",
      "actions=(4,): 8252 win=5684 loss=2541 draw=27 win1%=68.88 model-win1%=67.69\n",
      "actions=(4, 1): 1624 win=1190 loss=431 draw=3 win1%=73.28 model-win1%=68.65\n",
      "actions=(4, 2): 1133 win=810 loss=317 draw=6 win1%=71.49 model-win1%=68.14\n",
      "actions=(4, 3): 1350 win=863 loss=481 draw=6 win1%=63.93 model-win1%=66.03\n",
      "actions=(4, 5): 906 win=609 loss=292 draw=5 win1%=67.22 model-win1%=67.21\n",
      "actions=(4, 6): 1333 win=910 loss=421 draw=2 win1%=68.27 model-win1%=67.35\n",
      "actions=(4, 7): 1092 win=780 loss=310 draw=2 win1%=71.43 model-win1%=67.96\n",
      "actions=(5,): 2926 win=1856 loss=1055 draw=15 win1%=63.43 model-win1%=60.74\n",
      "actions=(6,): 2171 win=1267 loss=893 draw=11 win1%=58.36 model-win1%=57.09\n",
      "actions=(7,): 1167 win=654 loss=510 draw=3 win1%=56.04 model-win1%=56.78\n",
      "\n",
      "=== Generation 10 ===\n",
      "Playing 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   3%|▎         | 68/2000 [00:44<05:59,  5.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.013 seconds, size=1000, eval-per-second=74454.22, total-batches=1000, mean-eval-per-second=100950.28, mean-time-per-batch=0.010, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  18%|█▊        | 366/2000 [01:39<06:45,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.018 seconds, size=1000, eval-per-second=56513.30, total-batches=2000, mean-eval-per-second=90951.35, mean-time-per-batch=0.011, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  40%|███▉      | 791/2000 [02:34<01:54, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.011 seconds, size=1000, eval-per-second=93809.22, total-batches=3000, mean-eval-per-second=86845.31, mean-time-per-batch=0.012, mean-batch-size=1000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  62%|██████▏   | 1237/2000 [03:30<02:10,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.011 seconds, size=764, eval-per-second=67778.85, total-batches=4000, mean-eval-per-second=80116.92, mean-time-per-batch=0.012, mean-batch-size=983.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  81%|████████▏ | 1626/2000 [04:10<00:47,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.093 seconds, size=375, eval-per-second=4036.56, total-batches=5000, mean-eval-per-second=67026.20, mean-time-per-batch=0.013, mean-batch-size=899.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  94%|█████████▎| 1870/2000 [04:40<00:10, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.002 seconds, size=133, eval-per-second=55672.90, total-batches=6000, mean-eval-per-second=53744.30, mean-time-per-batch=0.015, mean-batch-size=788.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:  99%|█████████▊| 1971/2000 [04:46<00:02, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.003 seconds, size=31, eval-per-second=11376.62, total-batches=7000, mean-eval-per-second=51863.67, mean-time-per-batch=0.013, mean-batch-size=685.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|█████████▉| 1998/2000 [04:49<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.001 seconds, size=3, eval-per-second=2454.72, total-batches=8000, mean-eval-per-second=50789.09, mean-time-per-batch=0.012, mean-batch-size=601.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play: 100%|██████████| 2000/2000 [04:49<00:00,  6.90it/s]\n",
      "/Users/rodo/src/rgi3-sync/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2000 trajectories...\n",
      "Training model for gen 10...\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: losses: train:2.5752, train_policy_loss:1.9118, train_value_loss:0.6634, val:2.5719, val_policy_loss:1.9120, val_value_loss:0.6599\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10/best.pt\n",
      "iter 0/36/10000: loss 2.5862, policy_loss:1.9097, value_loss:0.6764, time 4.09s, iter_time: 0.00ms\n",
      "step 35: losses: train:2.5693, train_policy_loss:1.9072, train_value_loss:0.6621, val:2.5663, val_policy_loss:1.9073, val_value_loss:0.6589\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10\n",
      "step 71: losses: train:2.5638, train_policy_loss:1.9024, train_value_loss:0.6613, val:2.5625, val_policy_loss:1.9027, val_value_loss:0.6598\n",
      "saving best checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10/best.pt\n",
      "saving checkpoint to /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10\n",
      "Reloading best model from /Users/rodo/src/rgi3-sync/models/smoketest-e2e-v7/gen-10/best.pt (val_loss=2.5625)\n",
      "Saved model to /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v7/models/gen-10.pt\n",
      "Dataset Stats:\n",
      "  Trajectories: 20000\n",
      "  Total actions: 348146\n",
      "  Avg trajectory length: 17.41\n",
      "Prefix Stats:\n",
      "actions=(): 20000 win=12906 loss=7017 draw=77 win1%=64.53 model-win1%=64.42\n",
      "actions=(1,): 1319 win=774 loss=539 draw=6 win1%=58.68 model-win1%=62.33\n",
      "actions=(2,): 1281 win=769 loss=505 draw=7 win1%=60.03 model-win1%=62.36\n",
      "actions=(3,): 1141 win=697 loss=440 draw=4 win1%=61.09 model-win1%=59.99\n",
      "actions=(4,): 9648 win=6687 loss=2930 draw=31 win1%=69.31 model-win1%=68.88\n",
      "actions=(4, 1): 1842 win=1352 loss=487 draw=3 win1%=73.40 model-win1%=70.78\n",
      "actions=(4, 2): 1289 win=928 loss=354 draw=7 win1%=71.99 model-win1%=69.93\n",
      "actions=(4, 3): 1671 win=1084 loss=581 draw=6 win1%=64.87 model-win1%=66.79\n",
      "actions=(4, 5): 1042 win=709 loss=327 draw=6 win1%=68.04 model-win1%=68.53\n",
      "actions=(4, 6): 1595 win=1104 loss=488 draw=3 win1%=69.22 model-win1%=69.15\n",
      "actions=(4, 7): 1299 win=929 loss=367 draw=3 win1%=71.52 model-win1%=69.47\n",
      "actions=(5,): 3103 win=1968 loss=1120 draw=15 win1%=63.42 model-win1%=64.33\n",
      "actions=(6,): 2276 win=1322 loss=943 draw=11 win1%=58.08 model-win1%=59.27\n",
      "actions=(7,): 1232 win=689 loss=540 draw=3 win1%=55.93 model-win1%=58.75\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, experiment_config.num_generations+1):\n",
    "        current_model = await experiment_runner.run_generation_step_async(generation_id, current_model)\n",
    "        dataset_paths = experiment_runner.get_trajectory_paths(generation_id)\n",
    "        \n",
    "        # print stats for visibility\n",
    "        print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=current_model, game=game)\n",
    "        \n",
    "        model_dict[generation_id] = current_model\n",
    "else:\n",
    "    generation_id = experiment_config.num_generations\n",
    "    current_model = experiment_runner.load_model(generation_id)\n",
    "\n",
    "# 10m to play 2x10k generations... probabilities still very wrong.\n",
    "# Evaluation time: 0.015 seconds, size=574, eval-per-second=37837.60, total-batches=6000, mean-eval-per-second=94963.99, mean-time-per-batch=0.010, mean-batch-size=990.34\n",
    "\n",
    "# >>> log(2) + log(7) -> 2.6390573296152584\n",
    "## Model doesn't seem to improve loss at all?\n",
    "# step.   0: losses: train:2.5971, train_policy_loss:1.9146, train_value_loss:0.6825, val:2.5972, val_policy_loss:1.9147, val_value_loss:0.6825\n",
    "# step 1000: losses: train:2.6036, train_policy_loss:1.9122, train_value_loss:0.6914, val:2.6050, val_policy_loss:1.9132, val_value_loss:0.6917\n",
    "# step 2000: losses: train:2.6056, train_policy_loss:1.9119, train_value_loss:0.6937, val:2.6056, val_policy_loss:1.9123, val_value_loss:0.6933\n",
    "# iter    0/1170/5000: loss 2.5699, policy_loss:1.9129, value_loss:0.6570, time 5.18s, iter_time: 0.00ms\n",
    "# iter 1000/1170/5000: loss 2.5996, policy_loss:1.9068, value_loss:0.6928, time 1.96s, iter_time: 1957.74ms\n",
    "# iter 2339/2340/5000: loss 2.6014, policy_loss:1.9131, value_loss:0.6884, time 0.01s, iter_time: 14.61ms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2635050600.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSTOP!\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_model = await experiment_runner.run_generation_step_async(generation_id, current_model)\n",
    "\n",
    "experiment_config.experiment_name='smoketest-e2e-v3-hack'   # Use sliding window.\n",
    "experiment_config.parent_experiment_name='smoketest-e2e-v3'\n",
    "experiment_config.num_generations=41\n",
    "experiment_config.num_games_per_gen=1\n",
    "\n",
    "experiment_runner = ExperimentRunner(experiment_config, experiment_base_dir, training_args=tuned_params)\n",
    "experiment_runner.progress_bar = False\n",
    "await experiment_runner.play_generation_async(current_model, gen_id=experiment_config.num_generations, write_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model (initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "state_0 = game.initial_state()\n",
    "NUM_GENERATIONS = 5\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Parameters which will never be used for tuning.\n",
    "fixed_params = dict(\n",
    "    model_name='c4-smoketest',\n",
    "    model_version='0.1',\n",
    "    num_players = game.num_players(state_0),\n",
    "    vocab_size = action_vocab.vocab_size,\n",
    "    dataset_paths = tuple(experiment_runner.get_trajectory_paths(experiment_config.num_generations)),\n",
    "\n",
    "\n",
    "    eval_iters = 200,\n",
    "    log_interval = 1000,\n",
    "    eval_interval = 10_000,\n",
    "\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "initial_params = dict(\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_embd=8,  # tiny model\n",
    "\n",
    "    n_max_context=n_max_context,\n",
    "    batch_size = 32,\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    max_iters=100,\n",
    "    max_epochs=1_000_000, # Make max_epoch high, rely on max_iters to stop.\n",
    "        \n",
    "    learning_rate = LEARNING_RATE,    \n",
    "    decay_lr = True,  # whether to decay the learning rate\n",
    "    lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    "\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    grad_clip = 1.0,  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = \"float16\",\n",
    "\n",
    "    dropout = 0.0,\n",
    "    bias = False,  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    last_file = None,   # Used in tuning key only.\n",
    ")\n",
    "\n",
    "tune_options = dict(\n",
    "    n_layer = [1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 32],\n",
    "    # n_head = [1, 2, 4, 8, 16, 32],   # Needs to be calcualted to ensure n_embed % n_head == 0\n",
    "    n_embd = [8, 16, 32, 64, 128, 256, 512, 1024, 2048],\n",
    "\n",
    "    n_max_context = [initial_params['n_max_context']],\n",
    "    batch_size = [16, 32, 64, 128, 256, 512, 1024],\n",
    "    gradient_accumulation_steps = [1],  # TODO: We only support 1 for now. This fails is we don't have an exact multiple of the batch size per epoch.\n",
    "\n",
    "    max_iters = [100, 300, 1_000, 3_000, 5_000, 10_000, 30_000, 100_000, 300_000],\n",
    "    max_epochs = [1_000_000], # Make max_epoch high, rely on max_iters to stop.\n",
    " \n",
    "    learning_rate = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "    decay_lr = [False, True],\n",
    "\n",
    "    # TODO: What is a sensible range here?\n",
    "    beta1 = [0.90, 0.95, 0.99],\n",
    "    beta2 = [0.95, 0.98, 0.99],\n",
    "\n",
    "    weight_decay = [0.01, 0.05, 0.1, 0.2],\n",
    "    grad_clip = [0,0, 1.0],  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = [\"bfloat16\", \"float16\"],\n",
    "    dropout = [0.0, 0.01, 0.02, 0.05, 0.1],\n",
    "    bias = [True, False],    \n",
    ")\n",
    "\n",
    "_n_head_options = [1, 2, 4, 8, 16, 32]\n",
    "computed_tune_options = dict(\n",
    "    min_lr = lambda opt: [opt['learning_rate'] / 10],\n",
    "    lr_decay_iters = lambda opt: [opt['max_iters']],\n",
    "    warmup_iters = lambda opt: [x for x in [0, 100, 500, 1000] if x < opt['lr_decay_iters']] if opt['decay_lr'] else [0],\n",
    "    n_head = lambda opt: [n for n in _n_head_options if opt['n_embd'] % n == 0],\n",
    "    last_file = lambda opt: [str(opt['dataset_paths'][-1])],\n",
    ")\n",
    "\n",
    "TUNER_VERSION = \"0.0.6-smoketest\"\n",
    "\n",
    "from rgi.rgizero.models.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=1.00)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.1)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0001)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Skip this...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "\n",
    "tuner_result = tuner.autotune_smart()\n",
    "# print(f'tuner_result={tuner_result}')\n",
    "\n",
    "best_params = tuner.best_params.copy()\n",
    "## Recalculating with best_params = {'batch_size': 512, 'beta1': 0.9, 'beta2': 0.99, 'bias': False, 'decay_lr': True, 'dropout': 0.0, 'dtype': 'float16', 'grad_clip': 1.0, 'gradient_accumulation_steps': 1, 'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20', 'learning_rate': 0.01, 'lr_decay_iters': 5000, 'max_epochs': 1000000, 'max_iters': 30000, 'min_lr': 0.001, 'n_embd': 64, 'n_head': 8, 'n_layer': 4, 'n_max_context': 44, 'warmup_iters': 1000, 'weight_decay': 0.2, 'model_name': 'c4-smoketest', 'model_version': '0.1', 'num_players': 2, 'vocab_size': 8, 'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')), 'eval_iters': 200, 'log_interval': 1000, 'eval_interval': 10000, 'device': 'mps'}\n",
    "## {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "\n",
    "best_params['max_iters'] = 30_000 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "# best_params['max_iters'] = 10_000 #{'train': 2.2972692108154296, 'train_policy_loss': 1.7478227978944778, 'train_value_loss': 0.5494464221596718, 'val': 2.3170768583522126, 'val_policy_loss': 1.7486604136579178, 'val_value_loss': 0.5684164271635168, 'elapsed': 529.6186480522156, 'param_hash': 'c81f68b9b00650b83a229a76b93d77d51f7bd1af43148f6c1af75e8f562f2670'}\n",
    "# best_params['max_iters'] = 5000 # {'train': 2.3059648656845093, 'train_policy_loss': 1.749609624147415, 'train_value_loss': 0.5563552376627922, 'val': 2.319357395172119, 'val_policy_loss': 1.7505432332263273, 'val_value_loss': 0.5688141549334806, 'elapsed': 267.94705629348755, 'param_hash': 'a1194e1f289d35c51e5a0f01692109358f947530b50dd54364f01baefdb6bedf'}\n",
    "# best_params['max_iters'] = 3000 # {'train': 2.3192384707927705, 'train_policy_loss': 1.752758464217186, 'train_value_loss': 0.5664800041913987, 'val': 2.326388120651245, 'val_policy_loss': 1.7533490412375505, 'val_value_loss': 0.5730390969444724, 'elapsed': 172.16889691352844, 'param_hash': '9e0204edd0b23fd54025b0c4de66c870a268b95197e0e47c4460ec03875b3dcb'}\n",
    "\n",
    "# best_params['learning_rate'] = 0.01 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "best_params['learning_rate'] = 0.0005 # {'train': 2.2302739822864535, 'train_policy_loss': 1.7535288149118424, 'train_value_loss': 0.47674516707658765, 'val': 2.4826266765594482, 'val_policy_loss': 1.755559900227715, 'val_value_loss': 0.7270667658132666, 'elapsed': 1617.1719007492065, 'param_hash': '4364fb3cfa7a3b4d33fe30f70ec9957a0a14bc7d9a195b5a2de2247d2a72a6d1'}\n",
    "\n",
    "print(f'\\n## Recalculating with best_params = {best_params}')\n",
    "best_params = tuner._recalculate_tunable_params(best_params)\n",
    "best_model = tuner.get_model_for_params(best_params)\n",
    "print(tuner.train_and_compute_loss(best_params, reload_model=True)[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = experiment_runner.get_trajectory_paths(NUM_GENERATIONS)\n",
    "print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=best_model, game=game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        # dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:5].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:6].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:7].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:8].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:9].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:10].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=n_max_context, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):    \n",
    "    list(dd.items())[10][1]['*'].sum() > 100\n",
    "    top_k = sorted(dd.items(), key=lambda kv: kv[1]['*'].sum(), reverse=True)[:20]\n",
    "    top_k_keys = sorted(k for k, v in top_k)\n",
    "    \n",
    "    prefix_list = top_k_keys\n",
    "\n",
    "    # prefix_list = [\n",
    "    #     (0,), \n",
    "    #     (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "    #     (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "    #     (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7),\n",
    "    # ]\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            if gen == '*':\n",
    "                print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        # # assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tournament to calcualte ELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from contextlib import asynccontextmanager\n",
    "from rgi.rgizero.tournament import Tournament\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_player_factory(model, simulations, game, device, block_size, action_vocab, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a shared evaluator and returns a factory function that produces \n",
    "    new AlphazeroPlayer instances using that shared evaluator.\n",
    "    \"\"\"\n",
    "    # 1. Setup the shared evaluator\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(\n",
    "        model, \n",
    "        device=device, \n",
    "        block_size=n_max_context, \n",
    "        vocab=action_vocab\n",
    "    )\n",
    "    async_evaluator = AsyncNetworkEvaluator(\n",
    "        base_evaluator=serial_evaluator, \n",
    "        max_batch_size=max_batch_size, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 2. Start the evaluator background task\n",
    "    await async_evaluator.start()\n",
    "    \n",
    "    try:\n",
    "        # 3. Define the factory. This is called by Tournament for every game.\n",
    "        # It creates a NEW player instance but uses the SHARED async_evaluator.\n",
    "        def player_factory():\n",
    "            # Create a fresh RNG for each game/player instance\n",
    "            rng = np.random.default_rng(np.random.randint(0, 2**31))\n",
    "            return AlphazeroPlayer(\n",
    "                game, \n",
    "                async_evaluator, \n",
    "                rng=rng, \n",
    "                add_noise=True, \n",
    "                simulations=simulations\n",
    "            )\n",
    "            \n",
    "        yield player_factory\n",
    "        \n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        await async_evaluator.stop()\n",
    "\n",
    "async def run_tournament_async():\n",
    "    # Use async with to manage the lifecycle of the evaluators\n",
    "    async with (\n",
    "        # create_player_factory(model_dict[0], 100, game, device, block_size, action_vocab, 10) as factory_gen0_100,\n",
    "        # create_player_factory(model_dict[1], 100, game, device, block_size, action_vocab, 10) as factory_gen1_100,\n",
    "        # create_player_factory(model_dict[2], 100, game, device, block_size, action_vocab, 10) as factory_gen2_100,\n",
    "        # create_player_factory(model_dict[3], 100, game, device, block_size, action_vocab, 10) as factory_gen3_100,\n",
    "        # create_player_factory(model_dict[4], 100, game, device, block_size, action_vocab, 10) as factory_gen4_100,\n",
    "        # create_player_factory(model_dict[5], 100, game, device, block_size, action_vocab, 10) as factory_gen5_100,\n",
    "        # create_player_factory(model_dict[10], 100, game, device, block_size, action_vocab, 10) as factory_gen6_100,\n",
    "        # create_player_factory(model_dict[15], 100, game, device, block_size, action_vocab, 10) as factory_gen7_100,\n",
    "        # create_player_factory(model_dict[20], 100, game, device, block_size, action_vocab, 10) as factory_gen8_100,\n",
    "\n",
    "        create_player_factory(model_dict[0], 200, game, device, block_size, action_vocab, 10) as factory_gen0_200,\n",
    "        #create_player_factory(model_dict[1], 200, game, device, block_size, action_vocab, 10) as factory_gen1_200,\n",
    "        #create_player_factory(model_dict[2], 200, game, device, block_size, action_vocab, 10) as factory_gen2_200,\n",
    "        #create_player_factory(model_dict[3], 200, game, device, block_size, action_vocab, 10) as factory_gen3_200,\n",
    "        #create_player_factory(model_dict[4], 200, game, device, block_size, action_vocab, 10) as factory_gen4_200,\n",
    "        create_player_factory(model_dict[5], 200, game, device, block_size, action_vocab, 10) as factory_gen5_200,\n",
    "        #create_player_factory(model_dict[10], 200, game, device, block_size, action_vocab, 10) as factory_gen10_200,\n",
    "        #create_player_factory(model_dict[15], 200, game, device, block_size, action_vocab, 10) as factory_gen15_200,\n",
    "        create_player_factory(model_dict[20], 200, game, device, block_size, action_vocab, 10) as factory_gen20_200,\n",
    "        ):\n",
    "        \n",
    "        # The dictionary now maps names to FACTORIES (Callables), not Player instances\n",
    "        player_factories = {\n",
    "            # \"factory_gen0_100\": factory_gen0_100,\n",
    "            # \"factory_gen1_100\": factory_gen1_100,\n",
    "            # \"factory_gen2_100\": factory_gen2_100,\n",
    "            # \"factory_gen3_100\": factory_gen3_100,\n",
    "            # \"factory_gen4_100\": factory_gen4_100,\n",
    "            # \"factory_gen5_100\": factory_gen5_100,\n",
    "            # \"factory_gen6_100\": factory_gen6_100,\n",
    "            # \"factory_gen7_100\": factory_gen7_100,\n",
    "\n",
    "            \"factory_gen0_200\": factory_gen0_200,\n",
    "            #\"factory_gen1_200\": factory_gen1_200,\n",
    "            #\"factory_gen2_200\": factory_gen2_200,\n",
    "            #\"factory_gen3_200\": factory_gen3_200,\n",
    "            #\"factory_gen4_200\": factory_gen4_200,\n",
    "            \"factory_gen5_200\": factory_gen5_200,\n",
    "            #\"factory_gen10_200\": factory_gen10_200,\n",
    "            #\"factory_gen15_200\": factory_gen15_200,\n",
    "            \"factory_gen20_200\": factory_gen20_200,\n",
    "        }\n",
    "        \n",
    "        tournament = Tournament(game, player_factories, initial_elo=1000)\n",
    "        \n",
    "        print(\"Running tournament...\")\n",
    "        # await tournament.run(num_games=1_000, concurrent_games=2000)\n",
    "        await tournament.run(num_games=100, concurrent_games=2000)\n",
    "        tournament.print_standings()\n",
    "\n",
    "# RUN_TOURNAMENT = True\n",
    "if RUN_TOURNAMENT:\n",
    "    await run_tournament_async()\n",
    "\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 10000/10000 [1:25:59<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen6_200     1140.5     1247     827-419-1      \n",
    "# 2     factory_gen2_200     1100.1     1251     693-554-4      \n",
    "# 3     factory_gen5_100     1074.4     1251     598-652-1      \n",
    "# 4     factory_gen3_200     1029.1     1252     674-573-5      \n",
    "# 5     factory_gen4_200     1027.0     1248     711-536-1      \n",
    "# 6     factory_gen0_200     1020.0     1254     444-810-0      \n",
    "# 7     factory_gen5_200     990.2      1248     742-502-4      \n",
    "# 8     factory_gen7_100     987.5      1250     650-597-3      \n",
    "# 9     factory_gen7_200     979.2      1248     768-476-4      \n",
    "# 10    factory_gen2_100     974.0      1249     522-723-4      \n",
    "# 11    factory_gen6_100     966.6      1248     684-564-0      \n",
    "# 12    factory_gen4_100     964.2      1251     557-693-1      \n",
    "# 13    factory_gen1_100     962.5      1252     547-705-0      \n",
    "# 14    factory_gen3_100     947.0      1251     528-723-0      \n",
    "# 15    factory_gen1_200     941.1      1252     630-620-2      \n",
    "# 16    factory_gen0_100     896.5      1248     410-838-0     \n",
    "\n",
    "\n",
    "## 20 generations.\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 1000/1000 [08:35<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen10_200    1114.2     333      212-120-1      \n",
    "# 2     factory_gen2_200     1032.6     333      190-141-2      \n",
    "# 3     factory_gen1_200     1003.9     334      159-175-0      \n",
    "# 4     factory_gen20_200    1000.9     335      171-164-0      \n",
    "# 5     factory_gen5_200     974.6      331      183-146-2      \n",
    "# 6     factory_gen0_200     873.8      334      82-251-1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model (continued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune_smart()\n",
    "\n",
    "# Using initial model as baseline.\n",
    "# ## Initial Model, loss=2.1298508644104004 elapsed=171.78943705558777s\n",
    "# ## Searching generation 0 with 22 candidates, including ['bias: False -> True', 'learning_rate: 0.005 -> 0.002', 'learning_rate: 0.005 -> 0.002', 'dtype: bfloat16 -> float16', 'weight_decay: 0.1 -> 0.2']\n",
    "# ## improved: False, loss=2.1332 elapsed=178.64s, mutation bias: False -> True\n",
    "# ## improved: False, loss=2.1395 elapsed=172.03s, mutation learning_rate: 0.005 -> 0.002\n",
    "# ## improved: False, loss=2.1395 elapsed=172.03s, mutation learning_rate: 0.005 -> 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune_smart()\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "tiny_config: TransformerConfig = TransformerConfig(n_max_context=100, n_layer=2, n_head=2, n_embd=8)\n",
    "tiny_model = ActionHistoryTransformer(config=tiny_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0))\n",
    "tiny_model.to(device)\n",
    "tiny_evaluator = ActionHistoryTransformerEvaluator(tiny_model, device=device, block_size=5, vocab=action_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0001)\n",
    "tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0)\n",
    "tuner.autotune_smart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "best_model = tuner.load_best_model()\n",
    "compare_model_vs_data(best_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(f'tuner.best_loss={tuner.best_loss}')\n",
    "print(f'tuner.best_loss_elapsed={int(tuner.best_loss_elapsed)//60}m{tuner.best_loss_elapsed%60:.0f}s')\n",
    "pprint(tuner.best_params)\n",
    "# best_params = tuner.initial_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print tuner stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "# print stats based on cached results.\n",
    "tuner_stats = tuner.print_hparam_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(k,v['mean_val_delta']) for (k,v) in sorted(tuner_stats.items(), key=lambda x: x[1]['mean_val_delta'], reverse=True)]\n",
    "\n",
    "for x in  sorted([(v['mean_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True): print(x)\n",
    "# sorted([(v['mean_val_delta'], k) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted([(v['std_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['std_val_delta'])], reverse=True): print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"xxx STOP HERE xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
