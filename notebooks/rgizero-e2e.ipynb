{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"tiny\"  # \"tiny\" or \"small\" or \"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "RUN_GENERATIONS = True\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 10_000\n",
    "MAX_TRAINING_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 2048\n",
    "MAX_TRAINING_ITERS = 1_000_000 // TRAIN_BATCH_SIZE\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 11\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MODEL_SIZE = \"small\"\n",
    "MAX_TRAINING_ITERS = 100_000_000 // TRAIN_BATCH_SIZE\n",
    "MAX_TRAINING_EPOCHS = 10_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        win2_pct = 100 * counts[2] / total if total > 0 else 0\n",
    "        draw_pct = 100 * counts[None] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}, win[2]={win2_pct:.2f}% draw={draw_pct:.2f}%\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "# TODO: Use MODEL_SIZE!\n",
    "# model_config = model_config_dict[\"small\"] # Override to see if we can fit better.\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer], max_concurrent_games: int = 1000):\n",
    "    sem = asyncio.Semaphore(max_concurrent_games)\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        async with sem:\n",
    "            t0 = time.time()\n",
    "            player = player_factory()\n",
    "            game_result = await play_game_async(game, [player, player])\n",
    "            t1 = time.time()\n",
    "            game_result['time'] = t1 - t0\n",
    "            return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS, max_concurrent_games=1024):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=max_concurrent_games, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory, max_concurrent_games=max_concurrent_games)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}, draw={100*winner_stats[None]/sum(winner_stats.values()):.2f}%\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}, win[2]={100*counts[2]/sum(counts.values()):.2f}% draw={100*counts[None]/sum(counts.values()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = MAX_TRAINING_EPOCHS,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "\n",
    "    learning_rate = LEARNING_RATE,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = MAX_TRAINING_ITERS,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n",
    "        device=device, workers=num_workers, shuffle=True)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        # TODO: We're continuing training on a previosu model here ... should we train a new model from scratch?\n",
    "        print(train_config)\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 140493\n",
      "  Avg trajectory length: 14.05\n",
      "  Trajectory length - min: 7, max: 42, mean: 14.05\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=61.93% win[2]=38.06%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=1564 win[1]=51.92% counts=Counter({1: 812, 2: 752}), win[2]=48.08% draw=0.00%\n",
      "  a=2: n=1344 win[1]=57.37% counts=Counter({1: 771, 2: 573}), win[2]=42.63% draw=0.00%\n",
      "  a=3: n=1297 win[1]=65.23% counts=Counter({1: 846, 2: 451}), win[2]=34.77% draw=0.00%\n",
      "  a=4: n=1493 win[1]=76.36% counts=Counter({1: 1140, 2: 353}), win[2]=23.64% draw=0.00%\n",
      "  a=5: n=1525 win[1]=66.49% counts=Counter({1: 1014, 2: 511}), win[2]=33.51% draw=0.00%\n",
      "  a=6: n=1401 win[1]=62.03% counts=Counter({1: 869, 2: 532}), win[2]=37.97% draw=0.00%\n",
      "  a=7: n=1376 win[1]=53.85% counts=Counter({1: 741, 2: 634, None: 1}), win[2]=46.08% draw=0.07%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-2\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 136161\n",
      "  Avg trajectory length: 13.62\n",
      "  Trajectory length - min: 7, max: 38, mean: 13.62\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=46.45% win[2]=53.55%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=933 win[1]=34.19% counts=Counter({2: 614, 1: 319}), win[2]=65.81% draw=0.00%\n",
      "  a=2: n=328 win[1]=20.12% counts=Counter({2: 262, 1: 66}), win[2]=79.88% draw=0.00%\n",
      "  a=3: n=1647 win[1]=43.90% counts=Counter({2: 924, 1: 723}), win[2]=56.10% draw=0.00%\n",
      "  a=4: n=4260 win[1]=57.54% counts=Counter({1: 2451, 2: 1809}), win[2]=42.46% draw=0.00%\n",
      "  a=5: n=1508 win[1]=41.71% counts=Counter({2: 879, 1: 629}), win[2]=58.29% draw=0.00%\n",
      "  a=6: n=330 win[1]=26.06% counts=Counter({2: 244, 1: 86}), win[2]=73.94% draw=0.00%\n",
      "  a=7: n=994 win[1]=37.32% counts=Counter({2: 623, 1: 371}), win[2]=62.68% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-2.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 141481\n",
      "  Avg trajectory length: 14.15\n",
      "  Trajectory length - min: 7, max: 42, mean: 14.15\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=52.01% win[2]=47.96%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=43.62% counts=Counter({2: 53, 1: 41}), win[2]=56.38% draw=0.00%\n",
      "  a=2: n=109 win[1]=32.11% counts=Counter({2: 74, 1: 35}), win[2]=67.89% draw=0.00%\n",
      "  a=3: n=103 win[1]=22.33% counts=Counter({2: 80, 1: 23}), win[2]=77.67% draw=0.00%\n",
      "  a=4: n=9218 win[1]=53.38% counts=Counter({1: 4921, 2: 4295, None: 2}), win[2]=46.59% draw=0.02%\n",
      "  a=5: n=132 win[1]=50.00% counts=Counter({1: 66, 2: 65, None: 1}), win[2]=49.24% draw=0.76%\n",
      "  a=6: n=148 win[1]=34.46% counts=Counter({2: 97, 1: 51}), win[2]=65.54% draw=0.00%\n",
      "  a=7: n=196 win[1]=32.65% counts=Counter({2: 132, 1: 64}), win[2]=67.35% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-3.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-4\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 139426\n",
      "  Avg trajectory length: 13.94\n",
      "  Trajectory length - min: 7, max: 39, mean: 13.94\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=45.70% win[2]=54.30%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=38.85% counts=Counter({2: 85, 1: 54}), win[2]=61.15% draw=0.00%\n",
      "  a=2: n=7686 win[1]=43.98% counts=Counter({2: 4306, 1: 3380}), win[2]=56.02% draw=0.00%\n",
      "  a=3: n=349 win[1]=42.69% counts=Counter({2: 200, 1: 149}), win[2]=57.31% draw=0.00%\n",
      "  a=4: n=882 win[1]=64.06% counts=Counter({1: 565, 2: 317}), win[2]=35.94% draw=0.00%\n",
      "  a=5: n=319 win[1]=57.68% counts=Counter({1: 184, 2: 135}), win[2]=42.32% draw=0.00%\n",
      "  a=6: n= 94 win[1]=39.36% counts=Counter({2: 57, 1: 37}), win[2]=60.64% draw=0.00%\n",
      "  a=7: n=531 win[1]=37.85% counts=Counter({2: 330, 1: 201}), win[2]=62.15% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-4.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-5\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 137305\n",
      "  Avg trajectory length: 13.73\n",
      "  Trajectory length - min: 7, max: 42, mean: 13.73\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=58.39% win[2]=41.61%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=462 win[1]=37.66% counts=Counter({2: 288, 1: 174}), win[2]=62.34% draw=0.00%\n",
      "  a=2: n=1005 win[1]=54.63% counts=Counter({1: 549, 2: 456}), win[2]=45.37% draw=0.00%\n",
      "  a=3: n=1499 win[1]=48.10% counts=Counter({2: 778, 1: 721}), win[2]=51.90% draw=0.00%\n",
      "  a=4: n=5503 win[1]=67.85% counts=Counter({1: 3734, 2: 1769}), win[2]=32.15% draw=0.00%\n",
      "  a=5: n=864 win[1]=41.32% counts=Counter({2: 507, 1: 357}), win[2]=58.68% draw=0.00%\n",
      "  a=6: n=191 win[1]=65.45% counts=Counter({1: 125, 2: 66}), win[2]=34.55% draw=0.00%\n",
      "  a=7: n=476 win[1]=37.61% counts=Counter({2: 297, 1: 179}), win[2]=62.39% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-5.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 6 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-6\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 123991\n",
      "  Avg trajectory length: 12.40\n",
      "  Trajectory length - min: 7, max: 39, mean: 12.40\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=67.09% win[2]=32.91%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=379 win[1]=42.48% counts=Counter({2: 218, 1: 161}), win[2]=57.52% draw=0.00%\n",
      "  a=2: n=237 win[1]=37.97% counts=Counter({2: 147, 1: 90}), win[2]=62.03% draw=0.00%\n",
      "  a=3: n=2182 win[1]=78.92% counts=Counter({1: 1722, 2: 460}), win[2]=21.08% draw=0.00%\n",
      "  a=4: n=2659 win[1]=78.19% counts=Counter({1: 2079, 2: 580}), win[2]=21.81% draw=0.00%\n",
      "  a=5: n=1111 win[1]=52.57% counts=Counter({1: 584, 2: 527}), win[2]=47.43% draw=0.00%\n",
      "  a=6: n=3288 win[1]=60.64% counts=Counter({1: 1994, 2: 1294}), win[2]=39.36% draw=0.00%\n",
      "  a=7: n=144 win[1]=54.86% counts=Counter({1: 79, 2: 65}), win[2]=45.14% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-6.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 7 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-7\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 129576\n",
      "  Avg trajectory length: 12.96\n",
      "  Trajectory length - min: 7, max: 42, mean: 12.96\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=64.54% win[2]=35.45%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=306 win[1]=29.74% counts=Counter({2: 215, 1: 91}), win[2]=70.26% draw=0.00%\n",
      "  a=2: n=797 win[1]=40.65% counts=Counter({2: 473, 1: 324}), win[2]=59.35% draw=0.00%\n",
      "  a=3: n=258 win[1]=55.43% counts=Counter({1: 143, 2: 115}), win[2]=44.57% draw=0.00%\n",
      "  a=4: n=7409 win[1]=70.45% counts=Counter({1: 5220, 2: 2188, None: 1}), win[2]=29.53% draw=0.01%\n",
      "  a=5: n=754 win[1]=51.06% counts=Counter({1: 385, 2: 369}), win[2]=48.94% draw=0.00%\n",
      "  a=6: n=237 win[1]=55.27% counts=Counter({1: 131, 2: 106}), win[2]=44.73% draw=0.00%\n",
      "  a=7: n=239 win[1]=66.95% counts=Counter({1: 160, 2: 79}), win[2]=33.05% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-7.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 8 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [14:41<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=50.73% win[2]=49.27%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 40, mean: 13.29\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=44.60% counts=Counter({2: 77, 1: 62}), win[2]=55.40% draw=0.00%\n",
      "  a=2: n=2769 win[1]=42.58% counts=Counter({2: 1590, 1: 1179}), win[2]=57.42% draw=0.00%\n",
      "  a=3: n=379 win[1]=50.66% counts=Counter({1: 192, 2: 187}), win[2]=49.34% draw=0.00%\n",
      "  a=4: n=2369 win[1]=74.00% counts=Counter({1: 1753, 2: 616}), win[2]=26.00% draw=0.00%\n",
      "  a=5: n=1463 win[1]=64.18% counts=Counter({1: 939, 2: 524}), win[2]=35.82% draw=0.00%\n",
      "  a=6: n=415 win[1]=57.35% counts=Counter({1: 238, 2: 177}), win[2]=42.65% draw=0.00%\n",
      "  a=7: n=2466 win[1]=28.79% counts=Counter({2: 1756, 1: 710}), win[2]=71.21% draw=0.00%\n",
      "Training model on gen-8\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.2365, val loss 2.2371\n",
      "iter 0/40/48828: loss 2.2367, time 4830.89ms\n",
      "iter 100/120/48828: loss 2.5334, time 154.24ms\n",
      "iter 200/240/48828: loss 2.5284, time 166.03ms\n",
      "iter 300/320/48828: loss 2.5439, time 205.29ms\n",
      "iter 400/440/48828: loss 2.5387, time 99.67ms\n",
      "iter 500/520/48828: loss 2.5307, time 107.28ms\n",
      "iter 600/640/48828: loss 2.5332, time 214.77ms\n",
      "iter 700/720/48828: loss 2.5229, time 103.85ms\n",
      "iter 800/840/48828: loss 2.5409, time 272.24ms\n",
      "iter 900/920/48828: loss 2.5395, time 109.76ms\n",
      "step 1000: train loss 2.5176, val loss 2.5179\n",
      "iter 1000/1040/48828: loss 2.5168, time 4781.57ms\n",
      "iter 1100/1120/48828: loss 2.5529, time 284.42ms\n",
      "iter 1200/1240/48828: loss 2.5565, time 101.16ms\n",
      "iter 1300/1320/48828: loss 2.5311, time 101.20ms\n",
      "iter 1400/1440/48828: loss 2.5420, time 109.12ms\n",
      "iter 1500/1520/48828: loss 2.5555, time 104.01ms\n",
      "iter 1600/1640/48828: loss 2.5504, time 102.30ms\n",
      "iter 1700/1720/48828: loss 2.5737, time 107.12ms\n",
      "iter 1800/1840/48828: loss 2.5434, time 103.31ms\n",
      "iter 1900/1920/48828: loss 2.5381, time 104.53ms\n",
      "step 2000: train loss 2.5482, val loss 2.5477\n",
      "iter 2000/2040/48828: loss 2.5465, time 4119.67ms\n",
      "iter 2100/2120/48828: loss 2.5445, time 103.37ms\n",
      "iter 2200/2240/48828: loss 2.5388, time 99.99ms\n",
      "iter 2300/2320/48828: loss 2.5343, time 103.16ms\n",
      "iter 2400/2440/48828: loss 2.5297, time 101.46ms\n",
      "iter 2500/2520/48828: loss 2.5408, time 101.33ms\n",
      "iter 2600/2640/48828: loss 2.5352, time 100.88ms\n",
      "iter 2700/2720/48828: loss 2.5339, time 101.51ms\n",
      "iter 2800/2840/48828: loss 2.5395, time 101.40ms\n",
      "iter 2900/2920/48828: loss 2.5328, time 103.61ms\n",
      "step 3000: train loss 2.5295, val loss 2.5289\n",
      "iter 3000/3040/48828: loss 2.5272, time 4368.46ms\n",
      "iter 3100/3120/48828: loss 2.5299, time 102.30ms\n",
      "iter 3200/3240/48828: loss 2.5327, time 107.99ms\n",
      "iter 3300/3320/48828: loss 2.5306, time 104.25ms\n",
      "iter 3400/3440/48828: loss 2.5247, time 96.71ms\n",
      "iter 3500/3520/48828: loss 2.5417, time 101.64ms\n",
      "iter 3600/3640/48828: loss 2.5282, time 101.13ms\n",
      "iter 3700/3720/48828: loss 2.5284, time 101.08ms\n",
      "iter 3800/3840/48828: loss 2.5470, time 102.84ms\n",
      "iter 3900/3920/48828: loss 2.5349, time 106.18ms\n",
      "step 4000: train loss 2.5401, val loss 2.5402\n",
      "iter 4000/4040/48828: loss 2.5423, time 3852.73ms\n",
      "iter 4100/4120/48828: loss 2.5397, time 102.65ms\n",
      "iter 4200/4240/48828: loss 2.5283, time 102.61ms\n",
      "iter 4300/4320/48828: loss 2.5394, time 102.60ms\n",
      "iter 4400/4440/48828: loss 2.5468, time 97.16ms\n",
      "iter 4500/4520/48828: loss 2.5318, time 102.55ms\n",
      "iter 4600/4640/48828: loss 2.5679, time 97.56ms\n",
      "iter 4700/4720/48828: loss 2.5466, time 104.29ms\n",
      "iter 4800/4840/48828: loss 2.5398, time 222.84ms\n",
      "iter 4900/4920/48828: loss 2.5463, time 104.59ms\n",
      "step 5000: train loss 2.5425, val loss 2.5429\n",
      "iter 5000/5040/48828: loss 2.5418, time 3463.76ms\n",
      "iter 5100/5120/48828: loss 2.5424, time 103.22ms\n",
      "iter 5200/5240/48828: loss 2.5426, time 100.41ms\n",
      "iter 5300/5320/48828: loss 2.5477, time 102.28ms\n",
      "iter 5400/5440/48828: loss 2.5385, time 98.47ms\n",
      "iter 5500/5520/48828: loss 2.5341, time 102.51ms\n",
      "iter 5600/5640/48828: loss 2.5434, time 100.54ms\n",
      "iter 5700/5720/48828: loss 2.5399, time 103.48ms\n",
      "iter 5800/5840/48828: loss 2.5443, time 99.45ms\n",
      "iter 5900/5920/48828: loss 2.5607, time 236.54ms\n",
      "step 6000: train loss 2.5552, val loss 2.5554\n",
      "iter 6000/6040/48828: loss 2.5555, time 3551.05ms\n",
      "iter 6100/6120/48828: loss 2.5466, time 102.35ms\n",
      "iter 6200/6240/48828: loss 2.5414, time 101.43ms\n",
      "iter 6300/6320/48828: loss 2.5270, time 102.35ms\n",
      "iter 6400/6440/48828: loss 2.5299, time 110.92ms\n",
      "iter 6500/6520/48828: loss 2.5408, time 103.24ms\n",
      "iter 6600/6640/48828: loss 2.5162, time 105.19ms\n",
      "iter 6700/6720/48828: loss 2.5287, time 103.47ms\n",
      "iter 6800/6840/48828: loss 2.5606, time 96.84ms\n",
      "iter 6900/6920/48828: loss 2.5360, time 102.43ms\n",
      "step 7000: train loss 2.5479, val loss 2.5485\n",
      "iter 7000/7040/48828: loss 2.5439, time 3494.59ms\n",
      "iter 7100/7120/48828: loss 2.5389, time 105.43ms\n",
      "iter 7200/7240/48828: loss 2.5382, time 95.57ms\n",
      "iter 7300/7320/48828: loss 2.5198, time 103.13ms\n",
      "iter 7400/7440/48828: loss 2.5495, time 99.33ms\n",
      "iter 7500/7520/48828: loss 2.5369, time 104.09ms\n",
      "iter 7600/7640/48828: loss 2.5377, time 97.21ms\n",
      "iter 7700/7720/48828: loss 2.5421, time 102.42ms\n",
      "iter 7800/7840/48828: loss 2.5311, time 97.79ms\n",
      "iter 7900/7920/48828: loss 2.5521, time 103.90ms\n",
      "step 8000: train loss 2.5511, val loss 2.5509\n",
      "iter 8000/8040/48828: loss 2.5529, time 2951.54ms\n",
      "iter 8100/8120/48828: loss 2.5287, time 103.23ms\n",
      "iter 8200/8240/48828: loss 2.5482, time 103.36ms\n",
      "iter 8300/8320/48828: loss 2.5275, time 552.45ms\n",
      "iter 8400/8440/48828: loss 2.5279, time 98.13ms\n",
      "iter 8500/8520/48828: loss 2.5341, time 105.21ms\n",
      "iter 8600/8640/48828: loss 2.5504, time 96.55ms\n",
      "iter 8700/8720/48828: loss 2.5586, time 103.43ms\n",
      "iter 8800/8840/48828: loss 2.5523, time 95.68ms\n",
      "iter 8900/8920/48828: loss 2.5375, time 103.47ms\n",
      "step 9000: train loss 2.5415, val loss 2.5419\n",
      "iter 9000/9040/48828: loss 2.5464, time 3504.76ms\n",
      "iter 9100/9120/48828: loss 2.5617, time 104.36ms\n",
      "iter 9200/9240/48828: loss 2.5351, time 100.86ms\n",
      "iter 9300/9320/48828: loss 2.5476, time 103.61ms\n",
      "iter 9400/9440/48828: loss 2.5571, time 97.34ms\n",
      "iter 9500/9520/48828: loss 2.5402, time 104.04ms\n",
      "iter 9600/9640/48828: loss 2.5586, time 96.57ms\n",
      "iter 9700/9720/48828: loss 2.5194, time 103.92ms\n",
      "iter 9800/9840/48828: loss 2.5571, time 96.47ms\n",
      "iter 9900/9920/48828: loss 2.5470, time 103.07ms\n",
      "step 10000: train loss 2.5288, val loss 2.5297\n",
      "iter 10000/10040/48828: loss 2.5266, time 3093.29ms\n",
      "iter 10100/10120/48828: loss 2.5405, time 101.25ms\n",
      "iter 10200/10240/48828: loss 2.5333, time 98.98ms\n",
      "iter 10300/10320/48828: loss 2.5382, time 102.96ms\n",
      "iter 10400/10440/48828: loss 2.5237, time 101.75ms\n",
      "iter 10500/10520/48828: loss 2.5287, time 104.05ms\n",
      "iter 10600/10640/48828: loss 2.5428, time 101.06ms\n",
      "iter 10700/10720/48828: loss 2.5178, time 102.58ms\n",
      "iter 10800/10840/48828: loss 2.5477, time 95.48ms\n",
      "iter 10900/10920/48828: loss 2.5385, time 106.65ms\n",
      "step 11000: train loss 2.5344, val loss 2.5356\n",
      "iter 11000/11040/48828: loss 2.5381, time 3510.66ms\n",
      "iter 11100/11120/48828: loss 2.5262, time 102.37ms\n",
      "iter 11200/11240/48828: loss 2.5322, time 99.79ms\n",
      "iter 11300/11320/48828: loss 2.5406, time 104.31ms\n",
      "iter 11400/11440/48828: loss 2.5397, time 100.74ms\n",
      "iter 11500/11520/48828: loss 2.5369, time 104.65ms\n",
      "iter 11600/11640/48828: loss 2.5243, time 100.64ms\n",
      "iter 11700/11720/48828: loss 2.5412, time 101.81ms\n",
      "iter 11800/11840/48828: loss 2.5373, time 101.21ms\n",
      "iter 11900/11920/48828: loss 2.5468, time 102.74ms\n",
      "step 12000: train loss 2.5376, val loss 2.5371\n",
      "iter 12000/12040/48828: loss 2.5401, time 3103.25ms\n",
      "iter 12100/12120/48828: loss 2.5590, time 104.77ms\n",
      "iter 12200/12240/48828: loss 2.5499, time 100.05ms\n",
      "iter 12300/12320/48828: loss 2.5311, time 103.02ms\n",
      "iter 12400/12440/48828: loss 2.5376, time 101.98ms\n",
      "iter 12500/12520/48828: loss 2.5566, time 107.62ms\n",
      "iter 12600/12640/48828: loss 2.5413, time 97.41ms\n",
      "iter 12700/12720/48828: loss 2.5352, time 101.98ms\n",
      "iter 12800/12840/48828: loss 2.5397, time 96.00ms\n",
      "iter 12900/12920/48828: loss 2.5429, time 103.13ms\n",
      "step 13000: train loss 2.5356, val loss 2.5348\n",
      "iter 13000/13040/48828: loss 2.5379, time 3967.85ms\n",
      "iter 13100/13120/48828: loss 2.5411, time 103.27ms\n",
      "iter 13200/13240/48828: loss 2.5465, time 101.52ms\n",
      "iter 13300/13320/48828: loss 2.5331, time 103.20ms\n",
      "iter 13400/13440/48828: loss 2.5376, time 219.83ms\n",
      "iter 13500/13520/48828: loss 2.5359, time 104.69ms\n",
      "iter 13600/13640/48828: loss 2.5336, time 97.77ms\n",
      "iter 13700/13720/48828: loss 2.5275, time 103.79ms\n",
      "iter 13800/13840/48828: loss 2.5303, time 98.01ms\n",
      "iter 13900/13920/48828: loss 2.5482, time 105.88ms\n",
      "step 14000: train loss 2.5260, val loss 2.5253\n",
      "iter 14000/14040/48828: loss 2.5249, time 2972.79ms\n",
      "iter 14100/14120/48828: loss 2.5413, time 103.44ms\n",
      "iter 14200/14240/48828: loss 2.5380, time 95.83ms\n",
      "iter 14300/14320/48828: loss 2.5436, time 105.22ms\n",
      "iter 14400/14440/48828: loss 2.5275, time 96.20ms\n",
      "iter 14500/14520/48828: loss 2.5378, time 239.18ms\n",
      "iter 14600/14640/48828: loss 2.5387, time 101.78ms\n",
      "iter 14700/14720/48828: loss 2.5320, time 103.84ms\n",
      "iter 14800/14840/48828: loss 2.5530, time 102.99ms\n",
      "iter 14900/14920/48828: loss 2.5159, time 104.01ms\n",
      "step 15000: train loss 2.5344, val loss 2.5342\n",
      "iter 15000/15040/48828: loss 2.5345, time 2946.29ms\n",
      "iter 15100/15120/48828: loss 2.5350, time 102.50ms\n",
      "iter 15200/15240/48828: loss 2.5533, time 104.70ms\n",
      "iter 15300/15320/48828: loss 2.5641, time 102.54ms\n",
      "iter 15400/15440/48828: loss 2.5508, time 97.14ms\n",
      "iter 15500/15520/48828: loss 2.5527, time 102.42ms\n",
      "iter 15600/15640/48828: loss 2.5384, time 96.04ms\n",
      "iter 15700/15720/48828: loss 2.5401, time 103.96ms\n",
      "iter 15800/15840/48828: loss 2.5458, time 98.82ms\n",
      "iter 15900/15920/48828: loss 2.5289, time 101.19ms\n",
      "step 16000: train loss 2.5345, val loss 2.5338\n",
      "iter 16000/16040/48828: loss 2.5387, time 3550.04ms\n",
      "iter 16100/16120/48828: loss 2.5306, time 103.30ms\n",
      "iter 16200/16240/48828: loss 2.5389, time 98.98ms\n",
      "iter 16300/16320/48828: loss 2.5351, time 104.01ms\n",
      "iter 16400/16440/48828: loss 2.5254, time 98.21ms\n",
      "iter 16500/16520/48828: loss 2.5326, time 103.53ms\n",
      "iter 16600/16640/48828: loss 2.5553, time 98.85ms\n",
      "iter 16700/16720/48828: loss 2.5477, time 102.45ms\n",
      "iter 16800/16840/48828: loss 2.5546, time 96.17ms\n",
      "iter 16900/16920/48828: loss 2.5362, time 102.84ms\n",
      "step 17000: train loss 2.5403, val loss 2.5394\n",
      "iter 17000/17040/48828: loss 2.5362, time 3142.23ms\n",
      "iter 17100/17120/48828: loss 2.5585, time 102.70ms\n",
      "iter 17200/17240/48828: loss 2.5429, time 96.41ms\n",
      "iter 17300/17320/48828: loss 2.5606, time 103.76ms\n",
      "iter 17400/17440/48828: loss 2.5481, time 100.23ms\n",
      "iter 17500/17520/48828: loss 2.5462, time 104.19ms\n",
      "iter 17600/17640/48828: loss 2.5374, time 97.32ms\n",
      "iter 17700/17720/48828: loss 2.5407, time 102.96ms\n",
      "iter 17800/17840/48828: loss 2.5383, time 99.02ms\n",
      "iter 17900/17920/48828: loss 2.5334, time 103.58ms\n",
      "step 18000: train loss 2.5372, val loss 2.5357\n",
      "iter 18000/18040/48828: loss 2.5310, time 3073.74ms\n",
      "iter 18100/18120/48828: loss 2.5325, time 107.69ms\n",
      "iter 18200/18240/48828: loss 2.5449, time 93.61ms\n",
      "iter 18300/18320/48828: loss 2.5253, time 101.88ms\n",
      "iter 18400/18440/48828: loss 2.5367, time 97.21ms\n",
      "iter 18500/18520/48828: loss 2.5188, time 102.37ms\n",
      "iter 18600/18640/48828: loss 2.5461, time 94.80ms\n",
      "iter 18700/18720/48828: loss 2.5313, time 101.51ms\n",
      "iter 18800/18840/48828: loss 2.5363, time 101.03ms\n",
      "iter 18900/18920/48828: loss 2.5590, time 103.19ms\n",
      "step 19000: train loss 2.5272, val loss 2.5275\n",
      "iter 19000/19040/48828: loss 2.5298, time 3568.45ms\n",
      "iter 19100/19120/48828: loss 2.5222, time 102.52ms\n",
      "iter 19200/19240/48828: loss 2.5471, time 98.25ms\n",
      "iter 19300/19320/48828: loss 2.5238, time 105.79ms\n",
      "iter 19400/19440/48828: loss 2.5194, time 98.40ms\n",
      "iter 19500/19520/48828: loss 2.5452, time 103.44ms\n",
      "iter 19600/19640/48828: loss 2.5221, time 98.64ms\n",
      "iter 19700/19720/48828: loss 2.5335, time 103.51ms\n",
      "iter 19800/19840/48828: loss 2.5434, time 100.29ms\n",
      "iter 19900/19920/48828: loss 2.5169, time 103.37ms\n",
      "step 20000: train loss 2.5224, val loss 2.5224\n",
      "iter 20000/20040/48828: loss 2.5232, time 3571.29ms\n",
      "iter 20100/20120/48828: loss 2.5124, time 102.68ms\n",
      "iter 20200/20240/48828: loss 2.5293, time 103.92ms\n",
      "iter 20300/20320/48828: loss 2.5411, time 101.92ms\n",
      "iter 20400/20440/48828: loss 2.5355, time 103.58ms\n",
      "iter 20500/20520/48828: loss 2.5261, time 103.80ms\n",
      "iter 20600/20640/48828: loss 2.5375, time 98.81ms\n",
      "iter 20700/20720/48828: loss 2.5280, time 103.06ms\n",
      "iter 20800/20840/48828: loss 2.5260, time 96.07ms\n",
      "iter 20900/20920/48828: loss 2.5208, time 102.39ms\n",
      "step 21000: train loss 2.5407, val loss 2.5404\n",
      "iter 21000/21040/48828: loss 2.5412, time 2977.84ms\n",
      "iter 21100/21120/48828: loss 2.5241, time 103.91ms\n",
      "iter 21200/21240/48828: loss 2.5529, time 96.92ms\n",
      "iter 21300/21320/48828: loss 2.5285, time 98.74ms\n",
      "iter 21400/21440/48828: loss 2.5328, time 98.16ms\n",
      "iter 21500/21520/48828: loss 2.5376, time 101.46ms\n",
      "iter 21600/21640/48828: loss 2.5411, time 100.06ms\n",
      "iter 21700/21720/48828: loss 2.5277, time 102.08ms\n",
      "iter 21800/21840/48828: loss 2.5418, time 96.39ms\n",
      "iter 21900/21920/48828: loss 2.5438, time 102.92ms\n",
      "step 22000: train loss 2.5262, val loss 2.5253\n",
      "iter 22000/22040/48828: loss 2.5262, time 3105.42ms\n",
      "iter 22100/22120/48828: loss 2.5412, time 101.81ms\n",
      "iter 22200/22240/48828: loss 2.5310, time 98.84ms\n",
      "iter 22300/22320/48828: loss 2.5322, time 102.78ms\n",
      "iter 22400/22440/48828: loss 2.5452, time 97.33ms\n",
      "iter 22500/22520/48828: loss 2.5241, time 102.50ms\n",
      "iter 22600/22640/48828: loss 2.5346, time 96.78ms\n",
      "iter 22700/22720/48828: loss 2.5463, time 103.70ms\n",
      "iter 22800/22840/48828: loss 2.5220, time 103.09ms\n",
      "iter 22900/22920/48828: loss 2.5403, time 103.42ms\n",
      "step 23000: train loss 2.5348, val loss 2.5340\n",
      "iter 23000/23040/48828: loss 2.5372, time 3103.00ms\n",
      "iter 23100/23120/48828: loss 2.5322, time 275.59ms\n",
      "iter 23200/23240/48828: loss 2.5366, time 95.69ms\n",
      "iter 23300/23320/48828: loss 2.5269, time 107.59ms\n",
      "iter 23400/23440/48828: loss 2.5270, time 96.84ms\n",
      "iter 23500/23520/48828: loss 2.5332, time 102.15ms\n",
      "iter 23600/23640/48828: loss 2.5283, time 96.46ms\n",
      "iter 23700/23720/48828: loss 2.5206, time 102.07ms\n",
      "iter 23800/23840/48828: loss 2.5229, time 98.15ms\n",
      "iter 23900/23920/48828: loss 2.5361, time 107.78ms\n",
      "step 24000: train loss 2.5402, val loss 2.5401\n",
      "iter 24000/24040/48828: loss 2.5422, time 3105.68ms\n",
      "iter 24100/24120/48828: loss 2.5342, time 103.09ms\n",
      "iter 24200/24240/48828: loss 2.5180, time 100.39ms\n",
      "iter 24300/24320/48828: loss 2.5072, time 103.76ms\n",
      "iter 24400/24440/48828: loss 2.5330, time 96.73ms\n",
      "iter 24500/24520/48828: loss 2.5377, time 104.13ms\n",
      "iter 24600/24640/48828: loss 2.5223, time 100.74ms\n",
      "iter 24700/24720/48828: loss 2.5506, time 101.98ms\n",
      "iter 24800/24840/48828: loss 2.5263, time 94.90ms\n",
      "iter 24900/24920/48828: loss 2.5321, time 103.69ms\n",
      "step 25000: train loss 2.5228, val loss 2.5223\n",
      "iter 25000/25040/48828: loss 2.5263, time 4014.59ms\n",
      "iter 25100/25120/48828: loss 2.5072, time 102.68ms\n",
      "iter 25200/25240/48828: loss 2.5187, time 100.31ms\n",
      "iter 25300/25320/48828: loss 2.5210, time 107.27ms\n",
      "iter 25400/25440/48828: loss 2.5291, time 97.36ms\n",
      "iter 25500/25520/48828: loss 2.4999, time 102.56ms\n",
      "iter 25600/25640/48828: loss 2.5066, time 97.31ms\n",
      "iter 25700/25720/48828: loss 2.5096, time 103.29ms\n",
      "iter 25800/25840/48828: loss 2.5315, time 99.91ms\n",
      "iter 25900/25920/48828: loss 2.5309, time 108.33ms\n",
      "step 26000: train loss 2.5518, val loss 2.5519\n",
      "iter 26000/26040/48828: loss 2.5551, time 3095.62ms\n",
      "iter 26100/26120/48828: loss 2.5008, time 103.26ms\n",
      "iter 26200/26240/48828: loss 2.5133, time 97.80ms\n",
      "iter 26300/26320/48828: loss 2.5051, time 103.04ms\n",
      "iter 26400/26440/48828: loss 2.5326, time 98.90ms\n",
      "iter 26500/26520/48828: loss 2.5156, time 108.12ms\n",
      "iter 26600/26640/48828: loss 2.4996, time 97.30ms\n",
      "iter 26700/26720/48828: loss 2.5066, time 103.43ms\n",
      "iter 26800/26840/48828: loss 2.4922, time 94.98ms\n",
      "iter 26900/26920/48828: loss 2.5092, time 103.50ms\n",
      "step 27000: train loss 2.5015, val loss 2.4999\n",
      "iter 27000/27040/48828: loss 2.5000, time 3099.14ms\n",
      "iter 27100/27120/48828: loss 2.4953, time 102.77ms\n",
      "iter 27200/27240/48828: loss 2.5222, time 98.04ms\n",
      "iter 27300/27320/48828: loss 2.5190, time 103.05ms\n",
      "iter 27400/27440/48828: loss 2.5021, time 96.52ms\n",
      "iter 27500/27520/48828: loss 2.4937, time 109.59ms\n",
      "iter 27600/27640/48828: loss 2.5015, time 99.01ms\n",
      "iter 27700/27720/48828: loss 2.5118, time 102.81ms\n",
      "iter 27800/27840/48828: loss 2.5259, time 100.51ms\n",
      "iter 27900/27920/48828: loss 2.5044, time 102.76ms\n",
      "step 28000: train loss 2.4905, val loss 2.4897\n",
      "iter 28000/28040/48828: loss 2.4900, time 2980.87ms\n",
      "iter 28100/28120/48828: loss 2.5244, time 102.89ms\n",
      "iter 28200/28240/48828: loss 2.5356, time 96.13ms\n",
      "iter 28300/28320/48828: loss 2.5075, time 102.48ms\n",
      "iter 28400/28440/48828: loss 2.5176, time 98.13ms\n",
      "iter 28500/28520/48828: loss 2.5237, time 106.42ms\n",
      "iter 28600/28640/48828: loss 2.5254, time 96.42ms\n",
      "iter 28700/28720/48828: loss 2.5160, time 101.23ms\n",
      "iter 28800/28840/48828: loss 2.5104, time 102.05ms\n",
      "iter 28900/28920/48828: loss 2.4961, time 103.30ms\n",
      "step 29000: train loss 2.5160, val loss 2.5148\n",
      "iter 29000/29040/48828: loss 2.5124, time 3101.62ms\n",
      "iter 29100/29120/48828: loss 2.5282, time 100.98ms\n",
      "iter 29200/29240/48828: loss 2.5439, time 98.16ms\n",
      "iter 29300/29320/48828: loss 2.5332, time 103.15ms\n",
      "iter 29400/29440/48828: loss 2.5294, time 97.64ms\n",
      "iter 29500/29520/48828: loss 2.5279, time 102.18ms\n",
      "iter 29600/29640/48828: loss 2.5416, time 98.07ms\n",
      "iter 29700/29720/48828: loss 2.5166, time 103.50ms\n",
      "iter 29800/29840/48828: loss 2.5044, time 95.49ms\n",
      "iter 29900/29920/48828: loss 2.4854, time 107.78ms\n",
      "step 30000: train loss 2.5025, val loss 2.5031\n",
      "iter 30000/30040/48828: loss 2.4976, time 3123.15ms\n",
      "iter 30100/30120/48828: loss 2.4787, time 101.92ms\n",
      "iter 30200/30240/48828: loss 2.4645, time 94.88ms\n",
      "iter 30300/30320/48828: loss 2.4828, time 104.09ms\n",
      "iter 30400/30440/48828: loss 2.5018, time 99.38ms\n",
      "iter 30500/30520/48828: loss 2.4823, time 101.70ms\n",
      "iter 30600/30640/48828: loss 2.5282, time 103.20ms\n",
      "iter 30700/30720/48828: loss 2.5027, time 102.06ms\n",
      "iter 30800/30840/48828: loss 2.5066, time 98.00ms\n",
      "iter 30900/30920/48828: loss 2.5097, time 102.45ms\n",
      "step 31000: train loss 2.4855, val loss 2.4843\n",
      "iter 31000/31040/48828: loss 2.4815, time 3085.00ms\n",
      "iter 31100/31120/48828: loss 2.5095, time 103.11ms\n",
      "iter 31200/31240/48828: loss 2.5023, time 97.77ms\n",
      "iter 31300/31320/48828: loss 2.4971, time 102.86ms\n",
      "iter 31400/31440/48828: loss 2.4932, time 98.56ms\n",
      "iter 31500/31520/48828: loss 2.4807, time 103.10ms\n",
      "iter 31600/31640/48828: loss 2.4972, time 97.81ms\n",
      "iter 31700/31720/48828: loss 2.5035, time 107.73ms\n",
      "iter 31800/31840/48828: loss 2.4903, time 96.05ms\n",
      "iter 31900/31920/48828: loss 2.5011, time 105.18ms\n",
      "step 32000: train loss 2.5185, val loss 2.5190\n",
      "iter 32000/32040/48828: loss 2.5138, time 3129.54ms\n",
      "iter 32100/32120/48828: loss 2.4791, time 102.35ms\n",
      "iter 32200/32240/48828: loss 2.5251, time 97.00ms\n",
      "iter 32300/32320/48828: loss 2.5022, time 106.99ms\n",
      "iter 32400/32440/48828: loss 2.5263, time 95.37ms\n",
      "iter 32500/32520/48828: loss 2.5130, time 102.72ms\n",
      "iter 32600/32640/48828: loss 2.4936, time 95.66ms\n",
      "iter 32700/32720/48828: loss 2.4902, time 102.66ms\n",
      "iter 32800/32840/48828: loss 2.4937, time 102.31ms\n",
      "iter 32900/32920/48828: loss 2.5001, time 102.12ms\n",
      "step 33000: train loss 2.5104, val loss 2.5115\n",
      "iter 33000/33040/48828: loss 2.5100, time 3106.82ms\n",
      "iter 33100/33120/48828: loss 2.4920, time 102.36ms\n",
      "iter 33200/33240/48828: loss 2.5077, time 95.27ms\n",
      "iter 33300/33320/48828: loss 2.4881, time 107.31ms\n",
      "iter 33400/33440/48828: loss 2.4983, time 95.59ms\n",
      "iter 33500/33520/48828: loss 2.4848, time 102.97ms\n",
      "iter 33600/33640/48828: loss 2.4952, time 96.34ms\n",
      "iter 33700/33720/48828: loss 2.4917, time 101.98ms\n",
      "iter 33800/33840/48828: loss 2.4763, time 96.90ms\n",
      "iter 33900/33920/48828: loss 2.5025, time 101.94ms\n",
      "step 34000: train loss 2.5107, val loss 2.5107\n",
      "iter 34000/34040/48828: loss 2.5160, time 2954.16ms\n",
      "iter 34100/34120/48828: loss 2.4909, time 103.01ms\n",
      "iter 34200/34240/48828: loss 2.4768, time 98.47ms\n",
      "iter 34300/34320/48828: loss 2.4707, time 101.69ms\n",
      "iter 34400/34440/48828: loss 2.5163, time 100.10ms\n",
      "iter 34500/34520/48828: loss 2.4958, time 107.50ms\n",
      "iter 34600/34640/48828: loss 2.4868, time 95.87ms\n",
      "iter 34700/34720/48828: loss 2.4678, time 102.28ms\n",
      "iter 34800/34840/48828: loss 2.4615, time 98.98ms\n",
      "iter 34900/34920/48828: loss 2.4980, time 103.43ms\n",
      "step 35000: train loss 2.5054, val loss 2.5042\n",
      "iter 35000/35040/48828: loss 2.5008, time 3491.21ms\n",
      "iter 35100/35120/48828: loss 2.5043, time 102.73ms\n",
      "iter 35200/35240/48828: loss 2.5156, time 96.60ms\n",
      "iter 35300/35320/48828: loss 2.4927, time 102.59ms\n",
      "iter 35400/35440/48828: loss 2.5115, time 98.67ms\n",
      "iter 35500/35520/48828: loss 2.4807, time 102.07ms\n",
      "iter 35600/35640/48828: loss 2.4903, time 227.77ms\n",
      "iter 35700/35720/48828: loss 2.4735, time 103.35ms\n",
      "iter 35800/35840/48828: loss 2.4612, time 97.20ms\n",
      "iter 35900/35920/48828: loss 2.4787, time 105.72ms\n",
      "step 36000: train loss 2.5033, val loss 2.5037\n",
      "iter 36000/36040/48828: loss 2.5039, time 3102.77ms\n",
      "iter 36100/36120/48828: loss 2.4894, time 103.63ms\n",
      "iter 36200/36240/48828: loss 2.4684, time 97.98ms\n",
      "iter 36300/36320/48828: loss 2.4639, time 102.03ms\n",
      "iter 36400/36440/48828: loss 2.4841, time 95.78ms\n",
      "iter 36500/36520/48828: loss 2.4691, time 101.73ms\n",
      "iter 36600/36640/48828: loss 2.4894, time 96.78ms\n",
      "iter 36700/36720/48828: loss 2.4710, time 233.09ms\n",
      "iter 36800/36840/48828: loss 2.4575, time 98.73ms\n",
      "iter 36900/36920/48828: loss 2.4753, time 102.53ms\n",
      "step 37000: train loss 2.4820, val loss 2.4823\n",
      "iter 37000/37040/48828: loss 2.4756, time 3091.04ms\n",
      "iter 37100/37120/48828: loss 2.4481, time 102.90ms\n",
      "iter 37200/37240/48828: loss 2.4459, time 100.49ms\n",
      "iter 37300/37320/48828: loss 2.4576, time 102.49ms\n",
      "iter 37400/37440/48828: loss 2.4529, time 97.57ms\n",
      "iter 37500/37520/48828: loss 2.4475, time 103.11ms\n",
      "iter 37600/37640/48828: loss 2.4234, time 98.98ms\n",
      "iter 37700/37720/48828: loss 2.4517, time 103.38ms\n",
      "iter 37800/37840/48828: loss 2.4670, time 96.59ms\n",
      "iter 37900/37920/48828: loss 2.4739, time 103.34ms\n",
      "step 38000: train loss 2.4535, val loss 2.4529\n",
      "iter 38000/38040/48828: loss 2.4403, time 3096.94ms\n",
      "iter 38100/38120/48828: loss 2.4560, time 102.40ms\n",
      "iter 38200/38240/48828: loss 2.4475, time 100.08ms\n",
      "iter 38300/38320/48828: loss 2.4573, time 101.59ms\n",
      "iter 38400/38440/48828: loss 2.4430, time 92.39ms\n",
      "iter 38500/38520/48828: loss 2.4305, time 102.15ms\n",
      "iter 38600/38640/48828: loss 2.4469, time 96.63ms\n",
      "iter 38700/38720/48828: loss 2.4396, time 110.72ms\n",
      "iter 38800/38840/48828: loss 2.4448, time 91.82ms\n",
      "iter 38900/38920/48828: loss 2.4183, time 104.32ms\n",
      "step 39000: train loss 2.4434, val loss 2.4437\n",
      "iter 39000/39040/48828: loss 2.4473, time 3084.45ms\n",
      "iter 39100/39120/48828: loss 2.4446, time 102.57ms\n",
      "iter 39200/39240/48828: loss 2.4477, time 101.77ms\n",
      "iter 39300/39320/48828: loss 2.4255, time 102.93ms\n",
      "iter 39400/39440/48828: loss 2.4442, time 96.84ms\n",
      "iter 39500/39520/48828: loss 2.4378, time 102.94ms\n",
      "iter 39600/39640/48828: loss 2.4307, time 99.66ms\n",
      "iter 39700/39720/48828: loss 2.4329, time 104.26ms\n",
      "iter 39800/39840/48828: loss 2.4368, time 97.19ms\n",
      "iter 39900/39920/48828: loss 2.4348, time 103.17ms\n",
      "step 40000: train loss 2.4297, val loss 2.4287\n",
      "iter 40000/40040/48828: loss 2.4299, time 3099.72ms\n",
      "iter 40100/40120/48828: loss 2.4236, time 103.90ms\n",
      "iter 40200/40240/48828: loss 2.4282, time 98.49ms\n",
      "iter 40300/40320/48828: loss 2.3844, time 103.63ms\n",
      "iter 40400/40440/48828: loss 2.4019, time 97.29ms\n",
      "iter 40500/40520/48828: loss 2.4295, time 103.57ms\n",
      "iter 40600/40640/48828: loss 2.4071, time 101.80ms\n",
      "iter 40700/40720/48828: loss 2.4115, time 104.39ms\n",
      "iter 40800/40840/48828: loss 2.4037, time 98.38ms\n",
      "iter 40900/40920/48828: loss 2.4124, time 107.50ms\n",
      "step 41000: train loss 2.4129, val loss 2.4147\n",
      "iter 41000/41040/48828: loss 2.4124, time 2969.85ms\n",
      "iter 41100/41120/48828: loss 2.3982, time 103.29ms\n",
      "iter 41200/41240/48828: loss 2.3922, time 103.35ms\n",
      "iter 41300/41320/48828: loss 2.4170, time 103.68ms\n",
      "iter 41400/41440/48828: loss 2.3827, time 95.65ms\n",
      "iter 41500/41520/48828: loss 2.3851, time 103.72ms\n",
      "iter 41600/41640/48828: loss 2.4116, time 100.48ms\n",
      "iter 41700/41720/48828: loss 2.4186, time 103.24ms\n",
      "iter 41800/41840/48828: loss 2.4267, time 102.67ms\n",
      "iter 41900/41920/48828: loss 2.4189, time 104.11ms\n",
      "step 42000: train loss 2.3903, val loss 2.3915\n",
      "iter 42000/42040/48828: loss 2.3803, time 3127.73ms\n",
      "iter 42100/42120/48828: loss 2.3713, time 102.55ms\n",
      "iter 42200/42240/48828: loss 2.3782, time 97.77ms\n",
      "iter 42300/42320/48828: loss 2.3589, time 103.54ms\n",
      "iter 42400/42440/48828: loss 2.3900, time 99.31ms\n",
      "iter 42500/42520/48828: loss 2.4080, time 102.54ms\n",
      "iter 42600/42640/48828: loss 2.3671, time 96.24ms\n",
      "iter 42700/42720/48828: loss 2.3754, time 107.75ms\n",
      "iter 42800/42840/48828: loss 2.4046, time 96.43ms\n",
      "iter 42900/42920/48828: loss 2.3746, time 102.90ms\n",
      "step 43000: train loss 2.4109, val loss 2.4106\n",
      "iter 43000/43040/48828: loss 2.4220, time 3106.91ms\n",
      "iter 43100/43120/48828: loss 2.4069, time 102.45ms\n",
      "iter 43200/43240/48828: loss 2.3992, time 103.50ms\n",
      "iter 43300/43320/48828: loss 2.3994, time 104.35ms\n",
      "iter 43400/43440/48828: loss 2.3717, time 97.70ms\n",
      "iter 43500/43520/48828: loss 2.4092, time 104.69ms\n",
      "iter 43600/43640/48828: loss 2.4007, time 97.75ms\n",
      "iter 43700/43720/48828: loss 2.3984, time 101.02ms\n",
      "iter 43800/43840/48828: loss 2.3669, time 97.61ms\n",
      "iter 43900/43920/48828: loss 2.3749, time 104.12ms\n",
      "step 44000: train loss 2.3656, val loss 2.3655\n",
      "iter 44000/44040/48828: loss 2.3620, time 3099.32ms\n",
      "iter 44100/44120/48828: loss 2.3625, time 102.88ms\n",
      "iter 44200/44240/48828: loss 2.3634, time 222.29ms\n",
      "iter 44300/44320/48828: loss 2.3589, time 102.73ms\n",
      "iter 44400/44440/48828: loss 2.3471, time 97.93ms\n",
      "iter 44500/44520/48828: loss 2.3429, time 102.95ms\n",
      "iter 44600/44640/48828: loss 2.3497, time 98.59ms\n",
      "iter 44700/44720/48828: loss 2.3605, time 109.10ms\n",
      "iter 44800/44840/48828: loss 2.3634, time 97.56ms\n",
      "iter 44900/44920/48828: loss 2.3405, time 103.08ms\n",
      "step 45000: train loss 2.3545, val loss 2.3536\n",
      "iter 45000/45040/48828: loss 2.3596, time 3114.39ms\n",
      "iter 45100/45120/48828: loss 2.3266, time 102.37ms\n",
      "iter 45200/45240/48828: loss 2.3322, time 97.33ms\n",
      "iter 45300/45320/48828: loss 2.3175, time 228.01ms\n",
      "iter 45400/45440/48828: loss 2.3395, time 103.08ms\n",
      "iter 45500/45520/48828: loss 2.3361, time 107.46ms\n",
      "iter 45600/45640/48828: loss 2.3167, time 97.05ms\n",
      "iter 45700/45720/48828: loss 2.3373, time 104.69ms\n",
      "iter 45800/45840/48828: loss 2.3148, time 96.34ms\n",
      "iter 45900/45920/48828: loss 2.3267, time 103.48ms\n",
      "step 46000: train loss 2.3128, val loss 2.3133\n",
      "iter 46000/46040/48828: loss 2.3150, time 3121.71ms\n",
      "iter 46100/46120/48828: loss 2.3090, time 107.16ms\n",
      "iter 46200/46240/48828: loss 2.3257, time 96.54ms\n",
      "iter 46300/46320/48828: loss 2.3036, time 101.94ms\n",
      "iter 46400/46440/48828: loss 2.2748, time 96.58ms\n",
      "iter 46500/46520/48828: loss 2.2983, time 103.69ms\n",
      "iter 46600/46640/48828: loss 2.3049, time 96.85ms\n",
      "iter 46700/46720/48828: loss 2.2915, time 102.48ms\n",
      "iter 46800/46840/48828: loss 2.2815, time 99.28ms\n",
      "iter 46900/46920/48828: loss 2.2722, time 101.83ms\n",
      "step 47000: train loss 2.2822, val loss 2.2811\n",
      "iter 47000/47040/48828: loss 2.2940, time 2958.60ms\n",
      "iter 47100/47120/48828: loss 2.2730, time 101.76ms\n",
      "iter 47200/47240/48828: loss 2.2717, time 97.39ms\n",
      "iter 47300/47320/48828: loss 2.2822, time 107.41ms\n",
      "iter 47400/47440/48828: loss 2.2586, time 96.24ms\n",
      "iter 47500/47520/48828: loss 2.2702, time 102.91ms\n",
      "iter 47600/47640/48828: loss 2.2801, time 97.21ms\n",
      "iter 47700/47720/48828: loss 2.2847, time 101.88ms\n",
      "iter 47800/47840/48828: loss 2.2589, time 97.63ms\n",
      "iter 47900/47920/48828: loss 2.2802, time 104.20ms\n",
      "step 48000: train loss 2.2783, val loss 2.2745\n",
      "iter 48000/48040/48828: loss 2.2781, time 2968.09ms\n",
      "iter 48100/48120/48828: loss 2.2694, time 106.59ms\n",
      "iter 48200/48240/48828: loss 2.2627, time 100.87ms\n",
      "iter 48300/48320/48828: loss 2.2581, time 102.88ms\n",
      "iter 48400/48440/48828: loss 2.2599, time 96.84ms\n",
      "iter 48500/48520/48828: loss 2.2767, time 103.34ms\n",
      "iter 48600/48640/48828: loss 2.2635, time 98.84ms\n",
      "iter 48700/48720/48828: loss 2.2492, time 103.05ms\n",
      "iter 48800/48828/48828: loss 2.2506, time 103.14ms\n",
      "\n",
      "\n",
      "## Running generation 9 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [14:59<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=57.54% win[2]=42.45%, n=10000, draw=0.01%\n",
      "Game Length min: 7, max: 42, mean: 13.04\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=422 win[1]=25.12% counts=Counter({2: 316, 1: 106}), win[2]=74.88% draw=0.00%\n",
      "  a=2: n=455 win[1]=26.59% counts=Counter({2: 334, 1: 121}), win[2]=73.41% draw=0.00%\n",
      "  a=3: n=1830 win[1]=40.66% counts=Counter({2: 1086, 1: 744}), win[2]=59.34% draw=0.00%\n",
      "  a=4: n=4827 win[1]=75.66% counts=Counter({1: 3652, 2: 1175}), win[2]=24.34% draw=0.00%\n",
      "  a=5: n=774 win[1]=36.43% counts=Counter({2: 492, 1: 282}), win[2]=63.57% draw=0.00%\n",
      "  a=6: n=1261 win[1]=57.97% counts=Counter({1: 731, 2: 529, None: 1}), win[2]=41.95% draw=0.08%\n",
      "  a=7: n=431 win[1]=27.38% counts=Counter({2: 313, 1: 118}), win[2]=72.62% draw=0.00%\n",
      "Training model on gen-9\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.2531, val loss 2.2535\n",
      "iter 0/44/48828: loss 2.2494, time 3306.17ms\n",
      "iter 100/132/48828: loss 2.5127, time 103.14ms\n",
      "iter 200/220/48828: loss 2.5361, time 103.85ms\n",
      "iter 300/308/48828: loss 2.5274, time 103.12ms\n",
      "iter 400/440/48828: loss 2.5394, time 102.43ms\n",
      "iter 500/528/48828: loss 2.5256, time 102.43ms\n",
      "iter 600/616/48828: loss 2.5319, time 98.35ms\n",
      "iter 700/704/48828: loss 2.5507, time 102.75ms\n",
      "iter 800/836/48828: loss 2.5301, time 617.12ms\n",
      "iter 900/924/48828: loss 2.5331, time 102.66ms\n",
      "step 1000: train loss 2.5371, val loss 2.5370\n",
      "iter 1000/1012/48828: loss 2.5425, time 3125.14ms\n",
      "iter 1100/1144/48828: loss 2.5342, time 109.07ms\n",
      "iter 1200/1232/48828: loss 2.5267, time 102.80ms\n",
      "iter 1300/1320/48828: loss 2.5532, time 102.09ms\n",
      "iter 1400/1408/48828: loss 2.5321, time 103.29ms\n",
      "iter 1500/1540/48828: loss 2.5372, time 103.61ms\n",
      "iter 1600/1628/48828: loss 2.5303, time 102.60ms\n",
      "iter 1700/1716/48828: loss 2.5339, time 106.98ms\n",
      "iter 1800/1804/48828: loss 2.5314, time 103.63ms\n",
      "iter 1900/1936/48828: loss 2.5442, time 103.07ms\n",
      "step 2000: train loss 2.5249, val loss 2.5251\n",
      "iter 2000/2024/48828: loss 2.5229, time 3121.25ms\n",
      "iter 2100/2112/48828: loss 2.5462, time 103.44ms\n",
      "iter 2200/2244/48828: loss 2.5344, time 115.94ms\n",
      "iter 2300/2332/48828: loss 2.5250, time 102.51ms\n",
      "iter 2400/2420/48828: loss 2.5377, time 104.55ms\n",
      "iter 2500/2508/48828: loss 2.5301, time 102.27ms\n",
      "iter 2600/2640/48828: loss 2.5281, time 103.10ms\n",
      "iter 2700/2728/48828: loss 2.5381, time 104.15ms\n",
      "iter 2800/2816/48828: loss 2.5364, time 102.79ms\n",
      "iter 2900/2904/48828: loss 2.5404, time 103.63ms\n",
      "step 3000: train loss 2.5223, val loss 2.5232\n",
      "iter 3000/3036/48828: loss 2.5224, time 3162.34ms\n",
      "iter 3100/3124/48828: loss 2.5672, time 108.41ms\n",
      "iter 3200/3212/48828: loss 2.5353, time 102.95ms\n",
      "iter 3300/3344/48828: loss 2.5400, time 120.66ms\n",
      "iter 3400/3432/48828: loss 2.5228, time 97.40ms\n",
      "iter 3500/3520/48828: loss 2.5230, time 102.96ms\n",
      "iter 3600/3608/48828: loss 2.5266, time 104.62ms\n",
      "iter 3700/3740/48828: loss 2.5253, time 102.65ms\n",
      "iter 3800/3828/48828: loss 2.5303, time 102.43ms\n",
      "iter 3900/3916/48828: loss 2.5447, time 105.84ms\n",
      "step 4000: train loss 2.5602, val loss 2.5596\n",
      "iter 4000/4004/48828: loss 2.5547, time 2957.64ms\n",
      "iter 4100/4136/48828: loss 2.5220, time 101.35ms\n",
      "iter 4200/4224/48828: loss 2.5395, time 102.55ms\n",
      "iter 4300/4312/48828: loss 2.5407, time 104.25ms\n",
      "iter 4400/4444/48828: loss 2.5309, time 108.95ms\n",
      "iter 4500/4532/48828: loss 2.5278, time 103.84ms\n",
      "iter 4600/4620/48828: loss 2.5465, time 103.41ms\n",
      "iter 4700/4708/48828: loss 2.5294, time 102.36ms\n",
      "iter 4800/4840/48828: loss 2.5146, time 106.10ms\n",
      "iter 4900/4928/48828: loss 2.5499, time 101.37ms\n",
      "step 5000: train loss 2.5344, val loss 2.5337\n",
      "iter 5000/5016/48828: loss 2.5327, time 3146.88ms\n",
      "iter 5100/5104/48828: loss 2.5309, time 104.13ms\n",
      "iter 5200/5236/48828: loss 2.5294, time 104.52ms\n",
      "iter 5300/5324/48828: loss 2.5221, time 104.98ms\n",
      "iter 5400/5412/48828: loss 2.5121, time 103.90ms\n",
      "iter 5500/5544/48828: loss 2.5488, time 100.40ms\n",
      "iter 5600/5632/48828: loss 2.5265, time 103.12ms\n",
      "iter 5700/5720/48828: loss 2.5188, time 103.20ms\n",
      "iter 5800/5808/48828: loss 2.5337, time 101.70ms\n",
      "iter 5900/5940/48828: loss 2.5312, time 102.12ms\n",
      "step 6000: train loss 2.5361, val loss 2.5366\n",
      "iter 6000/6028/48828: loss 2.5368, time 3481.03ms\n",
      "iter 6100/6116/48828: loss 2.5389, time 103.20ms\n",
      "iter 6200/6204/48828: loss 2.5433, time 106.99ms\n",
      "iter 6300/6336/48828: loss 2.5160, time 103.17ms\n",
      "iter 6400/6424/48828: loss 2.5351, time 103.52ms\n",
      "iter 6500/6512/48828: loss 2.5383, time 102.53ms\n",
      "iter 6600/6644/48828: loss 2.5326, time 112.64ms\n",
      "iter 6700/6732/48828: loss 2.5587, time 103.58ms\n",
      "iter 6800/6820/48828: loss 2.5316, time 103.92ms\n",
      "iter 6900/6908/48828: loss 2.5396, time 102.36ms\n",
      "step 7000: train loss 2.5253, val loss 2.5246\n",
      "iter 7000/7040/48828: loss 2.5277, time 3648.65ms\n",
      "iter 7100/7128/48828: loss 2.5157, time 101.93ms\n",
      "iter 7200/7216/48828: loss 2.5515, time 102.70ms\n",
      "iter 7300/7304/48828: loss 2.5333, time 103.26ms\n",
      "iter 7400/7436/48828: loss 2.5339, time 102.69ms\n",
      "iter 7500/7524/48828: loss 2.5226, time 102.13ms\n",
      "iter 7600/7612/48828: loss 2.5287, time 104.54ms\n",
      "iter 7700/7744/48828: loss 2.5229, time 119.07ms\n",
      "iter 7800/7832/48828: loss 2.5245, time 103.39ms\n",
      "iter 7900/7920/48828: loss 2.5228, time 102.94ms\n",
      "step 8000: train loss 2.5358, val loss 2.5356\n",
      "iter 8000/8008/48828: loss 2.5333, time 3137.92ms\n",
      "iter 8100/8140/48828: loss 2.5203, time 101.85ms\n",
      "iter 8200/8228/48828: loss 2.5100, time 103.64ms\n",
      "iter 8300/8316/48828: loss 2.5315, time 107.40ms\n",
      "iter 8400/8404/48828: loss 2.5284, time 104.73ms\n",
      "iter 8500/8536/48828: loss 2.5434, time 107.55ms\n",
      "iter 8600/8624/48828: loss 2.5180, time 102.57ms\n",
      "iter 8700/8712/48828: loss 2.5331, time 102.44ms\n",
      "iter 8800/8844/48828: loss 2.5361, time 110.54ms\n",
      "iter 8900/8932/48828: loss 2.5223, time 104.25ms\n",
      "step 9000: train loss 2.5439, val loss 2.5440\n",
      "iter 9000/9020/48828: loss 2.5453, time 3678.30ms\n",
      "iter 9100/9108/48828: loss 2.5215, time 102.33ms\n",
      "iter 9200/9240/48828: loss 2.5328, time 292.22ms\n",
      "iter 9300/9328/48828: loss 2.5204, time 104.40ms\n",
      "iter 9400/9416/48828: loss 2.5398, time 102.52ms\n",
      "iter 9500/9504/48828: loss 2.5415, time 278.24ms\n",
      "iter 9600/9636/48828: loss 2.5275, time 104.26ms\n",
      "iter 9700/9724/48828: loss 2.5299, time 102.27ms\n",
      "iter 9800/9812/48828: loss 2.5236, time 104.31ms\n",
      "iter 9900/9944/48828: loss 2.5194, time 111.12ms\n",
      "step 10000: train loss 2.5178, val loss 2.5181\n",
      "iter 10000/10032/48828: loss 2.5145, time 3145.16ms\n",
      "iter 10100/10120/48828: loss 2.5140, time 102.49ms\n",
      "iter 10200/10208/48828: loss 2.5475, time 105.03ms\n",
      "iter 10300/10340/48828: loss 2.5355, time 106.04ms\n",
      "iter 10400/10428/48828: loss 2.5305, time 102.64ms\n",
      "iter 10500/10516/48828: loss 2.5291, time 103.31ms\n",
      "iter 10600/10604/48828: loss 2.5178, time 103.93ms\n",
      "iter 10700/10736/48828: loss 2.5282, time 102.94ms\n",
      "iter 10800/10824/48828: loss 2.5419, time 104.22ms\n",
      "iter 10900/10912/48828: loss 2.5324, time 102.47ms\n",
      "step 11000: train loss 2.5228, val loss 2.5232\n",
      "iter 11000/11044/48828: loss 2.5227, time 2952.41ms\n",
      "iter 11100/11132/48828: loss 2.5280, time 101.78ms\n",
      "iter 11200/11220/48828: loss 2.5197, time 102.56ms\n",
      "iter 11300/11308/48828: loss 2.5209, time 103.53ms\n",
      "iter 11400/11440/48828: loss 2.5425, time 102.68ms\n",
      "iter 11500/11528/48828: loss 2.5147, time 107.45ms\n",
      "iter 11600/11616/48828: loss 2.5306, time 104.94ms\n",
      "iter 11700/11704/48828: loss 2.5216, time 102.20ms\n",
      "iter 11800/11836/48828: loss 2.5231, time 103.26ms\n",
      "iter 11900/11924/48828: loss 2.5218, time 104.16ms\n",
      "step 12000: train loss 2.5294, val loss 2.5304\n",
      "iter 12000/12012/48828: loss 2.5331, time 3702.20ms\n",
      "iter 12100/12144/48828: loss 2.5331, time 99.75ms\n",
      "iter 12200/12232/48828: loss 2.5248, time 103.65ms\n",
      "iter 12300/12320/48828: loss 2.5376, time 107.86ms\n",
      "iter 12400/12408/48828: loss 2.5188, time 103.05ms\n",
      "iter 12500/12540/48828: loss 2.5229, time 104.00ms\n",
      "iter 12600/12628/48828: loss 2.5217, time 104.33ms\n",
      "iter 12700/12716/48828: loss 2.5241, time 103.35ms\n",
      "iter 12800/12804/48828: loss 2.5315, time 105.44ms\n",
      "iter 12900/12936/48828: loss 2.5340, time 105.84ms\n",
      "step 13000: train loss 2.5316, val loss 2.5314\n",
      "iter 13000/13024/48828: loss 2.5330, time 3054.73ms\n",
      "iter 13100/13112/48828: loss 2.5150, time 104.32ms\n",
      "iter 13200/13244/48828: loss 2.5278, time 106.92ms\n",
      "iter 13300/13332/48828: loss 2.5264, time 104.09ms\n",
      "iter 13400/13420/48828: loss 2.5484, time 102.81ms\n",
      "iter 13500/13508/48828: loss 2.5316, time 102.78ms\n",
      "iter 13600/13640/48828: loss 2.5325, time 103.88ms\n",
      "iter 13700/13728/48828: loss 2.5387, time 102.24ms\n",
      "iter 13800/13816/48828: loss 2.5296, time 108.83ms\n",
      "iter 13900/13904/48828: loss 2.5418, time 102.82ms\n",
      "step 14000: train loss 2.5463, val loss 2.5455\n",
      "iter 14000/14036/48828: loss 2.5457, time 3170.39ms\n",
      "iter 14100/14124/48828: loss 2.5315, time 102.37ms\n",
      "iter 14200/14212/48828: loss 2.5285, time 105.14ms\n",
      "iter 14300/14344/48828: loss 2.5242, time 106.60ms\n",
      "iter 14400/14432/48828: loss 2.5245, time 102.78ms\n",
      "iter 14500/14520/48828: loss 2.5116, time 102.44ms\n",
      "iter 14600/14608/48828: loss 2.5194, time 103.04ms\n",
      "iter 14700/14740/48828: loss 2.5288, time 101.84ms\n",
      "iter 14800/14828/48828: loss 2.5293, time 106.13ms\n",
      "iter 14900/14916/48828: loss 2.5264, time 112.44ms\n",
      "step 15000: train loss 2.5264, val loss 2.5269\n",
      "iter 15000/15004/48828: loss 2.5262, time 2983.77ms\n",
      "iter 15100/15136/48828: loss 2.5335, time 103.40ms\n",
      "iter 15200/15224/48828: loss 2.5360, time 103.57ms\n",
      "iter 15300/15312/48828: loss 2.5349, time 104.94ms\n",
      "iter 15400/15444/48828: loss 2.5364, time 100.91ms\n",
      "iter 15500/15532/48828: loss 2.5247, time 104.37ms\n",
      "iter 15600/15620/48828: loss 2.5206, time 103.83ms\n",
      "iter 15700/15708/48828: loss 2.5293, time 105.33ms\n",
      "iter 15800/15840/48828: loss 2.5153, time 97.82ms\n",
      "iter 15900/15928/48828: loss 2.5122, time 102.82ms\n",
      "step 16000: train loss 2.5160, val loss 2.5151\n",
      "iter 16000/16016/48828: loss 2.5112, time 3166.46ms\n",
      "iter 16100/16104/48828: loss 2.5354, time 101.87ms\n",
      "iter 16200/16236/48828: loss 2.5343, time 102.53ms\n",
      "iter 16300/16324/48828: loss 2.5450, time 102.35ms\n",
      "iter 16400/16412/48828: loss 2.5334, time 102.91ms\n",
      "iter 16500/16544/48828: loss 2.5226, time 110.83ms\n",
      "iter 16600/16632/48828: loss 2.5304, time 102.67ms\n",
      "iter 16700/16720/48828: loss 2.5537, time 97.73ms\n",
      "iter 16800/16808/48828: loss 2.5356, time 108.95ms\n",
      "iter 16900/16940/48828: loss 2.5171, time 102.92ms\n",
      "step 17000: train loss 2.5415, val loss 2.5418\n",
      "iter 17000/17028/48828: loss 2.5423, time 2976.62ms\n",
      "iter 17100/17116/48828: loss 2.5337, time 104.25ms\n",
      "iter 17200/17204/48828: loss 2.5237, time 102.71ms\n",
      "iter 17300/17336/48828: loss 2.5412, time 103.05ms\n",
      "iter 17400/17424/48828: loss 2.5365, time 103.59ms\n",
      "iter 17500/17512/48828: loss 2.5246, time 104.30ms\n",
      "iter 17600/17644/48828: loss 2.5256, time 100.45ms\n",
      "iter 17700/17732/48828: loss 2.5366, time 104.07ms\n",
      "iter 17800/17820/48828: loss 2.5299, time 109.00ms\n",
      "iter 17900/17908/48828: loss 2.5309, time 104.49ms\n",
      "step 18000: train loss 2.5247, val loss 2.5248\n",
      "iter 18000/18040/48828: loss 2.5221, time 3146.95ms\n",
      "iter 18100/18128/48828: loss 2.5435, time 102.46ms\n",
      "iter 18200/18216/48828: loss 2.5309, time 103.37ms\n",
      "iter 18300/18304/48828: loss 2.5264, time 104.73ms\n",
      "iter 18400/18436/48828: loss 2.5532, time 101.67ms\n",
      "iter 18500/18524/48828: loss 2.5292, time 104.52ms\n",
      "iter 18600/18612/48828: loss 2.5280, time 103.89ms\n",
      "iter 18700/18744/48828: loss 2.5328, time 100.64ms\n",
      "iter 18800/18832/48828: loss 2.5418, time 104.41ms\n",
      "iter 18900/18920/48828: loss 2.5523, time 102.92ms\n",
      "step 19000: train loss 2.5098, val loss 2.5097\n",
      "iter 19000/19008/48828: loss 2.5065, time 2975.01ms\n",
      "iter 19100/19140/48828: loss 2.5254, time 101.19ms\n",
      "iter 19200/19228/48828: loss 2.5705, time 104.13ms\n",
      "iter 19300/19316/48828: loss 2.5030, time 101.79ms\n",
      "iter 19400/19404/48828: loss 2.5181, time 103.78ms\n",
      "iter 19500/19536/48828: loss 2.5253, time 102.47ms\n",
      "iter 19600/19624/48828: loss 2.5223, time 103.07ms\n",
      "iter 19700/19712/48828: loss 2.5289, time 103.53ms\n",
      "iter 19800/19844/48828: loss 2.5365, time 108.89ms\n",
      "iter 19900/19932/48828: loss 2.5065, time 102.04ms\n",
      "step 20000: train loss 2.5186, val loss 2.5189\n",
      "iter 20000/20020/48828: loss 2.5212, time 3150.76ms\n",
      "iter 20100/20108/48828: loss 2.5150, time 104.33ms\n",
      "iter 20200/20240/48828: loss 2.5278, time 103.13ms\n",
      "iter 20300/20328/48828: loss 2.5452, time 103.73ms\n",
      "iter 20400/20416/48828: loss 2.5316, time 101.95ms\n",
      "iter 20500/20504/48828: loss 2.5358, time 102.67ms\n",
      "iter 20600/20636/48828: loss 2.5325, time 104.56ms\n",
      "iter 20700/20724/48828: loss 2.5310, time 104.06ms\n",
      "iter 20800/20812/48828: loss 2.5370, time 102.36ms\n",
      "iter 20900/20944/48828: loss 2.5278, time 99.83ms\n",
      "step 21000: train loss 2.5116, val loss 2.5121\n",
      "iter 21000/21032/48828: loss 2.5126, time 3165.58ms\n",
      "iter 21100/21120/48828: loss 2.5296, time 103.36ms\n",
      "iter 21200/21208/48828: loss 2.5235, time 103.55ms\n",
      "iter 21300/21340/48828: loss 2.5133, time 101.83ms\n",
      "iter 21400/21428/48828: loss 2.5180, time 102.59ms\n",
      "iter 21500/21516/48828: loss 2.5065, time 103.61ms\n",
      "iter 21600/21604/48828: loss 2.5198, time 104.10ms\n",
      "iter 21700/21736/48828: loss 2.5200, time 102.22ms\n",
      "iter 21800/21824/48828: loss 2.5074, time 103.33ms\n",
      "iter 21900/21912/48828: loss 2.5297, time 103.02ms\n",
      "step 22000: train loss 2.5175, val loss 2.5168\n",
      "iter 22000/22044/48828: loss 2.5262, time 3164.13ms\n",
      "iter 22100/22132/48828: loss 2.5267, time 101.72ms\n",
      "iter 22200/22220/48828: loss 2.5289, time 104.09ms\n",
      "iter 22300/22308/48828: loss 2.5303, time 103.73ms\n",
      "iter 22400/22440/48828: loss 2.5246, time 102.87ms\n",
      "iter 22500/22528/48828: loss 2.5197, time 103.06ms\n",
      "iter 22600/22616/48828: loss 2.5124, time 105.73ms\n",
      "iter 22700/22704/48828: loss 2.5036, time 102.04ms\n",
      "iter 22800/22836/48828: loss 2.5475, time 102.22ms\n",
      "iter 22900/22924/48828: loss 2.5118, time 102.62ms\n",
      "step 23000: train loss 2.5119, val loss 2.5116\n",
      "iter 23000/23012/48828: loss 2.5102, time 3147.18ms\n",
      "iter 23100/23144/48828: loss 2.5034, time 104.57ms\n",
      "iter 23200/23232/48828: loss 2.5148, time 105.44ms\n",
      "iter 23300/23320/48828: loss 2.5123, time 104.06ms\n",
      "iter 23400/23408/48828: loss 2.5220, time 652.64ms\n",
      "iter 23500/23540/48828: loss 2.5299, time 103.51ms\n",
      "iter 23600/23628/48828: loss 2.5178, time 103.15ms\n",
      "iter 23700/23716/48828: loss 2.5160, time 102.89ms\n",
      "iter 23800/23804/48828: loss 2.5244, time 108.84ms\n",
      "iter 23900/23936/48828: loss 2.5132, time 102.82ms\n",
      "step 24000: train loss 2.5174, val loss 2.5175\n",
      "iter 24000/24024/48828: loss 2.5154, time 2996.81ms\n",
      "iter 24100/24112/48828: loss 2.5299, time 104.17ms\n",
      "iter 24200/24244/48828: loss 2.5211, time 115.36ms\n",
      "iter 24300/24332/48828: loss 2.5179, time 102.62ms\n",
      "iter 24400/24420/48828: loss 2.5122, time 106.10ms\n",
      "iter 24500/24508/48828: loss 2.5092, time 101.82ms\n",
      "iter 24600/24640/48828: loss 2.5122, time 103.38ms\n",
      "iter 24700/24728/48828: loss 2.5034, time 102.62ms\n",
      "iter 24800/24816/48828: loss 2.5285, time 102.50ms\n",
      "iter 24900/24904/48828: loss 2.5106, time 108.29ms\n",
      "step 25000: train loss 2.5283, val loss 2.5287\n",
      "iter 25000/25036/48828: loss 2.5313, time 3133.20ms\n",
      "iter 25100/25124/48828: loss 2.5175, time 275.25ms\n",
      "iter 25200/25212/48828: loss 2.5653, time 103.48ms\n",
      "iter 25300/25344/48828: loss 2.5318, time 106.56ms\n",
      "iter 25400/25432/48828: loss 2.5100, time 104.33ms\n",
      "iter 25500/25520/48828: loss 2.5260, time 104.64ms\n",
      "iter 25600/25608/48828: loss 2.4808, time 103.85ms\n",
      "iter 25700/25740/48828: loss 2.5270, time 103.32ms\n",
      "iter 25800/25828/48828: loss 2.5292, time 107.43ms\n",
      "iter 25900/25916/48828: loss 2.4898, time 103.25ms\n",
      "step 26000: train loss 2.5117, val loss 2.5111\n",
      "iter 26000/26004/48828: loss 2.5128, time 2971.68ms\n",
      "iter 26100/26136/48828: loss 2.4966, time 102.44ms\n",
      "iter 26200/26224/48828: loss 2.4937, time 104.16ms\n",
      "iter 26300/26312/48828: loss 2.5113, time 277.93ms\n",
      "iter 26400/26444/48828: loss 2.4899, time 100.57ms\n",
      "iter 26500/26532/48828: loss 2.5229, time 102.54ms\n",
      "iter 26600/26620/48828: loss 2.5155, time 104.89ms\n",
      "iter 26700/26708/48828: loss 2.4984, time 104.71ms\n",
      "iter 26800/26840/48828: loss 2.5287, time 103.15ms\n",
      "iter 26900/26928/48828: loss 2.4988, time 103.27ms\n",
      "step 27000: train loss 2.5119, val loss 2.5124\n",
      "iter 27000/27016/48828: loss 2.5156, time 3120.48ms\n",
      "iter 27100/27104/48828: loss 2.5198, time 103.40ms\n",
      "iter 27200/27236/48828: loss 2.5209, time 108.04ms\n",
      "iter 27300/27324/48828: loss 2.5062, time 104.03ms\n",
      "iter 27400/27412/48828: loss 2.4910, time 279.69ms\n",
      "iter 27500/27544/48828: loss 2.4803, time 110.91ms\n",
      "iter 27600/27632/48828: loss 2.5039, time 101.21ms\n",
      "iter 27700/27720/48828: loss 2.5334, time 104.01ms\n",
      "iter 27800/27808/48828: loss 2.5125, time 103.58ms\n",
      "iter 27900/27940/48828: loss 2.5014, time 102.26ms\n",
      "step 28000: train loss 2.5147, val loss 2.5126\n",
      "iter 28000/28028/48828: loss 2.5082, time 2994.68ms\n",
      "iter 28100/28116/48828: loss 2.5218, time 102.94ms\n",
      "iter 28200/28204/48828: loss 2.5201, time 107.38ms\n",
      "iter 28300/28336/48828: loss 2.5009, time 101.11ms\n",
      "iter 28400/28424/48828: loss 2.5148, time 105.38ms\n",
      "iter 28500/28512/48828: loss 2.5060, time 103.44ms\n",
      "iter 28600/28644/48828: loss 2.5020, time 108.85ms\n",
      "iter 28700/28732/48828: loss 2.5098, time 104.08ms\n",
      "iter 28800/28820/48828: loss 2.5144, time 103.02ms\n",
      "iter 28900/28908/48828: loss 2.4946, time 101.32ms\n",
      "step 29000: train loss 2.5276, val loss 2.5268\n",
      "iter 29000/29040/48828: loss 2.5305, time 3142.25ms\n",
      "iter 29100/29128/48828: loss 2.5150, time 103.93ms\n",
      "iter 29200/29216/48828: loss 2.5398, time 103.59ms\n",
      "iter 29300/29304/48828: loss 2.5116, time 102.35ms\n",
      "iter 29400/29436/48828: loss 2.5178, time 103.85ms\n",
      "iter 29500/29524/48828: loss 2.5176, time 101.83ms\n",
      "iter 29600/29612/48828: loss 2.5216, time 104.01ms\n",
      "iter 29700/29744/48828: loss 2.5175, time 100.01ms\n",
      "iter 29800/29832/48828: loss 2.5214, time 103.92ms\n",
      "iter 29900/29920/48828: loss 2.5039, time 108.16ms\n",
      "step 30000: train loss 2.5261, val loss 2.5255\n",
      "iter 30000/30008/48828: loss 2.5254, time 2971.01ms\n",
      "iter 30100/30140/48828: loss 2.5061, time 103.75ms\n",
      "iter 30200/30228/48828: loss 2.5374, time 104.96ms\n",
      "iter 30300/30316/48828: loss 2.5295, time 102.79ms\n",
      "iter 30400/30404/48828: loss 2.5236, time 103.07ms\n",
      "iter 30500/30536/48828: loss 2.5167, time 101.93ms\n",
      "iter 30600/30624/48828: loss 2.5162, time 103.89ms\n",
      "iter 30700/30712/48828: loss 2.4908, time 103.80ms\n",
      "iter 30800/30844/48828: loss 2.5010, time 101.12ms\n",
      "iter 30900/30932/48828: loss 2.5072, time 103.27ms\n",
      "step 31000: train loss 2.4875, val loss 2.4890\n",
      "iter 31000/31020/48828: loss 2.4853, time 4709.19ms\n",
      "iter 31100/31108/48828: loss 2.5042, time 104.09ms\n",
      "iter 31200/31240/48828: loss 2.5128, time 97.43ms\n",
      "iter 31300/31328/48828: loss 2.5153, time 102.92ms\n",
      "iter 31400/31416/48828: loss 2.5263, time 102.92ms\n",
      "iter 31500/31504/48828: loss 2.5227, time 102.79ms\n",
      "iter 31600/31636/48828: loss 2.5080, time 106.03ms\n",
      "iter 31700/31724/48828: loss 2.4974, time 103.82ms\n",
      "iter 31800/31812/48828: loss 2.5283, time 105.50ms\n",
      "iter 31900/31944/48828: loss 2.5128, time 100.29ms\n",
      "step 32000: train loss 2.5030, val loss 2.5013\n",
      "iter 32000/32032/48828: loss 2.4966, time 3166.29ms\n",
      "iter 32100/32120/48828: loss 2.5083, time 103.57ms\n",
      "iter 32200/32208/48828: loss 2.5111, time 103.50ms\n",
      "iter 32300/32340/48828: loss 2.5106, time 105.53ms\n",
      "iter 32400/32428/48828: loss 2.5149, time 105.69ms\n",
      "iter 32500/32516/48828: loss 2.5037, time 103.60ms\n",
      "iter 32600/32604/48828: loss 2.5074, time 104.52ms\n",
      "iter 32700/32736/48828: loss 2.5132, time 102.89ms\n",
      "iter 32800/32824/48828: loss 2.5029, time 103.69ms\n",
      "iter 32900/32912/48828: loss 2.5167, time 103.17ms\n",
      "step 33000: train loss 2.5134, val loss 2.5133\n",
      "iter 33000/33044/48828: loss 2.5144, time 3227.52ms\n",
      "iter 33100/33132/48828: loss 2.5118, time 103.47ms\n",
      "iter 33200/33220/48828: loss 2.5023, time 102.43ms\n",
      "iter 33300/33308/48828: loss 2.4989, time 104.02ms\n",
      "iter 33400/33440/48828: loss 2.4998, time 107.38ms\n",
      "iter 33500/33528/48828: loss 2.5085, time 101.52ms\n",
      "iter 33600/33616/48828: loss 2.5107, time 103.45ms\n",
      "iter 33700/33704/48828: loss 2.5072, time 106.55ms\n",
      "iter 33800/33836/48828: loss 2.5432, time 104.42ms\n",
      "iter 33900/33924/48828: loss 2.5259, time 103.45ms\n",
      "step 34000: train loss 2.5211, val loss 2.5213\n",
      "iter 34000/34012/48828: loss 2.5256, time 3962.50ms\n",
      "iter 34100/34144/48828: loss 2.5111, time 100.98ms\n",
      "iter 34200/34232/48828: loss 2.5273, time 103.97ms\n",
      "iter 34300/34320/48828: loss 2.5299, time 101.56ms\n",
      "iter 34400/34408/48828: loss 2.5189, time 102.89ms\n",
      "iter 34500/34540/48828: loss 2.5092, time 106.54ms\n",
      "iter 34600/34628/48828: loss 2.5207, time 104.69ms\n",
      "iter 34700/34716/48828: loss 2.4943, time 103.65ms\n",
      "iter 34800/34804/48828: loss 2.5000, time 104.42ms\n",
      "iter 34900/34936/48828: loss 2.4791, time 103.19ms\n",
      "step 35000: train loss 2.4807, val loss 2.4814\n",
      "iter 35000/35024/48828: loss 2.4835, time 2977.73ms\n",
      "iter 35100/35112/48828: loss 2.4979, time 103.76ms\n",
      "iter 35200/35244/48828: loss 2.5018, time 100.59ms\n",
      "iter 35300/35332/48828: loss 2.4992, time 103.84ms\n",
      "iter 35400/35420/48828: loss 2.5066, time 104.96ms\n",
      "iter 35500/35508/48828: loss 2.4952, time 103.20ms\n",
      "iter 35600/35640/48828: loss 2.4826, time 101.31ms\n",
      "iter 35700/35728/48828: loss 2.4894, time 104.77ms\n",
      "iter 35800/35816/48828: loss 2.4841, time 103.84ms\n",
      "iter 35900/35904/48828: loss 2.4755, time 108.36ms\n",
      "step 36000: train loss 2.4860, val loss 2.4865\n",
      "iter 36000/36036/48828: loss 2.4837, time 3138.56ms\n",
      "iter 36100/36124/48828: loss 2.4774, time 102.47ms\n",
      "iter 36200/36212/48828: loss 2.4817, time 103.07ms\n",
      "iter 36300/36344/48828: loss 2.4935, time 99.56ms\n",
      "iter 36400/36432/48828: loss 2.4816, time 108.06ms\n",
      "iter 36500/36520/48828: loss 2.4821, time 103.89ms\n",
      "iter 36600/36608/48828: loss 2.5034, time 104.33ms\n",
      "iter 36700/36740/48828: loss 2.4824, time 268.64ms\n",
      "iter 36800/36828/48828: loss 2.4818, time 103.43ms\n",
      "iter 36900/36916/48828: loss 2.4984, time 104.32ms\n",
      "step 37000: train loss 2.5008, val loss 2.5004\n",
      "iter 37000/37004/48828: loss 2.4946, time 3003.22ms\n",
      "iter 37100/37136/48828: loss 2.4862, time 104.10ms\n",
      "iter 37200/37224/48828: loss 2.4882, time 102.48ms\n",
      "iter 37300/37312/48828: loss 2.5075, time 102.96ms\n",
      "iter 37400/37444/48828: loss 2.4845, time 109.58ms\n",
      "iter 37500/37532/48828: loss 2.4865, time 104.55ms\n",
      "iter 37600/37620/48828: loss 2.4831, time 104.78ms\n",
      "iter 37700/37708/48828: loss 2.4631, time 104.04ms\n",
      "iter 37800/37840/48828: loss 2.4654, time 102.59ms\n",
      "iter 37900/37928/48828: loss 2.4402, time 103.72ms\n",
      "step 38000: train loss 2.4483, val loss 2.4487\n",
      "iter 38000/38016/48828: loss 2.4482, time 3170.31ms\n",
      "iter 38100/38104/48828: loss 2.4658, time 103.56ms\n",
      "iter 38200/38236/48828: loss 2.4579, time 103.78ms\n",
      "iter 38300/38324/48828: loss 2.4575, time 103.41ms\n",
      "iter 38400/38412/48828: loss 2.4711, time 103.16ms\n",
      "iter 38500/38544/48828: loss 2.4781, time 106.33ms\n",
      "iter 38600/38632/48828: loss 2.4633, time 101.54ms\n",
      "iter 38700/38720/48828: loss 2.4786, time 103.94ms\n",
      "iter 38800/38808/48828: loss 2.4683, time 104.36ms\n",
      "iter 38900/38940/48828: loss 2.4865, time 103.07ms\n",
      "step 39000: train loss 2.4604, val loss 2.4619\n",
      "iter 39000/39028/48828: loss 2.4629, time 2992.71ms\n",
      "iter 39100/39116/48828: loss 2.4505, time 104.12ms\n",
      "iter 39200/39204/48828: loss 2.4357, time 103.14ms\n",
      "iter 39300/39336/48828: loss 2.4354, time 102.89ms\n",
      "iter 39400/39424/48828: loss 2.4476, time 103.87ms\n",
      "iter 39500/39512/48828: loss 2.4683, time 102.57ms\n",
      "iter 39600/39644/48828: loss 2.4567, time 107.26ms\n",
      "iter 39700/39732/48828: loss 2.4286, time 106.44ms\n",
      "iter 39800/39820/48828: loss 2.4253, time 103.18ms\n",
      "iter 39900/39908/48828: loss 2.4516, time 103.77ms\n",
      "step 40000: train loss 2.4716, val loss 2.4715\n",
      "iter 40000/40040/48828: loss 2.4697, time 3171.11ms\n",
      "iter 40100/40128/48828: loss 2.4280, time 105.91ms\n",
      "iter 40200/40216/48828: loss 2.4311, time 104.08ms\n",
      "iter 40300/40304/48828: loss 2.4328, time 103.46ms\n",
      "iter 40400/40436/48828: loss 2.4229, time 103.76ms\n",
      "iter 40500/40524/48828: loss 2.4306, time 104.75ms\n",
      "iter 40600/40612/48828: loss 2.4397, time 103.12ms\n",
      "iter 40700/40744/48828: loss 2.4230, time 100.03ms\n",
      "iter 40800/40832/48828: loss 2.4214, time 107.81ms\n",
      "iter 40900/40920/48828: loss 2.4192, time 107.32ms\n",
      "step 41000: train loss 2.4254, val loss 2.4255\n",
      "iter 41000/41008/48828: loss 2.4249, time 3211.68ms\n",
      "iter 41100/41140/48828: loss 2.4469, time 103.44ms\n",
      "iter 41200/41228/48828: loss 2.4453, time 107.95ms\n",
      "iter 41300/41316/48828: loss 2.4387, time 102.13ms\n",
      "iter 41400/41404/48828: loss 2.4256, time 103.54ms\n",
      "iter 41500/41536/48828: loss 2.4448, time 103.02ms\n",
      "iter 41600/41624/48828: loss 2.4390, time 102.69ms\n",
      "iter 41700/41712/48828: loss 2.4375, time 102.95ms\n",
      "iter 41800/41844/48828: loss 2.4254, time 111.66ms\n",
      "iter 41900/41932/48828: loss 2.4287, time 103.42ms\n",
      "step 42000: train loss 2.4245, val loss 2.4239\n",
      "iter 42000/42020/48828: loss 2.4197, time 3158.41ms\n",
      "iter 42100/42108/48828: loss 2.4147, time 103.35ms\n",
      "iter 42200/42240/48828: loss 2.4097, time 108.73ms\n",
      "iter 42300/42328/48828: loss 2.3987, time 108.24ms\n",
      "iter 42400/42416/48828: loss 2.4085, time 104.50ms\n",
      "iter 42500/42504/48828: loss 2.4176, time 105.33ms\n",
      "iter 42600/42636/48828: loss 2.4014, time 104.04ms\n",
      "iter 42700/42724/48828: loss 2.4038, time 104.28ms\n",
      "iter 42800/42812/48828: loss 2.4083, time 268.88ms\n",
      "iter 42900/42944/48828: loss 2.3899, time 112.43ms\n",
      "step 43000: train loss 2.4090, val loss 2.4095\n",
      "iter 43000/43032/48828: loss 2.4139, time 3184.07ms\n",
      "iter 43100/43120/48828: loss 2.4114, time 104.32ms\n",
      "iter 43200/43208/48828: loss 2.4016, time 104.25ms\n",
      "iter 43300/43340/48828: loss 2.3988, time 106.02ms\n",
      "iter 43400/43428/48828: loss 2.3914, time 104.04ms\n",
      "iter 43500/43516/48828: loss 2.4132, time 103.22ms\n",
      "iter 43600/43604/48828: loss 2.4318, time 98.51ms\n",
      "iter 43700/43736/48828: loss 2.3943, time 109.58ms\n",
      "iter 43800/43824/48828: loss 2.3943, time 108.18ms\n",
      "iter 43900/43912/48828: loss 2.4090, time 105.39ms\n",
      "step 44000: train loss 2.3999, val loss 2.4015\n",
      "iter 44000/44044/48828: loss 2.3969, time 3169.40ms\n",
      "iter 44100/44132/48828: loss 2.3890, time 104.07ms\n",
      "iter 44200/44220/48828: loss 2.3685, time 104.55ms\n",
      "iter 44300/44308/48828: loss 2.3848, time 103.59ms\n",
      "iter 44400/44440/48828: loss 2.3555, time 108.98ms\n",
      "iter 44500/44528/48828: loss 2.3365, time 103.53ms\n",
      "iter 44600/44616/48828: loss 2.3528, time 103.75ms\n",
      "iter 44700/44704/48828: loss 2.3519, time 104.23ms\n",
      "iter 44800/44836/48828: loss 2.3427, time 102.94ms\n",
      "iter 44900/44924/48828: loss 2.3569, time 103.83ms\n",
      "step 45000: train loss 2.3423, val loss 2.3438\n",
      "iter 45000/45012/48828: loss 2.3349, time 3169.27ms\n",
      "iter 45100/45144/48828: loss 2.3545, time 99.87ms\n",
      "iter 45200/45232/48828: loss 2.3671, time 102.47ms\n",
      "iter 45300/45320/48828: loss 2.3402, time 104.51ms\n",
      "iter 45400/45408/48828: loss 2.3413, time 114.22ms\n",
      "iter 45500/45540/48828: loss 2.3419, time 103.37ms\n",
      "iter 45600/45628/48828: loss 2.3422, time 104.66ms\n",
      "iter 45700/45716/48828: loss 2.3155, time 104.49ms\n",
      "iter 45800/45804/48828: loss 2.3264, time 103.75ms\n",
      "iter 45900/45936/48828: loss 2.3335, time 104.24ms\n",
      "step 46000: train loss 2.3302, val loss 2.3289\n",
      "iter 46000/46024/48828: loss 2.3214, time 3147.56ms\n",
      "iter 46100/46112/48828: loss 2.3351, time 104.08ms\n",
      "iter 46200/46244/48828: loss 2.2996, time 100.05ms\n",
      "iter 46300/46332/48828: loss 2.3131, time 104.50ms\n",
      "iter 46400/46420/48828: loss 2.3121, time 103.97ms\n",
      "iter 46500/46508/48828: loss 2.3187, time 103.87ms\n",
      "iter 46600/46640/48828: loss 2.3148, time 104.30ms\n",
      "iter 46700/46728/48828: loss 2.3147, time 103.27ms\n",
      "iter 46800/46816/48828: loss 2.3239, time 105.14ms\n",
      "iter 46900/46904/48828: loss 2.3220, time 104.74ms\n",
      "step 47000: train loss 2.2966, val loss 2.2994\n",
      "iter 47000/47036/48828: loss 2.3104, time 3176.64ms\n",
      "iter 47100/47124/48828: loss 2.2942, time 103.71ms\n",
      "iter 47200/47212/48828: loss 2.3182, time 104.33ms\n",
      "iter 47300/47344/48828: loss 2.2841, time 110.29ms\n",
      "iter 47400/47432/48828: loss 2.2805, time 103.35ms\n",
      "iter 47500/47520/48828: loss 2.2601, time 105.02ms\n",
      "iter 47600/47608/48828: loss 2.2806, time 102.64ms\n",
      "iter 47700/47740/48828: loss 2.2841, time 102.62ms\n",
      "iter 47800/47828/48828: loss 2.2750, time 98.69ms\n",
      "iter 47900/47916/48828: loss 2.2831, time 104.08ms\n",
      "step 48000: train loss 2.2644, val loss 2.2607\n",
      "iter 48000/48004/48828: loss 2.2764, time 3001.81ms\n",
      "iter 48100/48136/48828: loss 2.2592, time 103.03ms\n",
      "iter 48200/48224/48828: loss 2.2613, time 104.28ms\n",
      "iter 48300/48312/48828: loss 2.2486, time 104.17ms\n",
      "iter 48400/48444/48828: loss 2.2354, time 101.68ms\n",
      "iter 48500/48532/48828: loss 2.2446, time 104.77ms\n",
      "iter 48600/48620/48828: loss 2.2441, time 104.06ms\n",
      "iter 48700/48708/48828: loss 2.2406, time 104.04ms\n",
      "iter 48800/48828/48828: loss 2.2285, time 103.62ms\n",
      "\n",
      "\n",
      "## Running generation 10 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [15:36<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=40.74% win[2]=59.26%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 40, mean: 12.99\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=987 win[1]=23.20% counts=Counter({2: 758, 1: 229}), win[2]=76.80% draw=0.00%\n",
      "  a=2: n=769 win[1]=45.25% counts=Counter({2: 421, 1: 348}), win[2]=54.75% draw=0.00%\n",
      "  a=3: n=1000 win[1]=32.50% counts=Counter({2: 675, 1: 325}), win[2]=67.50% draw=0.00%\n",
      "  a=4: n=1449 win[1]=75.22% counts=Counter({1: 1090, 2: 359}), win[2]=24.78% draw=0.00%\n",
      "  a=5: n=2709 win[1]=39.83% counts=Counter({2: 1630, 1: 1079}), win[2]=60.17% draw=0.00%\n",
      "  a=6: n=2370 win[1]=36.41% counts=Counter({2: 1507, 1: 863}), win[2]=63.59% draw=0.00%\n",
      "  a=7: n=716 win[1]=19.55% counts=Counter({2: 576, 1: 140}), win[2]=80.45% draw=0.00%\n",
      "Training model on gen-10\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.2463, val loss 2.2477\n",
      "iter 0/49/48828: loss 2.2406, time 3691.87ms\n",
      "iter 100/147/48828: loss 2.5000, time 110.23ms\n",
      "iter 200/245/48828: loss 2.5215, time 105.05ms\n",
      "iter 300/343/48828: loss 2.5093, time 102.16ms\n",
      "iter 400/441/48828: loss 2.5273, time 103.60ms\n",
      "iter 500/539/48828: loss 2.5238, time 104.29ms\n",
      "iter 600/637/48828: loss 2.5252, time 103.70ms\n",
      "iter 700/735/48828: loss 2.5247, time 105.48ms\n",
      "iter 800/833/48828: loss 2.5187, time 105.12ms\n",
      "iter 900/931/48828: loss 2.5322, time 109.24ms\n",
      "step 1000: train loss 2.5282, val loss 2.5271\n",
      "iter 1000/1029/48828: loss 2.5246, time 3337.23ms\n",
      "iter 1100/1127/48828: loss 2.5254, time 103.70ms\n",
      "iter 1200/1225/48828: loss 2.5206, time 103.95ms\n",
      "iter 1300/1323/48828: loss 2.5266, time 110.69ms\n",
      "iter 1400/1421/48828: loss 2.5213, time 105.20ms\n",
      "iter 1500/1519/48828: loss 2.5279, time 105.31ms\n",
      "iter 1600/1617/48828: loss 2.5130, time 105.22ms\n",
      "iter 1700/1715/48828: loss 2.5261, time 312.68ms\n",
      "iter 1800/1813/48828: loss 2.5187, time 104.76ms\n",
      "iter 1900/1911/48828: loss 2.5323, time 108.28ms\n",
      "step 2000: train loss 2.5276, val loss 2.5270\n",
      "iter 2000/2009/48828: loss 2.5276, time 3213.57ms\n",
      "iter 2100/2107/48828: loss 2.5392, time 108.54ms\n",
      "iter 2200/2205/48828: loss 2.5180, time 104.54ms\n",
      "iter 2300/2303/48828: loss 2.5372, time 106.07ms\n",
      "iter 2400/2401/48828: loss 2.5238, time 847.44ms\n",
      "iter 2500/2548/48828: loss 2.5537, time 108.28ms\n",
      "iter 2600/2646/48828: loss 2.5327, time 100.93ms\n",
      "iter 2700/2744/48828: loss 2.5325, time 109.19ms\n",
      "iter 2800/2842/48828: loss 2.5249, time 104.66ms\n",
      "iter 2900/2940/48828: loss 2.5169, time 103.15ms\n",
      "step 3000: train loss 2.5328, val loss 2.5332\n",
      "iter 3000/3038/48828: loss 2.5349, time 3189.57ms\n",
      "iter 3100/3136/48828: loss 2.5138, time 102.83ms\n",
      "iter 3200/3234/48828: loss 2.5314, time 103.18ms\n",
      "iter 3300/3332/48828: loss 2.5160, time 106.29ms\n",
      "iter 3400/3430/48828: loss 2.5200, time 103.54ms\n",
      "iter 3500/3528/48828: loss 2.5094, time 103.33ms\n",
      "iter 3600/3626/48828: loss 2.5121, time 103.31ms\n",
      "iter 3700/3724/48828: loss 2.5326, time 107.97ms\n",
      "iter 3800/3822/48828: loss 2.5254, time 103.20ms\n",
      "iter 3900/3920/48828: loss 2.5240, time 104.05ms\n",
      "step 4000: train loss 2.5186, val loss 2.5181\n",
      "iter 4000/4018/48828: loss 2.5230, time 2981.82ms\n",
      "iter 4100/4116/48828: loss 2.5500, time 104.85ms\n",
      "iter 4200/4214/48828: loss 2.5188, time 311.08ms\n",
      "iter 4300/4312/48828: loss 2.5310, time 103.76ms\n",
      "iter 4400/4410/48828: loss 2.5249, time 103.14ms\n",
      "iter 4500/4508/48828: loss 2.5317, time 104.68ms\n",
      "iter 4600/4606/48828: loss 2.5315, time 108.71ms\n",
      "iter 4700/4704/48828: loss 2.5342, time 102.35ms\n",
      "iter 4800/4802/48828: loss 2.5428, time 103.30ms\n",
      "iter 4900/4949/48828: loss 2.5355, time 106.90ms\n",
      "step 5000: train loss 2.5387, val loss 2.5397\n",
      "iter 5000/5047/48828: loss 2.5382, time 2997.67ms\n",
      "iter 5100/5145/48828: loss 2.5316, time 105.07ms\n",
      "iter 5200/5243/48828: loss 2.5375, time 104.22ms\n",
      "iter 5300/5341/48828: loss 2.5278, time 108.66ms\n",
      "iter 5400/5439/48828: loss 2.5228, time 103.83ms\n",
      "iter 5500/5537/48828: loss 2.5364, time 104.21ms\n",
      "iter 5600/5635/48828: loss 2.5237, time 299.27ms\n",
      "iter 5700/5733/48828: loss 2.5348, time 103.79ms\n",
      "iter 5800/5831/48828: loss 2.5134, time 103.60ms\n",
      "iter 5900/5929/48828: loss 2.5140, time 102.46ms\n",
      "step 6000: train loss 2.5480, val loss 2.5484\n",
      "iter 6000/6027/48828: loss 2.5476, time 2974.18ms\n",
      "iter 6100/6125/48828: loss 2.5237, time 101.00ms\n",
      "iter 6200/6223/48828: loss 2.5210, time 105.00ms\n",
      "iter 6300/6321/48828: loss 2.5297, time 104.95ms\n",
      "iter 6400/6419/48828: loss 2.5256, time 106.99ms\n",
      "iter 6500/6517/48828: loss 2.5116, time 103.92ms\n",
      "iter 6600/6615/48828: loss 2.5373, time 103.40ms\n",
      "iter 6700/6713/48828: loss 2.5291, time 104.29ms\n",
      "iter 6800/6811/48828: loss 2.5230, time 102.87ms\n",
      "iter 6900/6909/48828: loss 2.5254, time 103.04ms\n",
      "step 7000: train loss 2.5284, val loss 2.5286\n",
      "iter 7000/7007/48828: loss 2.5322, time 3221.41ms\n",
      "iter 7100/7105/48828: loss 2.5305, time 103.90ms\n",
      "iter 7200/7203/48828: loss 2.5436, time 105.37ms\n",
      "iter 7300/7301/48828: loss 2.5182, time 909.55ms\n",
      "iter 7400/7448/48828: loss 2.5275, time 104.15ms\n",
      "iter 7500/7546/48828: loss 2.5347, time 102.52ms\n",
      "iter 7600/7644/48828: loss 2.5245, time 104.23ms\n",
      "iter 7700/7742/48828: loss 2.5250, time 105.33ms\n",
      "iter 7800/7840/48828: loss 2.5503, time 103.58ms\n",
      "iter 7900/7938/48828: loss 2.5633, time 102.89ms\n",
      "step 8000: train loss 2.5279, val loss 2.5285\n",
      "iter 8000/8036/48828: loss 2.5301, time 3193.06ms\n",
      "iter 8100/8134/48828: loss 2.5291, time 101.67ms\n",
      "iter 8200/8232/48828: loss 2.5254, time 108.33ms\n",
      "iter 8300/8330/48828: loss 2.5178, time 104.09ms\n",
      "iter 8400/8428/48828: loss 2.5311, time 102.94ms\n",
      "iter 8500/8526/48828: loss 2.5146, time 105.37ms\n",
      "iter 8600/8624/48828: loss 2.5256, time 105.41ms\n",
      "iter 8700/8722/48828: loss 2.5379, time 103.54ms\n",
      "iter 8800/8820/48828: loss 2.5366, time 102.85ms\n",
      "iter 8900/8918/48828: loss 2.5331, time 102.84ms\n",
      "step 9000: train loss 2.5330, val loss 2.5322\n",
      "iter 9000/9016/48828: loss 2.5287, time 3220.53ms\n",
      "iter 9100/9114/48828: loss 2.5300, time 104.07ms\n",
      "iter 9200/9212/48828: loss 2.5234, time 105.68ms\n",
      "iter 9300/9310/48828: loss 2.5368, time 103.70ms\n",
      "iter 9400/9408/48828: loss 2.5235, time 105.65ms\n",
      "iter 9500/9506/48828: loss 2.5182, time 104.40ms\n",
      "iter 9600/9604/48828: loss 2.5309, time 103.46ms\n",
      "iter 9700/9702/48828: loss 2.5366, time 104.33ms\n",
      "iter 9800/9849/48828: loss 2.5152, time 102.20ms\n",
      "iter 9900/9947/48828: loss 2.5534, time 103.97ms\n",
      "step 10000: train loss 2.5265, val loss 2.5273\n",
      "iter 10000/10045/48828: loss 2.5214, time 3205.28ms\n",
      "iter 10100/10143/48828: loss 2.5314, time 103.09ms\n",
      "iter 10200/10241/48828: loss 2.5211, time 103.19ms\n",
      "iter 10300/10339/48828: loss 2.5227, time 111.91ms\n",
      "iter 10400/10437/48828: loss 2.5284, time 102.58ms\n",
      "iter 10500/10535/48828: loss 2.5365, time 103.69ms\n",
      "iter 10600/10633/48828: loss 2.5218, time 103.91ms\n",
      "iter 10700/10731/48828: loss 2.5131, time 104.28ms\n",
      "iter 10800/10829/48828: loss 2.5071, time 108.68ms\n",
      "iter 10900/10927/48828: loss 2.5032, time 105.28ms\n",
      "step 11000: train loss 2.5103, val loss 2.5113\n",
      "iter 11000/11025/48828: loss 2.5172, time 3272.43ms\n",
      "iter 11100/11123/48828: loss 2.5165, time 105.80ms\n",
      "iter 11200/11221/48828: loss 2.5167, time 105.94ms\n",
      "iter 11300/11319/48828: loss 2.5204, time 103.85ms\n",
      "iter 11400/11417/48828: loss 2.5279, time 103.32ms\n",
      "iter 11500/11515/48828: loss 2.5231, time 104.94ms\n",
      "iter 11600/11613/48828: loss 2.5334, time 103.27ms\n",
      "iter 11700/11711/48828: loss 2.5237, time 107.59ms\n",
      "iter 11800/11809/48828: loss 2.5160, time 107.28ms\n",
      "iter 11900/11907/48828: loss 2.5158, time 103.91ms\n",
      "step 12000: train loss 2.5199, val loss 2.5190\n",
      "iter 12000/12005/48828: loss 2.5193, time 3025.02ms\n",
      "iter 12100/12103/48828: loss 2.5327, time 103.84ms\n",
      "iter 12200/12201/48828: loss 2.5163, time 937.22ms\n",
      "iter 12300/12348/48828: loss 2.5310, time 102.74ms\n",
      "iter 12400/12446/48828: loss 2.5372, time 108.76ms\n",
      "iter 12500/12544/48828: loss 2.5375, time 102.23ms\n",
      "iter 12600/12642/48828: loss 2.5276, time 104.44ms\n",
      "iter 12700/12740/48828: loss 2.5220, time 103.18ms\n",
      "iter 12800/12838/48828: loss 2.5503, time 103.43ms\n",
      "iter 12900/12936/48828: loss 2.5332, time 109.43ms\n",
      "step 13000: train loss 2.5360, val loss 2.5357\n",
      "iter 13000/13034/48828: loss 2.5406, time 3015.03ms\n",
      "iter 13100/13132/48828: loss 2.5445, time 103.85ms\n",
      "iter 13200/13230/48828: loss 2.5235, time 102.76ms\n",
      "iter 13300/13328/48828: loss 2.5222, time 939.01ms\n",
      "iter 13400/13426/48828: loss 2.5333, time 109.42ms\n",
      "iter 13500/13524/48828: loss 2.5229, time 104.44ms\n",
      "iter 13600/13622/48828: loss 2.5197, time 103.68ms\n",
      "iter 13700/13720/48828: loss 2.5216, time 104.00ms\n",
      "iter 13800/13818/48828: loss 2.5254, time 303.42ms\n",
      "iter 13900/13916/48828: loss 2.5210, time 104.93ms\n",
      "step 14000: train loss 2.5310, val loss 2.5312\n",
      "iter 14000/14014/48828: loss 2.5331, time 3012.08ms\n",
      "iter 14100/14112/48828: loss 2.5185, time 104.11ms\n",
      "iter 14200/14210/48828: loss 2.5425, time 104.48ms\n",
      "iter 14300/14308/48828: loss 2.5359, time 102.35ms\n",
      "iter 14400/14406/48828: loss 2.5283, time 105.27ms\n",
      "iter 14500/14504/48828: loss 2.5256, time 109.44ms\n",
      "iter 14600/14602/48828: loss 2.5276, time 104.69ms\n",
      "iter 14700/14749/48828: loss 2.5350, time 105.91ms\n",
      "iter 14800/14847/48828: loss 2.5465, time 102.26ms\n",
      "iter 14900/14945/48828: loss 2.5260, time 104.43ms\n",
      "step 15000: train loss 2.5184, val loss 2.5196\n",
      "iter 15000/15043/48828: loss 2.5204, time 3248.00ms\n",
      "iter 15100/15141/48828: loss 2.5378, time 102.67ms\n",
      "iter 15200/15239/48828: loss 2.5410, time 104.23ms\n",
      "iter 15300/15337/48828: loss 2.5342, time 110.46ms\n",
      "iter 15400/15435/48828: loss 2.5361, time 104.39ms\n",
      "iter 15500/15533/48828: loss 2.5180, time 112.57ms\n",
      "iter 15600/15631/48828: loss 2.5154, time 102.85ms\n",
      "iter 15700/15729/48828: loss 2.5305, time 113.19ms\n",
      "iter 15800/15827/48828: loss 2.5413, time 105.59ms\n",
      "iter 15900/15925/48828: loss 2.5229, time 103.93ms\n",
      "step 16000: train loss 2.5284, val loss 2.5276\n",
      "iter 16000/16023/48828: loss 2.5238, time 3237.72ms\n",
      "iter 16100/16121/48828: loss 2.5491, time 103.17ms\n",
      "iter 16200/16219/48828: loss 2.5258, time 103.93ms\n",
      "iter 16300/16317/48828: loss 2.5077, time 103.27ms\n",
      "iter 16400/16415/48828: loss 2.5473, time 102.88ms\n",
      "iter 16500/16513/48828: loss 2.5298, time 107.68ms\n",
      "iter 16600/16611/48828: loss 2.5159, time 105.72ms\n",
      "iter 16700/16709/48828: loss 2.5300, time 104.18ms\n",
      "iter 16800/16807/48828: loss 2.5117, time 103.83ms\n",
      "iter 16900/16905/48828: loss 2.5112, time 105.42ms\n",
      "step 17000: train loss 2.5123, val loss 2.5125\n",
      "iter 17000/17003/48828: loss 2.5152, time 4226.77ms\n",
      "iter 17100/17101/48828: loss 2.5202, time 918.91ms\n",
      "iter 17200/17248/48828: loss 2.5286, time 104.48ms\n",
      "iter 17300/17346/48828: loss 2.5239, time 103.63ms\n",
      "iter 17400/17444/48828: loss 2.5273, time 102.48ms\n",
      "iter 17500/17542/48828: loss 2.5400, time 104.64ms\n",
      "iter 17600/17640/48828: loss 2.5181, time 109.86ms\n",
      "iter 17700/17738/48828: loss 2.5226, time 103.49ms\n",
      "iter 17800/17836/48828: loss 2.5602, time 107.17ms\n",
      "iter 17900/17934/48828: loss 2.5375, time 105.32ms\n",
      "step 18000: train loss 2.5351, val loss 2.5354\n",
      "iter 18000/18032/48828: loss 2.5386, time 3223.73ms\n",
      "iter 18100/18130/48828: loss 2.5371, time 104.63ms\n",
      "iter 18200/18228/48828: loss 2.5189, time 107.99ms\n",
      "iter 18300/18326/48828: loss 2.5351, time 103.54ms\n",
      "iter 18400/18424/48828: loss 2.5436, time 104.27ms\n",
      "iter 18500/18522/48828: loss 2.5336, time 103.61ms\n",
      "iter 18600/18620/48828: loss 2.5265, time 102.26ms\n",
      "iter 18700/18718/48828: loss 2.5332, time 102.66ms\n",
      "iter 18800/18816/48828: loss 2.5310, time 104.36ms\n",
      "iter 18900/18914/48828: loss 2.5259, time 105.99ms\n",
      "step 19000: train loss 2.5192, val loss 2.5198\n",
      "iter 19000/19012/48828: loss 2.5225, time 3281.92ms\n",
      "iter 19100/19110/48828: loss 2.5483, time 104.62ms\n",
      "iter 19200/19208/48828: loss 2.5359, time 103.70ms\n",
      "iter 19300/19306/48828: loss 2.5205, time 104.75ms\n",
      "iter 19400/19404/48828: loss 2.5266, time 104.47ms\n",
      "iter 19500/19502/48828: loss 2.5155, time 103.55ms\n",
      "iter 19600/19649/48828: loss 2.5130, time 115.62ms\n",
      "iter 19700/19747/48828: loss 2.5310, time 104.21ms\n",
      "iter 19800/19845/48828: loss 2.5169, time 104.36ms\n",
      "iter 19900/19943/48828: loss 2.5183, time 103.67ms\n",
      "step 20000: train loss 2.5103, val loss 2.5108\n",
      "iter 20000/20041/48828: loss 2.5094, time 3241.02ms\n",
      "iter 20100/20139/48828: loss 2.5200, time 105.17ms\n",
      "iter 20200/20237/48828: loss 2.5286, time 104.14ms\n",
      "iter 20300/20335/48828: loss 2.5073, time 104.70ms\n",
      "iter 20400/20433/48828: loss 2.5256, time 103.63ms\n",
      "iter 20500/20531/48828: loss 2.5049, time 119.36ms\n",
      "iter 20600/20629/48828: loss 2.5362, time 104.44ms\n",
      "iter 20700/20727/48828: loss 2.5337, time 104.78ms\n",
      "iter 20800/20825/48828: loss 2.5163, time 104.83ms\n",
      "iter 20900/20923/48828: loss 2.5403, time 103.43ms\n",
      "step 21000: train loss 2.5480, val loss 2.5480\n",
      "iter 21000/21021/48828: loss 2.5447, time 3217.77ms\n",
      "iter 21100/21119/48828: loss 2.5360, time 105.36ms\n",
      "iter 21200/21217/48828: loss 2.5221, time 104.80ms\n",
      "iter 21300/21315/48828: loss 2.5269, time 104.89ms\n",
      "iter 21400/21413/48828: loss 2.5345, time 105.08ms\n",
      "iter 21500/21511/48828: loss 2.5297, time 104.68ms\n",
      "iter 21600/21609/48828: loss 2.5269, time 110.26ms\n",
      "iter 21700/21707/48828: loss 2.5220, time 104.21ms\n",
      "iter 21800/21805/48828: loss 2.5223, time 104.87ms\n",
      "iter 21900/21903/48828: loss 2.5344, time 104.95ms\n",
      "step 22000: train loss 2.5312, val loss 2.5306\n",
      "iter 22000/22001/48828: loss 2.5306, time 4027.49ms\n",
      "iter 22100/22148/48828: loss 2.5301, time 103.60ms\n",
      "iter 22200/22246/48828: loss 2.5242, time 101.83ms\n",
      "iter 22300/22344/48828: loss 2.5324, time 103.92ms\n",
      "iter 22400/22442/48828: loss 2.5152, time 102.16ms\n",
      "iter 22500/22540/48828: loss 2.5041, time 104.40ms\n",
      "iter 22600/22638/48828: loss 2.5216, time 423.84ms\n",
      "iter 22700/22736/48828: loss 2.5170, time 108.85ms\n",
      "iter 22800/22834/48828: loss 2.5297, time 359.16ms\n",
      "iter 22900/22932/48828: loss 2.5217, time 106.28ms\n",
      "step 23000: train loss 2.5243, val loss 2.5246\n",
      "iter 23000/23030/48828: loss 2.5287, time 3018.47ms\n",
      "iter 23100/23128/48828: loss 2.5144, time 103.45ms\n",
      "iter 23200/23226/48828: loss 2.5230, time 105.01ms\n",
      "iter 23300/23324/48828: loss 2.5189, time 103.46ms\n",
      "iter 23400/23422/48828: loss 2.5085, time 104.55ms\n",
      "iter 23500/23520/48828: loss 2.5116, time 104.41ms\n",
      "iter 23600/23618/48828: loss 2.5036, time 104.11ms\n",
      "iter 23700/23716/48828: loss 2.4988, time 110.17ms\n",
      "iter 23800/23814/48828: loss 2.5119, time 104.29ms\n",
      "iter 23900/23912/48828: loss 2.5340, time 110.80ms\n",
      "step 24000: train loss 2.5255, val loss 2.5248\n",
      "iter 24000/24010/48828: loss 2.5250, time 3014.61ms\n",
      "iter 24100/24108/48828: loss 2.5337, time 104.15ms\n",
      "iter 24200/24206/48828: loss 2.5272, time 103.67ms\n",
      "iter 24300/24304/48828: loss 2.5269, time 104.31ms\n",
      "iter 24400/24402/48828: loss 2.5043, time 105.02ms\n",
      "iter 24500/24549/48828: loss 2.5249, time 103.59ms\n",
      "iter 24600/24647/48828: loss 2.5329, time 103.58ms\n",
      "iter 24700/24745/48828: loss 2.5217, time 104.23ms\n",
      "iter 24800/24843/48828: loss 2.5088, time 104.05ms\n",
      "iter 24900/24941/48828: loss 2.5011, time 109.42ms\n",
      "step 25000: train loss 2.5068, val loss 2.5069\n",
      "iter 25000/25039/48828: loss 2.5020, time 3230.34ms\n",
      "iter 25100/25137/48828: loss 2.4946, time 108.08ms\n",
      "iter 25200/25235/48828: loss 2.5276, time 104.43ms\n",
      "iter 25300/25333/48828: loss 2.5358, time 104.67ms\n",
      "iter 25400/25431/48828: loss 2.5014, time 102.60ms\n",
      "iter 25500/25529/48828: loss 2.5060, time 103.27ms\n",
      "iter 25600/25627/48828: loss 2.5059, time 104.91ms\n",
      "iter 25700/25725/48828: loss 2.5185, time 105.01ms\n",
      "iter 25800/25823/48828: loss 2.5093, time 104.50ms\n",
      "iter 25900/25921/48828: loss 2.5118, time 104.53ms\n",
      "step 26000: train loss 2.5365, val loss 2.5360\n",
      "iter 26000/26019/48828: loss 2.5221, time 3283.39ms\n",
      "iter 26100/26117/48828: loss 2.5290, time 102.93ms\n",
      "iter 26200/26215/48828: loss 2.5105, time 104.25ms\n",
      "iter 26300/26313/48828: loss 2.4885, time 103.07ms\n",
      "iter 26400/26411/48828: loss 2.4955, time 104.82ms\n",
      "iter 26500/26509/48828: loss 2.5080, time 103.17ms\n",
      "iter 26600/26607/48828: loss 2.5138, time 106.24ms\n",
      "iter 26700/26705/48828: loss 2.5295, time 104.09ms\n",
      "iter 26800/26803/48828: loss 2.4982, time 104.16ms\n",
      "iter 26900/26901/48828: loss 2.5249, time 1041.16ms\n",
      "step 27000: train loss 2.5186, val loss 2.5183\n",
      "iter 27000/27048/48828: loss 2.5146, time 3277.43ms\n",
      "iter 27100/27146/48828: loss 2.5112, time 107.46ms\n",
      "iter 27200/27244/48828: loss 2.5324, time 119.37ms\n",
      "iter 27300/27342/48828: loss 2.5157, time 104.20ms\n",
      "iter 27400/27440/48828: loss 2.4989, time 104.08ms\n",
      "iter 27500/27538/48828: loss 2.5164, time 105.62ms\n",
      "iter 27600/27636/48828: loss 2.5159, time 108.83ms\n",
      "iter 27700/27734/48828: loss 2.5196, time 104.74ms\n",
      "iter 27800/27832/48828: loss 2.5373, time 109.18ms\n",
      "iter 27900/27930/48828: loss 2.5086, time 103.76ms\n",
      "step 28000: train loss 2.4985, val loss 2.4970\n",
      "iter 28000/28028/48828: loss 2.4955, time 3255.26ms\n",
      "iter 28100/28126/48828: loss 2.5029, time 108.92ms\n",
      "iter 28200/28224/48828: loss 2.4949, time 105.27ms\n",
      "iter 28300/28322/48828: loss 2.4739, time 104.80ms\n",
      "iter 28400/28420/48828: loss 2.4920, time 103.48ms\n",
      "iter 28500/28518/48828: loss 2.4884, time 109.98ms\n",
      "iter 28600/28616/48828: loss 2.5120, time 109.80ms\n",
      "iter 28700/28714/48828: loss 2.5188, time 100.95ms\n",
      "iter 28800/28812/48828: loss 2.5134, time 104.24ms\n",
      "iter 28900/28910/48828: loss 2.4996, time 103.85ms\n",
      "step 29000: train loss 2.5274, val loss 2.5269\n",
      "iter 29000/29008/48828: loss 2.5256, time 3229.66ms\n",
      "iter 29100/29106/48828: loss 2.5333, time 104.51ms\n",
      "iter 29200/29204/48828: loss 2.5288, time 105.24ms\n",
      "iter 29300/29302/48828: loss 2.5081, time 104.47ms\n",
      "iter 29400/29449/48828: loss 2.5218, time 116.20ms\n",
      "iter 29500/29547/48828: loss 2.5194, time 104.19ms\n",
      "iter 29600/29645/48828: loss 2.5179, time 103.66ms\n",
      "iter 29700/29743/48828: loss 2.5207, time 105.69ms\n",
      "iter 29800/29841/48828: loss 2.5085, time 103.47ms\n",
      "iter 29900/29939/48828: loss 2.4772, time 107.93ms\n",
      "step 30000: train loss 2.4795, val loss 2.4796\n",
      "iter 30000/30037/48828: loss 2.4757, time 3242.13ms\n",
      "iter 30100/30135/48828: loss 2.4800, time 103.99ms\n",
      "iter 30200/30233/48828: loss 2.5084, time 103.55ms\n",
      "iter 30300/30331/48828: loss 2.5108, time 103.10ms\n",
      "iter 30400/30429/48828: loss 2.5116, time 103.66ms\n",
      "iter 30500/30527/48828: loss 2.5054, time 105.36ms\n",
      "iter 30600/30625/48828: loss 2.5234, time 105.21ms\n",
      "iter 30700/30723/48828: loss 2.4977, time 109.25ms\n",
      "iter 30800/30821/48828: loss 2.5094, time 105.58ms\n",
      "iter 30900/30919/48828: loss 2.5140, time 105.24ms\n",
      "step 31000: train loss 2.5012, val loss 2.5010\n",
      "iter 31000/31017/48828: loss 2.5073, time 3037.06ms\n",
      "iter 31100/31115/48828: loss 2.5220, time 104.12ms\n",
      "iter 31200/31213/48828: loss 2.5185, time 102.68ms\n",
      "iter 31300/31311/48828: loss 2.5385, time 103.75ms\n",
      "iter 31400/31409/48828: loss 2.5400, time 104.43ms\n",
      "iter 31500/31507/48828: loss 2.5212, time 104.41ms\n",
      "iter 31600/31605/48828: loss 2.5108, time 103.93ms\n",
      "iter 31700/31703/48828: loss 2.5223, time 103.90ms\n",
      "iter 31800/31801/48828: loss 2.5114, time 1122.60ms\n",
      "iter 31900/31948/48828: loss 2.5069, time 105.09ms\n",
      "step 32000: train loss 2.5108, val loss 2.5108\n",
      "iter 32000/32046/48828: loss 2.5103, time 3091.93ms\n",
      "iter 32100/32144/48828: loss 2.5069, time 104.46ms\n",
      "iter 32200/32242/48828: loss 2.4970, time 106.19ms\n",
      "iter 32300/32340/48828: loss 2.5084, time 105.18ms\n",
      "iter 32400/32438/48828: loss 2.4854, time 102.02ms\n",
      "iter 32500/32536/48828: loss 2.4967, time 109.71ms\n",
      "iter 32600/32634/48828: loss 2.4976, time 103.91ms\n",
      "iter 32700/32732/48828: loss 2.5104, time 105.33ms\n",
      "iter 32800/32830/48828: loss 2.4896, time 106.17ms\n",
      "iter 32900/32928/48828: loss 2.4751, time 105.76ms\n",
      "step 33000: train loss 2.5013, val loss 2.5012\n",
      "iter 33000/33026/48828: loss 2.4967, time 3040.14ms\n",
      "iter 33100/33124/48828: loss 2.5053, time 104.11ms\n",
      "iter 33200/33222/48828: loss 2.5011, time 103.39ms\n",
      "iter 33300/33320/48828: loss 2.5078, time 104.94ms\n",
      "iter 33400/33418/48828: loss 2.5115, time 105.18ms\n",
      "iter 33500/33516/48828: loss 2.5135, time 105.22ms\n",
      "iter 33600/33614/48828: loss 2.5172, time 103.52ms\n",
      "iter 33700/33712/48828: loss 2.5082, time 111.31ms\n",
      "iter 33800/33810/48828: loss 2.4855, time 105.20ms\n",
      "iter 33900/33908/48828: loss 2.5015, time 105.29ms\n",
      "step 34000: train loss 2.4997, val loss 2.4984\n",
      "iter 34000/34006/48828: loss 2.4987, time 3036.37ms\n",
      "iter 34100/34104/48828: loss 2.4774, time 103.64ms\n",
      "iter 34200/34202/48828: loss 2.5108, time 104.73ms\n",
      "iter 34300/34349/48828: loss 2.4946, time 102.12ms\n",
      "iter 34400/34447/48828: loss 2.4857, time 103.64ms\n",
      "iter 34500/34545/48828: loss 2.4757, time 104.90ms\n",
      "iter 34600/34643/48828: loss 2.5123, time 106.18ms\n",
      "iter 34700/34741/48828: loss 2.4785, time 104.46ms\n",
      "iter 34800/34839/48828: loss 2.5184, time 111.31ms\n",
      "iter 34900/34937/48828: loss 2.5158, time 109.73ms\n",
      "step 35000: train loss 2.4938, val loss 2.4936\n",
      "iter 35000/35035/48828: loss 2.4889, time 3225.71ms\n",
      "iter 35100/35133/48828: loss 2.4854, time 101.70ms\n",
      "iter 35200/35231/48828: loss 2.4828, time 109.75ms\n",
      "iter 35300/35329/48828: loss 2.4596, time 103.37ms\n",
      "iter 35400/35427/48828: loss 2.4790, time 105.02ms\n",
      "iter 35500/35525/48828: loss 2.4841, time 104.78ms\n",
      "iter 35600/35623/48828: loss 2.5151, time 104.37ms\n",
      "iter 35700/35721/48828: loss 2.4854, time 99.36ms\n",
      "iter 35800/35819/48828: loss 2.5091, time 104.38ms\n",
      "iter 35900/35917/48828: loss 2.5067, time 104.71ms\n",
      "step 36000: train loss 2.4639, val loss 2.4632\n",
      "iter 36000/36015/48828: loss 2.4672, time 3217.82ms\n",
      "iter 36100/36113/48828: loss 2.4846, time 102.92ms\n",
      "iter 36200/36211/48828: loss 2.4788, time 103.57ms\n",
      "iter 36300/36309/48828: loss 2.4668, time 103.43ms\n",
      "iter 36400/36407/48828: loss 2.4691, time 108.39ms\n",
      "iter 36500/36505/48828: loss 2.4684, time 103.72ms\n",
      "iter 36600/36603/48828: loss 2.4773, time 107.15ms\n",
      "iter 36700/36701/48828: loss 2.4694, time 1097.02ms\n",
      "iter 36800/36848/48828: loss 2.4679, time 104.66ms\n",
      "iter 36900/36946/48828: loss 2.4755, time 102.48ms\n",
      "step 37000: train loss 2.4857, val loss 2.4865\n",
      "iter 37000/37044/48828: loss 2.4865, time 3301.26ms\n",
      "iter 37100/37142/48828: loss 2.5039, time 106.21ms\n",
      "iter 37200/37240/48828: loss 2.5036, time 104.86ms\n",
      "iter 37300/37338/48828: loss 2.4840, time 109.70ms\n",
      "iter 37400/37436/48828: loss 2.4720, time 108.29ms\n",
      "iter 37500/37534/48828: loss 2.4607, time 102.54ms\n",
      "iter 37600/37632/48828: loss 2.4854, time 104.23ms\n",
      "iter 37700/37730/48828: loss 2.4679, time 305.69ms\n",
      "iter 37800/37828/48828: loss 2.4495, time 112.15ms\n",
      "iter 37900/37926/48828: loss 2.4497, time 103.25ms\n",
      "step 38000: train loss 2.4421, val loss 2.4418\n",
      "iter 38000/38024/48828: loss 2.4363, time 3317.22ms\n",
      "iter 38100/38122/48828: loss 2.4473, time 105.15ms\n",
      "iter 38200/38220/48828: loss 2.4270, time 108.57ms\n",
      "iter 38300/38318/48828: loss 2.4929, time 104.93ms\n",
      "iter 38400/38416/48828: loss 2.4678, time 105.49ms\n",
      "iter 38500/38514/48828: loss 2.4502, time 105.21ms\n",
      "iter 38600/38612/48828: loss 2.4658, time 105.40ms\n",
      "iter 38700/38710/48828: loss 2.4840, time 103.07ms\n",
      "iter 38800/38808/48828: loss 2.4942, time 104.86ms\n",
      "iter 38900/38906/48828: loss 2.5042, time 101.87ms\n",
      "step 39000: train loss 2.4747, val loss 2.4743\n",
      "iter 39000/39004/48828: loss 2.4789, time 3273.45ms\n",
      "iter 39100/39102/48828: loss 2.4710, time 125.80ms\n",
      "iter 39200/39249/48828: loss 2.4755, time 99.93ms\n",
      "iter 39300/39347/48828: loss 2.4509, time 104.68ms\n",
      "iter 39400/39445/48828: loss 2.4572, time 121.61ms\n",
      "iter 39500/39543/48828: loss 2.4761, time 103.99ms\n",
      "iter 39600/39641/48828: loss 2.4521, time 108.61ms\n",
      "iter 39700/39739/48828: loss 2.4750, time 104.95ms\n",
      "iter 39800/39837/48828: loss 2.4436, time 105.73ms\n",
      "iter 39900/39935/48828: loss 2.4597, time 105.49ms\n",
      "step 40000: train loss 2.4474, val loss 2.4497\n",
      "iter 40000/40033/48828: loss 2.4368, time 3291.65ms\n",
      "iter 40100/40131/48828: loss 2.4529, time 120.67ms\n",
      "iter 40200/40229/48828: loss 2.4469, time 102.88ms\n",
      "iter 40300/40327/48828: loss 2.4566, time 110.87ms\n",
      "iter 40400/40425/48828: loss 2.4400, time 103.82ms\n",
      "iter 40500/40523/48828: loss 2.4193, time 106.14ms\n",
      "iter 40600/40621/48828: loss 2.4578, time 108.26ms\n",
      "iter 40700/40719/48828: loss 2.4548, time 103.44ms\n",
      "iter 40800/40817/48828: loss 2.4537, time 104.62ms\n",
      "iter 40900/40915/48828: loss 2.4507, time 110.84ms\n",
      "step 41000: train loss 2.4390, val loss 2.4404\n",
      "iter 41000/41013/48828: loss 2.4486, time 3082.02ms\n",
      "iter 41100/41111/48828: loss 2.4520, time 106.10ms\n",
      "iter 41200/41209/48828: loss 2.4489, time 104.68ms\n",
      "iter 41300/41307/48828: loss 2.4550, time 108.24ms\n",
      "iter 41400/41405/48828: loss 2.4269, time 106.26ms\n",
      "iter 41500/41503/48828: loss 2.4415, time 106.33ms\n",
      "iter 41600/41601/48828: loss 2.4346, time 402.07ms\n",
      "iter 41700/41748/48828: loss 2.4084, time 106.78ms\n",
      "iter 41800/41846/48828: loss 2.4300, time 108.60ms\n",
      "iter 41900/41944/48828: loss 2.4235, time 112.28ms\n",
      "step 42000: train loss 2.4255, val loss 2.4243\n",
      "iter 42000/42042/48828: loss 2.4335, time 3019.69ms\n",
      "iter 42100/42140/48828: loss 2.4186, time 109.56ms\n",
      "iter 42200/42238/48828: loss 2.4256, time 105.42ms\n",
      "iter 42300/42336/48828: loss 2.4197, time 104.02ms\n",
      "iter 42400/42434/48828: loss 2.4335, time 102.94ms\n",
      "iter 42500/42532/48828: loss 2.4304, time 103.98ms\n",
      "iter 42600/42630/48828: loss 2.4025, time 102.83ms\n",
      "iter 42700/42728/48828: loss 2.4067, time 110.38ms\n",
      "iter 42800/42826/48828: loss 2.4298, time 103.26ms\n",
      "iter 42900/42924/48828: loss 2.4188, time 104.59ms\n",
      "step 43000: train loss 2.4120, val loss 2.4092\n",
      "iter 43000/43022/48828: loss 2.4134, time 3012.87ms\n",
      "iter 43100/43120/48828: loss 2.4239, time 103.73ms\n",
      "iter 43200/43218/48828: loss 2.4061, time 103.00ms\n",
      "iter 43300/43316/48828: loss 2.4055, time 105.92ms\n",
      "iter 43400/43414/48828: loss 2.4020, time 105.62ms\n",
      "iter 43500/43512/48828: loss 2.4241, time 104.65ms\n",
      "iter 43600/43610/48828: loss 2.4048, time 104.99ms\n",
      "iter 43700/43708/48828: loss 2.4184, time 105.15ms\n",
      "iter 43800/43806/48828: loss 2.3981, time 105.56ms\n",
      "iter 43900/43904/48828: loss 2.4230, time 103.39ms\n",
      "step 44000: train loss 2.4013, val loss 2.3974\n",
      "iter 44000/44002/48828: loss 2.3929, time 3251.64ms\n",
      "iter 44100/44149/48828: loss 2.4012, time 103.01ms\n",
      "iter 44200/44247/48828: loss 2.4049, time 143.75ms\n",
      "iter 44300/44345/48828: loss 2.4037, time 105.13ms\n",
      "iter 44400/44443/48828: loss 2.4044, time 104.21ms\n",
      "iter 44500/44541/48828: loss 2.3804, time 105.37ms\n",
      "iter 44600/44639/48828: loss 2.3864, time 104.21ms\n",
      "iter 44700/44737/48828: loss 2.3802, time 103.75ms\n",
      "iter 44800/44835/48828: loss 2.3616, time 103.93ms\n",
      "iter 44900/44933/48828: loss 2.3579, time 105.74ms\n",
      "step 45000: train loss 2.3710, val loss 2.3726\n",
      "iter 45000/45031/48828: loss 2.3742, time 3260.25ms\n",
      "iter 45100/45129/48828: loss 2.3720, time 104.27ms\n",
      "iter 45200/45227/48828: loss 2.3542, time 108.41ms\n",
      "iter 45300/45325/48828: loss 2.3568, time 109.27ms\n",
      "iter 45400/45423/48828: loss 2.3530, time 110.62ms\n",
      "iter 45500/45521/48828: loss 2.3505, time 102.82ms\n",
      "iter 45600/45619/48828: loss 2.3366, time 103.60ms\n",
      "iter 45700/45717/48828: loss 2.3317, time 104.09ms\n",
      "iter 45800/45815/48828: loss 2.3308, time 109.41ms\n",
      "iter 45900/45913/48828: loss 2.3287, time 103.88ms\n",
      "step 46000: train loss 2.3576, val loss 2.3586\n",
      "iter 46000/46011/48828: loss 2.3703, time 3267.75ms\n",
      "iter 46100/46109/48828: loss 2.3357, time 104.13ms\n",
      "iter 46200/46207/48828: loss 2.3307, time 104.57ms\n",
      "iter 46300/46305/48828: loss 2.3227, time 103.13ms\n",
      "iter 46400/46403/48828: loss 2.3395, time 105.64ms\n",
      "iter 46500/46501/48828: loss 2.3095, time 110.39ms\n",
      "iter 46600/46648/48828: loss 2.3237, time 103.70ms\n",
      "iter 46700/46746/48828: loss 2.3139, time 104.96ms\n",
      "iter 46800/46844/48828: loss 2.3459, time 104.62ms\n",
      "iter 46900/46942/48828: loss 2.3200, time 105.99ms\n",
      "step 47000: train loss 2.3315, val loss 2.3308\n",
      "iter 47000/47040/48828: loss 2.3430, time 3234.59ms\n",
      "iter 47100/47138/48828: loss 2.3248, time 104.51ms\n",
      "iter 47200/47236/48828: loss 2.3277, time 103.74ms\n",
      "iter 47300/47334/48828: loss 2.3321, time 104.11ms\n",
      "iter 47400/47432/48828: loss 2.3531, time 104.82ms\n",
      "iter 47500/47530/48828: loss 2.3399, time 101.54ms\n",
      "iter 47600/47628/48828: loss 2.3065, time 104.68ms\n",
      "iter 47700/47726/48828: loss 2.3109, time 102.57ms\n",
      "iter 47800/47824/48828: loss 2.3490, time 105.27ms\n",
      "iter 47900/47922/48828: loss 2.3221, time 103.03ms\n",
      "step 48000: train loss 2.3617, val loss 2.3591\n",
      "iter 48000/48020/48828: loss 2.3629, time 3300.99ms\n",
      "iter 48100/48118/48828: loss 2.3481, time 103.73ms\n",
      "iter 48200/48216/48828: loss 2.3416, time 105.14ms\n",
      "iter 48300/48314/48828: loss 2.3306, time 104.94ms\n",
      "iter 48400/48412/48828: loss 2.3283, time 111.36ms\n",
      "iter 48500/48510/48828: loss 2.3339, time 106.15ms\n",
      "iter 48600/48608/48828: loss 2.3384, time 110.41ms\n",
      "iter 48700/48706/48828: loss 2.3047, time 103.65ms\n",
      "iter 48800/48804/48828: loss 2.3105, time 109.88ms\n",
      "\n",
      "\n",
      "## Running generation 11 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:58<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=88.11% win[2]=11.89%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 36, mean: 9.46\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=38.30% counts=Counter({2: 58, 1: 36}), win[2]=61.70% draw=0.00%\n",
      "  a=2: n=212 win[1]=49.53% counts=Counter({2: 107, 1: 105}), win[2]=50.47% draw=0.00%\n",
      "  a=3: n=731 win[1]=66.07% counts=Counter({1: 483, 2: 248}), win[2]=33.93% draw=0.00%\n",
      "  a=4: n=8296 win[1]=93.66% counts=Counter({1: 7770, 2: 526}), win[2]=6.34% draw=0.00%\n",
      "  a=5: n=191 win[1]=54.97% counts=Counter({1: 105, 2: 86}), win[2]=45.03% draw=0.00%\n",
      "  a=6: n=387 win[1]=73.39% counts=Counter({1: 284, 2: 103}), win[2]=26.61% draw=0.00%\n",
      "  a=7: n= 89 win[1]=31.46% counts=Counter({2: 61, 1: 28}), win[2]=68.54% draw=0.00%\n",
      "Training model on gen-11\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.2953, val loss 2.2938\n",
      "iter 0/54/48828: loss 2.2988, time 25339.29ms\n",
      "iter 100/108/48828: loss 2.5067, time 103.86ms\n",
      "iter 200/216/48828: loss 2.5091, time 112.46ms\n",
      "iter 300/324/48828: loss 2.4882, time 1200.57ms\n",
      "iter 400/432/48828: loss 2.5046, time 1078.55ms\n",
      "iter 500/540/48828: loss 2.5121, time 103.74ms\n",
      "iter 600/648/48828: loss 2.5129, time 105.59ms\n",
      "iter 700/702/48828: loss 2.4935, time 107.20ms\n",
      "iter 800/810/48828: loss 2.5227, time 120.28ms\n",
      "iter 900/918/48828: loss 2.5134, time 344.39ms\n",
      "step 1000: train loss 2.5087, val loss 2.5089\n",
      "iter 1000/1026/48828: loss 2.5019, time 8565.90ms\n",
      "iter 1100/1134/48828: loss 2.5198, time 103.19ms\n",
      "iter 1200/1242/48828: loss 2.5197, time 112.21ms\n",
      "iter 1300/1350/48828: loss 2.5295, time 117.20ms\n",
      "iter 1400/1404/48828: loss 2.5185, time 119.66ms\n",
      "iter 1500/1512/48828: loss 2.5247, time 103.43ms\n",
      "iter 1600/1620/48828: loss 2.5282, time 1146.01ms\n",
      "iter 1700/1728/48828: loss 2.5172, time 1217.50ms\n",
      "iter 1800/1836/48828: loss 2.5210, time 101.96ms\n",
      "iter 1900/1944/48828: loss 2.5083, time 104.25ms\n",
      "step 2000: train loss 2.5112, val loss 2.5119\n",
      "iter 2000/2052/48828: loss 2.5098, time 6486.69ms\n",
      "iter 2100/2106/48828: loss 2.5053, time 103.59ms\n",
      "iter 2200/2214/48828: loss 2.4778, time 108.10ms\n",
      "iter 2300/2322/48828: loss 2.4870, time 103.04ms\n",
      "iter 2400/2430/48828: loss 2.5125, time 104.83ms\n",
      "iter 2500/2538/48828: loss 2.5136, time 103.59ms\n",
      "iter 2600/2646/48828: loss 2.5022, time 111.15ms\n",
      "iter 2700/2754/48828: loss 2.5184, time 106.84ms\n",
      "iter 2800/2808/48828: loss 2.5048, time 104.88ms\n",
      "iter 2900/2916/48828: loss 2.5285, time 103.38ms\n",
      "step 3000: train loss 2.5293, val loss 2.5291\n",
      "iter 3000/3024/48828: loss 2.5253, time 3033.61ms\n",
      "iter 3100/3132/48828: loss 2.5147, time 104.01ms\n",
      "iter 3200/3240/48828: loss 2.5021, time 114.48ms\n",
      "iter 3300/3348/48828: loss 2.5013, time 104.44ms\n",
      "iter 3400/3402/48828: loss 2.5106, time 105.19ms\n",
      "iter 3500/3510/48828: loss 2.5239, time 106.33ms\n",
      "iter 3600/3618/48828: loss 2.5073, time 109.37ms\n",
      "iter 3700/3726/48828: loss 2.5172, time 104.92ms\n",
      "iter 3800/3834/48828: loss 2.5011, time 106.72ms\n",
      "iter 3900/3942/48828: loss 2.4885, time 103.86ms\n",
      "step 4000: train loss 2.5102, val loss 2.5104\n",
      "iter 4000/4050/48828: loss 2.5073, time 4574.39ms\n",
      "iter 4100/4104/48828: loss 2.5123, time 105.34ms\n",
      "iter 4200/4212/48828: loss 2.5121, time 117.93ms\n",
      "iter 4300/4320/48828: loss 2.5037, time 105.03ms\n",
      "iter 4400/4428/48828: loss 2.5059, time 104.77ms\n",
      "iter 4500/4536/48828: loss 2.5203, time 104.06ms\n",
      "iter 4600/4644/48828: loss 2.5168, time 104.08ms\n",
      "iter 4700/4752/48828: loss 2.5249, time 106.12ms\n",
      "iter 4800/4806/48828: loss 2.5015, time 105.06ms\n",
      "iter 4900/4914/48828: loss 2.5249, time 105.67ms\n",
      "step 5000: train loss 2.4989, val loss 2.5003\n",
      "iter 5000/5022/48828: loss 2.5060, time 3035.25ms\n",
      "iter 5100/5130/48828: loss 2.5050, time 104.57ms\n",
      "iter 5200/5238/48828: loss 2.5038, time 111.01ms\n",
      "iter 5300/5346/48828: loss 2.5257, time 104.63ms\n",
      "iter 5400/5454/48828: loss 2.5092, time 104.20ms\n",
      "iter 5500/5508/48828: loss 2.5301, time 105.60ms\n",
      "iter 5600/5616/48828: loss 2.5373, time 110.76ms\n",
      "iter 5700/5724/48828: loss 2.5071, time 113.14ms\n",
      "iter 5800/5832/48828: loss 2.5241, time 103.93ms\n",
      "iter 5900/5940/48828: loss 2.5011, time 105.59ms\n",
      "step 6000: train loss 2.5014, val loss 2.5012\n",
      "iter 6000/6048/48828: loss 2.5093, time 4361.01ms\n",
      "iter 6100/6102/48828: loss 2.5105, time 104.96ms\n",
      "iter 6200/6210/48828: loss 2.5073, time 102.97ms\n",
      "iter 6300/6318/48828: loss 2.5173, time 106.25ms\n",
      "iter 6400/6426/48828: loss 2.5321, time 104.31ms\n",
      "iter 6500/6534/48828: loss 2.5288, time 106.28ms\n",
      "iter 6600/6642/48828: loss 2.5342, time 108.60ms\n",
      "iter 6700/6750/48828: loss 2.5125, time 125.25ms\n",
      "iter 6800/6804/48828: loss 2.5114, time 105.25ms\n",
      "iter 6900/6912/48828: loss 2.5097, time 108.45ms\n",
      "step 7000: train loss 2.5087, val loss 2.5093\n",
      "iter 7000/7020/48828: loss 2.5078, time 3287.57ms\n",
      "iter 7100/7128/48828: loss 2.5135, time 104.01ms\n",
      "iter 7200/7236/48828: loss 2.5095, time 105.87ms\n",
      "iter 7300/7344/48828: loss 2.4985, time 105.51ms\n",
      "iter 7400/7452/48828: loss 2.5162, time 103.57ms\n",
      "iter 7500/7506/48828: loss 2.5165, time 104.69ms\n",
      "iter 7600/7614/48828: loss 2.5243, time 104.98ms\n",
      "iter 7700/7722/48828: loss 2.5090, time 106.53ms\n",
      "iter 7800/7830/48828: loss 2.5008, time 102.43ms\n",
      "iter 7900/7938/48828: loss 2.5228, time 154.89ms\n",
      "step 8000: train loss 2.5121, val loss 2.5132\n",
      "iter 8000/8046/48828: loss 2.5172, time 3026.58ms\n",
      "iter 8100/8154/48828: loss 2.4867, time 101.89ms\n",
      "iter 8200/8208/48828: loss 2.5084, time 103.98ms\n",
      "iter 8300/8316/48828: loss 2.5097, time 400.57ms\n",
      "iter 8400/8424/48828: loss 2.5348, time 103.64ms\n",
      "iter 8500/8532/48828: loss 2.5303, time 109.97ms\n",
      "iter 8600/8640/48828: loss 2.5031, time 105.24ms\n",
      "iter 8700/8748/48828: loss 2.5280, time 107.91ms\n",
      "iter 8800/8802/48828: loss 2.5103, time 105.29ms\n",
      "iter 8900/8910/48828: loss 2.5058, time 104.51ms\n",
      "step 9000: train loss 2.5124, val loss 2.5130\n",
      "iter 9000/9018/48828: loss 2.5165, time 3274.19ms\n",
      "iter 9100/9126/48828: loss 2.5199, time 100.01ms\n",
      "iter 9200/9234/48828: loss 2.5227, time 105.67ms\n",
      "iter 9300/9342/48828: loss 2.4931, time 1580.10ms\n",
      "iter 9400/9450/48828: loss 2.5315, time 103.63ms\n",
      "iter 9500/9504/48828: loss 2.5111, time 104.76ms\n",
      "iter 9600/9612/48828: loss 2.4877, time 110.27ms\n",
      "iter 9700/9720/48828: loss 2.4944, time 103.76ms\n",
      "iter 9800/9828/48828: loss 2.5151, time 105.16ms\n",
      "iter 9900/9936/48828: loss 2.5164, time 102.96ms\n",
      "step 10000: train loss 2.5103, val loss 2.5092\n",
      "iter 10000/10044/48828: loss 2.5064, time 3040.54ms\n",
      "iter 10100/10152/48828: loss 2.5111, time 104.69ms\n",
      "iter 10200/10206/48828: loss 2.5109, time 104.31ms\n",
      "iter 10300/10314/48828: loss 2.5044, time 104.29ms\n",
      "iter 10400/10422/48828: loss 2.5257, time 105.44ms\n",
      "iter 10500/10530/48828: loss 2.5031, time 104.46ms\n",
      "iter 10600/10638/48828: loss 2.5056, time 117.92ms\n",
      "iter 10700/10746/48828: loss 2.5134, time 104.55ms\n",
      "iter 10800/10854/48828: loss 2.5158, time 126.47ms\n",
      "iter 10900/10908/48828: loss 2.5020, time 109.56ms\n",
      "step 11000: train loss 2.5127, val loss 2.5131\n",
      "iter 11000/11016/48828: loss 2.5077, time 3269.05ms\n",
      "iter 11100/11124/48828: loss 2.5075, time 105.18ms\n",
      "iter 11200/11232/48828: loss 2.5103, time 105.19ms\n",
      "iter 11300/11340/48828: loss 2.5198, time 129.06ms\n",
      "iter 11400/11448/48828: loss 2.5090, time 105.28ms\n",
      "iter 11500/11502/48828: loss 2.5177, time 110.67ms\n",
      "iter 11600/11610/48828: loss 2.5183, time 108.83ms\n",
      "iter 11700/11718/48828: loss 2.5032, time 111.09ms\n",
      "iter 11800/11826/48828: loss 2.5241, time 105.08ms\n",
      "iter 11900/11934/48828: loss 2.5019, time 103.93ms\n",
      "step 12000: train loss 2.5063, val loss 2.5074\n",
      "iter 12000/12042/48828: loss 2.5101, time 3284.08ms\n",
      "iter 12100/12150/48828: loss 2.5137, time 103.57ms\n",
      "iter 12200/12204/48828: loss 2.5309, time 104.53ms\n",
      "iter 12300/12312/48828: loss 2.5112, time 104.92ms\n",
      "iter 12400/12420/48828: loss 2.5010, time 105.30ms\n",
      "iter 12500/12528/48828: loss 2.5211, time 109.40ms\n",
      "iter 12600/12636/48828: loss 2.5133, time 105.86ms\n",
      "iter 12700/12744/48828: loss 2.5277, time 112.27ms\n",
      "iter 12800/12852/48828: loss 2.5040, time 101.97ms\n",
      "iter 12900/12906/48828: loss 2.5245, time 105.21ms\n",
      "step 13000: train loss 2.5000, val loss 2.5008\n",
      "iter 13000/13014/48828: loss 2.5035, time 3040.00ms\n",
      "iter 13100/13122/48828: loss 2.5059, time 104.08ms\n",
      "iter 13200/13230/48828: loss 2.5145, time 105.26ms\n",
      "iter 13300/13338/48828: loss 2.4990, time 104.30ms\n",
      "iter 13400/13446/48828: loss 2.5016, time 104.68ms\n",
      "iter 13500/13554/48828: loss 2.5112, time 101.13ms\n",
      "iter 13600/13608/48828: loss 2.5131, time 104.96ms\n",
      "iter 13700/13716/48828: loss 2.5146, time 105.71ms\n",
      "iter 13800/13824/48828: loss 2.5218, time 104.35ms\n",
      "iter 13900/13932/48828: loss 2.4823, time 106.22ms\n",
      "step 14000: train loss 2.5053, val loss 2.5060\n",
      "iter 14000/14040/48828: loss 2.5083, time 3303.59ms\n",
      "iter 14100/14148/48828: loss 2.5084, time 104.38ms\n",
      "iter 14200/14202/48828: loss 2.5028, time 367.69ms\n",
      "iter 14300/14310/48828: loss 2.4980, time 105.47ms\n",
      "iter 14400/14418/48828: loss 2.5068, time 104.17ms\n",
      "iter 14500/14526/48828: loss 2.5138, time 104.79ms\n",
      "iter 14600/14634/48828: loss 2.5087, time 104.27ms\n",
      "iter 14700/14742/48828: loss 2.5216, time 106.56ms\n",
      "iter 14800/14850/48828: loss 2.5024, time 113.08ms\n",
      "iter 14900/14904/48828: loss 2.5089, time 103.83ms\n",
      "step 15000: train loss 2.5123, val loss 2.5136\n",
      "iter 15000/15012/48828: loss 2.5109, time 3049.23ms\n",
      "iter 15100/15120/48828: loss 2.4970, time 103.35ms\n",
      "iter 15200/15228/48828: loss 2.5050, time 104.95ms\n",
      "iter 15300/15336/48828: loss 2.5034, time 105.88ms\n",
      "iter 15400/15444/48828: loss 2.5035, time 104.81ms\n",
      "iter 15500/15552/48828: loss 2.5094, time 103.64ms\n",
      "iter 15600/15606/48828: loss 2.4895, time 119.91ms\n",
      "iter 15700/15714/48828: loss 2.5234, time 105.10ms\n",
      "iter 15800/15822/48828: loss 2.4883, time 104.25ms\n",
      "iter 15900/15930/48828: loss 2.5152, time 134.92ms\n",
      "step 16000: train loss 2.5039, val loss 2.5036\n",
      "iter 16000/16038/48828: loss 2.5155, time 3311.32ms\n",
      "iter 16100/16146/48828: loss 2.5086, time 160.46ms\n",
      "iter 16200/16254/48828: loss 2.5093, time 103.03ms\n",
      "iter 16300/16308/48828: loss 2.5382, time 106.09ms\n",
      "iter 16400/16416/48828: loss 2.4966, time 104.47ms\n",
      "iter 16500/16524/48828: loss 2.5155, time 106.98ms\n",
      "iter 16600/16632/48828: loss 2.5235, time 105.14ms\n",
      "iter 16700/16740/48828: loss 2.5019, time 110.27ms\n",
      "iter 16800/16848/48828: loss 2.5208, time 104.77ms\n",
      "iter 16900/16902/48828: loss 2.5180, time 129.94ms\n",
      "step 17000: train loss 2.4962, val loss 2.4978\n",
      "iter 17000/17010/48828: loss 2.5053, time 3080.65ms\n",
      "iter 17100/17118/48828: loss 2.5109, time 105.36ms\n",
      "iter 17200/17226/48828: loss 2.5126, time 104.85ms\n",
      "iter 17300/17334/48828: loss 2.4931, time 108.48ms\n",
      "iter 17400/17442/48828: loss 2.5192, time 98.96ms\n",
      "iter 17500/17550/48828: loss 2.5166, time 103.09ms\n",
      "iter 17600/17604/48828: loss 2.5168, time 112.52ms\n",
      "iter 17700/17712/48828: loss 2.4978, time 108.76ms\n",
      "iter 17800/17820/48828: loss 2.4778, time 104.71ms\n",
      "iter 17900/17928/48828: loss 2.4764, time 103.70ms\n",
      "step 18000: train loss 2.5088, val loss 2.5079\n",
      "iter 18000/18036/48828: loss 2.5135, time 3308.49ms\n",
      "iter 18100/18144/48828: loss 2.5144, time 104.00ms\n",
      "iter 18200/18252/48828: loss 2.5219, time 103.52ms\n",
      "iter 18300/18306/48828: loss 2.4936, time 106.10ms\n",
      "iter 18400/18414/48828: loss 2.5020, time 105.49ms\n",
      "iter 18500/18522/48828: loss 2.5043, time 104.52ms\n",
      "iter 18600/18630/48828: loss 2.4990, time 104.96ms\n",
      "iter 18700/18738/48828: loss 2.4986, time 105.02ms\n",
      "iter 18800/18846/48828: loss 2.5009, time 103.79ms\n",
      "iter 18900/18954/48828: loss 2.5096, time 114.49ms\n",
      "step 19000: train loss 2.4966, val loss 2.4951\n",
      "iter 19000/19008/48828: loss 2.5035, time 3289.13ms\n",
      "iter 19100/19116/48828: loss 2.5007, time 108.76ms\n",
      "iter 19200/19224/48828: loss 2.5213, time 104.51ms\n",
      "iter 19300/19332/48828: loss 2.5048, time 103.13ms\n",
      "iter 19400/19440/48828: loss 2.5182, time 104.12ms\n",
      "iter 19500/19548/48828: loss 2.5081, time 105.89ms\n",
      "iter 19600/19602/48828: loss 2.5091, time 105.76ms\n",
      "iter 19700/19710/48828: loss 2.5096, time 104.52ms\n",
      "iter 19800/19818/48828: loss 2.5240, time 106.96ms\n",
      "iter 19900/19926/48828: loss 2.5296, time 105.62ms\n",
      "step 20000: train loss 2.5101, val loss 2.5102\n",
      "iter 20000/20034/48828: loss 2.5122, time 3069.21ms\n",
      "iter 20100/20142/48828: loss 2.5235, time 105.22ms\n",
      "iter 20200/20250/48828: loss 2.5086, time 108.93ms\n",
      "iter 20300/20304/48828: loss 2.4984, time 104.62ms\n",
      "iter 20400/20412/48828: loss 2.5303, time 104.14ms\n",
      "iter 20500/20520/48828: loss 2.5201, time 111.29ms\n",
      "iter 20600/20628/48828: loss 2.5180, time 109.86ms\n",
      "iter 20700/20736/48828: loss 2.5170, time 105.98ms\n",
      "iter 20800/20844/48828: loss 2.5132, time 104.15ms\n",
      "iter 20900/20952/48828: loss 2.5153, time 106.77ms\n",
      "step 21000: train loss 2.4960, val loss 2.4954\n",
      "iter 21000/21006/48828: loss 2.4943, time 3300.54ms\n",
      "iter 21100/21114/48828: loss 2.4929, time 98.55ms\n",
      "iter 21200/21222/48828: loss 2.5066, time 103.96ms\n",
      "iter 21300/21330/48828: loss 2.5061, time 105.99ms\n",
      "iter 21400/21438/48828: loss 2.4861, time 108.18ms\n",
      "iter 21500/21546/48828: loss 2.5034, time 105.03ms\n",
      "iter 21600/21654/48828: loss 2.4997, time 119.83ms\n",
      "iter 21700/21708/48828: loss 2.5109, time 106.89ms\n",
      "iter 21800/21816/48828: loss 2.5087, time 105.45ms\n",
      "iter 21900/21924/48828: loss 2.4913, time 104.89ms\n",
      "step 22000: train loss 2.5076, val loss 2.5079\n",
      "iter 22000/22032/48828: loss 2.5041, time 3049.47ms\n",
      "iter 22100/22140/48828: loss 2.5028, time 104.15ms\n",
      "iter 22200/22248/48828: loss 2.5044, time 123.21ms\n",
      "iter 22300/22302/48828: loss 2.4923, time 107.78ms\n",
      "iter 22400/22410/48828: loss 2.5065, time 105.30ms\n",
      "iter 22500/22518/48828: loss 2.5106, time 103.70ms\n",
      "iter 22600/22626/48828: loss 2.4728, time 104.64ms\n",
      "iter 22700/22734/48828: loss 2.4805, time 103.55ms\n",
      "iter 22800/22842/48828: loss 2.4793, time 103.13ms\n",
      "iter 22900/22950/48828: loss 2.5070, time 105.24ms\n",
      "step 23000: train loss 2.5247, val loss 2.5248\n",
      "iter 23000/23004/48828: loss 2.5276, time 3359.70ms\n",
      "iter 23100/23112/48828: loss 2.5031, time 105.52ms\n",
      "iter 23200/23220/48828: loss 2.5084, time 115.28ms\n",
      "iter 23300/23328/48828: loss 2.5146, time 103.17ms\n",
      "iter 23400/23436/48828: loss 2.5080, time 108.85ms\n",
      "iter 23500/23544/48828: loss 2.4926, time 103.92ms\n",
      "iter 23600/23652/48828: loss 2.4999, time 107.11ms\n",
      "iter 23700/23706/48828: loss 2.4840, time 105.48ms\n",
      "iter 23800/23814/48828: loss 2.4978, time 104.33ms\n",
      "iter 23900/23922/48828: loss 2.4942, time 105.67ms\n",
      "step 24000: train loss 2.5118, val loss 2.5117\n",
      "iter 24000/24030/48828: loss 2.5154, time 3118.88ms\n",
      "iter 24100/24138/48828: loss 2.4901, time 107.54ms\n",
      "iter 24200/24246/48828: loss 2.5119, time 104.55ms\n",
      "iter 24300/24354/48828: loss 2.4848, time 100.37ms\n",
      "iter 24400/24408/48828: loss 2.4765, time 104.93ms\n",
      "iter 24500/24516/48828: loss 2.4872, time 105.02ms\n",
      "iter 24600/24624/48828: loss 2.5114, time 103.79ms\n",
      "iter 24700/24732/48828: loss 2.4771, time 105.81ms\n",
      "iter 24800/24840/48828: loss 2.4688, time 104.62ms\n",
      "iter 24900/24948/48828: loss 2.4795, time 104.51ms\n",
      "step 25000: train loss 2.5291, val loss 2.5291\n",
      "iter 25000/25002/48828: loss 2.5323, time 3312.24ms\n",
      "iter 25100/25110/48828: loss 2.5009, time 104.12ms\n",
      "iter 25200/25218/48828: loss 2.5077, time 104.06ms\n",
      "iter 25300/25326/48828: loss 2.4712, time 103.66ms\n",
      "iter 25400/25434/48828: loss 2.4954, time 105.42ms\n",
      "iter 25500/25542/48828: loss 2.5053, time 108.55ms\n",
      "iter 25600/25650/48828: loss 2.4871, time 104.44ms\n",
      "iter 25700/25704/48828: loss 2.5091, time 104.88ms\n",
      "iter 25800/25812/48828: loss 2.5073, time 107.10ms\n",
      "iter 25900/25920/48828: loss 2.5020, time 107.95ms\n",
      "step 26000: train loss 2.4882, val loss 2.4895\n",
      "iter 26000/26028/48828: loss 2.4887, time 3058.86ms\n",
      "iter 26100/26136/48828: loss 2.4707, time 111.99ms\n",
      "iter 26200/26244/48828: loss 2.5162, time 104.80ms\n",
      "iter 26300/26352/48828: loss 2.4984, time 105.63ms\n",
      "iter 26400/26406/48828: loss 2.4887, time 105.02ms\n",
      "iter 26500/26514/48828: loss 2.5050, time 103.90ms\n",
      "iter 26600/26622/48828: loss 2.5209, time 104.34ms\n",
      "iter 26700/26730/48828: loss 2.5032, time 105.12ms\n",
      "iter 26800/26838/48828: loss 2.4962, time 105.89ms\n",
      "iter 26900/26946/48828: loss 2.4992, time 103.36ms\n",
      "step 27000: train loss 2.5161, val loss 2.5165\n",
      "iter 27000/27054/48828: loss 2.5164, time 3611.89ms\n",
      "iter 27100/27108/48828: loss 2.5070, time 105.75ms\n",
      "iter 27200/27216/48828: loss 2.4990, time 104.11ms\n",
      "iter 27300/27324/48828: loss 2.4984, time 106.49ms\n",
      "iter 27400/27432/48828: loss 2.5175, time 105.13ms\n",
      "iter 27500/27540/48828: loss 2.5000, time 105.32ms\n",
      "iter 27600/27648/48828: loss 2.5002, time 129.87ms\n",
      "iter 27700/27702/48828: loss 2.5047, time 107.43ms\n",
      "iter 27800/27810/48828: loss 2.5100, time 102.41ms\n",
      "iter 27900/27918/48828: loss 2.5013, time 106.87ms\n",
      "step 28000: train loss 2.4776, val loss 2.4792\n",
      "iter 28000/28026/48828: loss 2.4804, time 3041.17ms\n",
      "iter 28100/28134/48828: loss 2.5032, time 103.22ms\n",
      "iter 28200/28242/48828: loss 2.4820, time 104.81ms\n",
      "iter 28300/28350/48828: loss 2.4991, time 104.19ms\n",
      "iter 28400/28404/48828: loss 2.4940, time 104.82ms\n",
      "iter 28500/28512/48828: loss 2.4979, time 104.58ms\n",
      "iter 28600/28620/48828: loss 2.5088, time 105.36ms\n",
      "iter 28700/28728/48828: loss 2.5031, time 104.93ms\n",
      "iter 28800/28836/48828: loss 2.4799, time 105.85ms\n",
      "iter 28900/28944/48828: loss 2.4903, time 104.30ms\n",
      "step 29000: train loss 2.4683, val loss 2.4688\n",
      "iter 29000/29052/48828: loss 2.4660, time 3325.98ms\n",
      "iter 29100/29106/48828: loss 2.5017, time 104.54ms\n",
      "iter 29200/29214/48828: loss 2.4961, time 106.90ms\n",
      "iter 29300/29322/48828: loss 2.5017, time 105.18ms\n",
      "iter 29400/29430/48828: loss 2.4991, time 103.61ms\n",
      "iter 29500/29538/48828: loss 2.4811, time 105.26ms\n",
      "iter 29600/29646/48828: loss 2.4959, time 99.04ms\n",
      "iter 29700/29754/48828: loss 2.4949, time 100.65ms\n",
      "iter 29800/29808/48828: loss 2.4875, time 110.07ms\n",
      "iter 29900/29916/48828: loss 2.4802, time 103.85ms\n",
      "step 30000: train loss 2.4885, val loss 2.4892\n",
      "iter 30000/30024/48828: loss 2.4859, time 3026.82ms\n",
      "iter 30100/30132/48828: loss 2.4845, time 102.44ms\n",
      "iter 30200/30240/48828: loss 2.4922, time 110.98ms\n",
      "iter 30300/30348/48828: loss 2.4949, time 102.94ms\n",
      "iter 30400/30402/48828: loss 2.4803, time 104.75ms\n",
      "iter 30500/30510/48828: loss 2.4585, time 107.82ms\n",
      "iter 30600/30618/48828: loss 2.4939, time 104.52ms\n",
      "iter 30700/30726/48828: loss 2.5090, time 110.58ms\n",
      "iter 30800/30834/48828: loss 2.4558, time 107.95ms\n",
      "iter 30900/30942/48828: loss 2.4807, time 105.40ms\n",
      "step 31000: train loss 2.4857, val loss 2.4864\n",
      "iter 31000/31050/48828: loss 2.4814, time 3480.07ms\n",
      "iter 31100/31104/48828: loss 2.4931, time 104.49ms\n",
      "iter 31200/31212/48828: loss 2.4888, time 105.32ms\n",
      "iter 31300/31320/48828: loss 2.4722, time 105.41ms\n",
      "iter 31400/31428/48828: loss 2.4907, time 102.74ms\n",
      "iter 31500/31536/48828: loss 2.4848, time 104.89ms\n",
      "iter 31600/31644/48828: loss 2.4742, time 105.18ms\n",
      "iter 31700/31752/48828: loss 2.4590, time 104.52ms\n",
      "iter 31800/31806/48828: loss 2.4762, time 104.50ms\n",
      "iter 31900/31914/48828: loss 2.5076, time 107.36ms\n",
      "step 32000: train loss 2.4825, val loss 2.4836\n",
      "iter 32000/32022/48828: loss 2.4750, time 3064.67ms\n",
      "iter 32100/32130/48828: loss 2.5035, time 110.48ms\n",
      "iter 32200/32238/48828: loss 2.4704, time 105.92ms\n",
      "iter 32300/32346/48828: loss 2.4655, time 132.77ms\n",
      "iter 32400/32454/48828: loss 2.4873, time 100.83ms\n",
      "iter 32500/32508/48828: loss 2.4767, time 111.70ms\n",
      "iter 32600/32616/48828: loss 2.4634, time 102.87ms\n",
      "iter 32700/32724/48828: loss 2.4850, time 104.74ms\n",
      "iter 32800/32832/48828: loss 2.4881, time 104.15ms\n",
      "iter 32900/32940/48828: loss 2.4791, time 126.50ms\n",
      "step 33000: train loss 2.4835, val loss 2.4821\n",
      "iter 33000/33048/48828: loss 2.4719, time 3311.03ms\n",
      "iter 33100/33102/48828: loss 2.4879, time 106.05ms\n",
      "iter 33200/33210/48828: loss 2.4593, time 105.94ms\n",
      "iter 33300/33318/48828: loss 2.4749, time 105.58ms\n",
      "iter 33400/33426/48828: loss 2.4858, time 103.87ms\n",
      "iter 33500/33534/48828: loss 2.4655, time 102.91ms\n",
      "iter 33600/33642/48828: loss 2.5098, time 104.40ms\n",
      "iter 33700/33750/48828: loss 2.4910, time 105.55ms\n",
      "iter 33800/33804/48828: loss 2.4898, time 104.76ms\n",
      "iter 33900/33912/48828: loss 2.5127, time 105.88ms\n",
      "step 34000: train loss 2.4689, val loss 2.4705\n",
      "iter 34000/34020/48828: loss 2.4706, time 3033.72ms\n",
      "iter 34100/34128/48828: loss 2.4812, time 106.06ms\n",
      "iter 34200/34236/48828: loss 2.4811, time 103.94ms\n",
      "iter 34300/34344/48828: loss 2.4590, time 103.98ms\n",
      "iter 34400/34452/48828: loss 2.4841, time 105.49ms\n",
      "iter 34500/34506/48828: loss 2.4829, time 104.53ms\n",
      "iter 34600/34614/48828: loss 2.4618, time 104.69ms\n",
      "iter 34700/34722/48828: loss 2.4651, time 105.66ms\n",
      "iter 34800/34830/48828: loss 2.4615, time 103.87ms\n",
      "iter 34900/34938/48828: loss 2.4460, time 105.02ms\n",
      "step 35000: train loss 2.4537, val loss 2.4562\n",
      "iter 35000/35046/48828: loss 2.4486, time 3286.52ms\n",
      "iter 35100/35154/48828: loss 2.4972, time 99.77ms\n",
      "iter 35200/35208/48828: loss 2.4885, time 104.19ms\n",
      "iter 35300/35316/48828: loss 2.4491, time 108.44ms\n",
      "iter 35400/35424/48828: loss 2.4874, time 110.84ms\n",
      "iter 35500/35532/48828: loss 2.4732, time 106.72ms\n",
      "iter 35600/35640/48828: loss 2.4635, time 106.19ms\n",
      "iter 35700/35748/48828: loss 2.4477, time 160.42ms\n",
      "iter 35800/35802/48828: loss 2.4941, time 1314.50ms\n",
      "iter 35900/35910/48828: loss 2.4465, time 104.59ms\n",
      "step 36000: train loss 2.4548, val loss 2.4525\n",
      "iter 36000/36018/48828: loss 2.4573, time 3258.43ms\n",
      "iter 36100/36126/48828: loss 2.4627, time 104.15ms\n",
      "iter 36200/36234/48828: loss 2.4696, time 110.53ms\n",
      "iter 36300/36342/48828: loss 2.4928, time 106.89ms\n",
      "iter 36400/36450/48828: loss 2.4764, time 103.53ms\n",
      "iter 36500/36504/48828: loss 2.4584, time 106.83ms\n",
      "iter 36600/36612/48828: loss 2.4529, time 116.43ms\n",
      "iter 36700/36720/48828: loss 2.4643, time 104.63ms\n",
      "iter 36800/36828/48828: loss 2.4886, time 105.80ms\n",
      "iter 36900/36936/48828: loss 2.4855, time 104.65ms\n",
      "step 37000: train loss 2.4593, val loss 2.4566\n",
      "iter 37000/37044/48828: loss 2.4500, time 3057.67ms\n",
      "iter 37100/37152/48828: loss 2.4512, time 103.71ms\n",
      "iter 37200/37206/48828: loss 2.4435, time 109.78ms\n",
      "iter 37300/37314/48828: loss 2.4790, time 105.63ms\n",
      "iter 37400/37422/48828: loss 2.4927, time 106.00ms\n",
      "iter 37500/37530/48828: loss 2.4658, time 105.52ms\n",
      "iter 37600/37638/48828: loss 2.4722, time 111.68ms\n",
      "iter 37700/37746/48828: loss 2.4748, time 107.97ms\n",
      "iter 37800/37854/48828: loss 2.4839, time 99.89ms\n",
      "iter 37900/37908/48828: loss 2.4705, time 104.96ms\n",
      "step 38000: train loss 2.4593, val loss 2.4551\n",
      "iter 38000/38016/48828: loss 2.4558, time 3314.08ms\n",
      "iter 38100/38124/48828: loss 2.4489, time 109.87ms\n",
      "iter 38200/38232/48828: loss 2.4269, time 98.81ms\n",
      "iter 38300/38340/48828: loss 2.4461, time 106.00ms\n",
      "iter 38400/38448/48828: loss 2.4161, time 109.78ms\n",
      "iter 38500/38502/48828: loss 2.4111, time 110.36ms\n",
      "iter 38600/38610/48828: loss 2.4144, time 105.27ms\n",
      "iter 38700/38718/48828: loss 2.4437, time 105.50ms\n",
      "iter 38800/38826/48828: loss 2.4523, time 104.06ms\n",
      "iter 38900/38934/48828: loss 2.4428, time 105.41ms\n",
      "step 39000: train loss 2.4192, val loss 2.4188\n",
      "iter 39000/39042/48828: loss 2.4210, time 3111.18ms\n",
      "iter 39100/39150/48828: loss 2.4268, time 105.56ms\n",
      "iter 39200/39204/48828: loss 2.4207, time 105.35ms\n",
      "iter 39300/39312/48828: loss 2.4235, time 108.42ms\n",
      "iter 39400/39420/48828: loss 2.3972, time 106.01ms\n",
      "iter 39500/39528/48828: loss 2.4388, time 105.32ms\n",
      "iter 39600/39636/48828: loss 2.4313, time 104.82ms\n",
      "iter 39700/39744/48828: loss 2.4233, time 107.01ms\n",
      "iter 39800/39852/48828: loss 2.4394, time 121.06ms\n",
      "iter 39900/39906/48828: loss 2.4298, time 105.65ms\n",
      "step 40000: train loss 2.4545, val loss 2.4566\n",
      "iter 40000/40014/48828: loss 2.4621, time 3337.84ms\n",
      "iter 40100/40122/48828: loss 2.4473, time 105.95ms\n",
      "iter 40200/40230/48828: loss 2.4291, time 102.40ms\n",
      "iter 40300/40338/48828: loss 2.4262, time 105.37ms\n",
      "iter 40400/40446/48828: loss 2.4670, time 105.15ms\n",
      "iter 40500/40554/48828: loss 2.4452, time 134.06ms\n",
      "iter 40600/40608/48828: loss 2.4443, time 105.76ms\n",
      "iter 40700/40716/48828: loss 2.4414, time 126.54ms\n",
      "iter 40800/40824/48828: loss 2.4291, time 111.65ms\n",
      "iter 40900/40932/48828: loss 2.4311, time 106.62ms\n",
      "step 41000: train loss 2.4204, val loss 2.4216\n",
      "iter 41000/41040/48828: loss 2.4288, time 3028.52ms\n",
      "iter 41100/41148/48828: loss 2.3936, time 104.51ms\n",
      "iter 41200/41202/48828: loss 2.4323, time 104.72ms\n",
      "iter 41300/41310/48828: loss 2.4471, time 110.95ms\n",
      "iter 41400/41418/48828: loss 2.4063, time 103.64ms\n",
      "iter 41500/41526/48828: loss 2.4253, time 102.91ms\n",
      "iter 41600/41634/48828: loss 2.4104, time 104.57ms\n",
      "iter 41700/41742/48828: loss 2.4207, time 105.49ms\n",
      "iter 41800/41850/48828: loss 2.4252, time 104.10ms\n",
      "iter 41900/41904/48828: loss 2.4153, time 105.73ms\n",
      "step 42000: train loss 2.4196, val loss 2.4192\n",
      "iter 42000/42012/48828: loss 2.4304, time 3463.41ms\n",
      "iter 42100/42120/48828: loss 2.4084, time 111.76ms\n",
      "iter 42200/42228/48828: loss 2.3914, time 103.83ms\n",
      "iter 42300/42336/48828: loss 2.4114, time 102.41ms\n",
      "iter 42400/42444/48828: loss 2.4121, time 104.47ms\n",
      "iter 42500/42552/48828: loss 2.3762, time 104.52ms\n",
      "iter 42600/42606/48828: loss 2.3776, time 105.49ms\n",
      "iter 42700/42714/48828: loss 2.4013, time 111.36ms\n",
      "iter 42800/42822/48828: loss 2.3808, time 104.88ms\n",
      "iter 42900/42930/48828: loss 2.3736, time 110.64ms\n",
      "step 43000: train loss 2.3704, val loss 2.3722\n",
      "iter 43000/43038/48828: loss 2.3771, time 3278.22ms\n",
      "iter 43100/43146/48828: loss 2.3718, time 103.84ms\n",
      "iter 43200/43254/48828: loss 2.3769, time 103.34ms\n",
      "iter 43300/43308/48828: loss 2.3741, time 105.43ms\n",
      "iter 43400/43416/48828: loss 2.3708, time 105.46ms\n",
      "iter 43500/43524/48828: loss 2.3860, time 105.23ms\n",
      "iter 43600/43632/48828: loss 2.3559, time 106.61ms\n",
      "iter 43700/43740/48828: loss 2.3646, time 107.33ms\n",
      "iter 43800/43848/48828: loss 2.3722, time 104.32ms\n",
      "iter 43900/43902/48828: loss 2.3756, time 350.10ms\n",
      "step 44000: train loss 2.3674, val loss 2.3626\n",
      "iter 44000/44010/48828: loss 2.3523, time 3091.10ms\n",
      "iter 44100/44118/48828: loss 2.3661, time 104.03ms\n",
      "iter 44200/44226/48828: loss 2.3809, time 110.78ms\n",
      "iter 44300/44334/48828: loss 2.3973, time 105.01ms\n",
      "iter 44400/44442/48828: loss 2.4877, time 107.17ms\n",
      "iter 44500/44550/48828: loss 2.4147, time 103.83ms\n",
      "iter 44600/44604/48828: loss 2.4249, time 104.80ms\n",
      "iter 44700/44712/48828: loss 2.3920, time 106.24ms\n",
      "iter 44800/44820/48828: loss 2.3685, time 105.37ms\n",
      "iter 44900/44928/48828: loss 2.3728, time 105.28ms\n",
      "step 45000: train loss 2.3802, val loss 2.3778\n",
      "iter 45000/45036/48828: loss 2.3808, time 3340.62ms\n",
      "iter 45100/45144/48828: loss 2.3801, time 105.60ms\n",
      "iter 45200/45252/48828: loss 2.3722, time 102.65ms\n",
      "iter 45300/45306/48828: loss 2.3487, time 103.68ms\n",
      "iter 45400/45414/48828: loss 2.3907, time 103.94ms\n",
      "iter 45500/45522/48828: loss 2.3611, time 115.31ms\n",
      "iter 45600/45630/48828: loss 2.3773, time 109.85ms\n",
      "iter 45700/45738/48828: loss 2.3541, time 104.99ms\n",
      "iter 45800/45846/48828: loss 2.3581, time 105.90ms\n",
      "iter 45900/45954/48828: loss 2.3597, time 101.49ms\n",
      "step 46000: train loss 2.3408, val loss 2.3380\n",
      "iter 46000/46008/48828: loss 2.3343, time 3042.06ms\n",
      "iter 46100/46116/48828: loss 2.3607, time 105.21ms\n",
      "iter 46200/46224/48828: loss 2.3801, time 105.05ms\n",
      "iter 46300/46332/48828: loss 2.3653, time 105.50ms\n",
      "iter 46400/46440/48828: loss 2.3717, time 104.42ms\n",
      "iter 46500/46548/48828: loss 2.3536, time 106.08ms\n",
      "iter 46600/46602/48828: loss 2.3714, time 115.32ms\n",
      "iter 46700/46710/48828: loss 2.3622, time 109.86ms\n",
      "iter 46800/46818/48828: loss 2.3529, time 104.90ms\n",
      "iter 46900/46926/48828: loss 2.3825, time 104.12ms\n",
      "step 47000: train loss 2.3582, val loss 2.3568\n",
      "iter 47000/47034/48828: loss 2.3597, time 3323.53ms\n",
      "iter 47100/47142/48828: loss 2.3769, time 104.18ms\n",
      "iter 47200/47250/48828: loss 2.3776, time 104.09ms\n",
      "iter 47300/47304/48828: loss 2.3842, time 117.21ms\n",
      "iter 47400/47412/48828: loss 2.3582, time 105.39ms\n",
      "iter 47500/47520/48828: loss 2.3546, time 105.55ms\n",
      "iter 47600/47628/48828: loss 2.3408, time 105.19ms\n",
      "iter 47700/47736/48828: loss 2.3549, time 105.29ms\n",
      "iter 47800/47844/48828: loss 2.3569, time 105.36ms\n",
      "iter 47900/47952/48828: loss 2.3393, time 104.84ms\n",
      "step 48000: train loss 2.3521, val loss 2.3547\n",
      "iter 48000/48006/48828: loss 2.3722, time 3056.83ms\n",
      "iter 48100/48114/48828: loss 2.3365, time 105.19ms\n",
      "iter 48200/48222/48828: loss 2.3402, time 105.89ms\n",
      "iter 48300/48330/48828: loss 2.3683, time 105.90ms\n",
      "iter 48400/48438/48828: loss 2.3263, time 106.19ms\n",
      "iter 48500/48546/48828: loss 2.3486, time 103.57ms\n",
      "iter 48600/48654/48828: loss 2.3548, time 119.62ms\n",
      "iter 48700/48708/48828: loss 2.3673, time 103.67ms\n",
      "iter 48800/48816/48828: loss 2.3672, time 104.57ms\n",
      "\n",
      "\n",
      "## Running generation 12 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [16:39<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=80.75% win[2]=19.25%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 40, mean: 11.91\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=36.17% counts=Counter({2: 60, 1: 34}), win[2]=63.83% draw=0.00%\n",
      "  a=2: n=212 win[1]=58.96% counts=Counter({1: 125, 2: 87}), win[2]=41.04% draw=0.00%\n",
      "  a=3: n=1213 win[1]=72.38% counts=Counter({1: 878, 2: 335}), win[2]=27.62% draw=0.00%\n",
      "  a=4: n=3363 win[1]=86.41% counts=Counter({1: 2906, 2: 457}), win[2]=13.59% draw=0.00%\n",
      "  a=5: n=2999 win[1]=81.99% counts=Counter({1: 2459, 2: 540}), win[2]=18.01% draw=0.00%\n",
      "  a=6: n=2030 win[1]=80.20% counts=Counter({1: 1628, 2: 402}), win[2]=19.80% draw=0.00%\n",
      "  a=7: n= 89 win[1]=50.56% counts=Counter({1: 45, 2: 44}), win[2]=49.44% draw=0.00%\n",
      "Training model on gen-12\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.3742, val loss 2.3762\n",
      "iter 0/59/48828: loss 2.3778, time 3863.87ms\n",
      "iter 100/118/48828: loss 2.5021, time 103.88ms\n",
      "iter 200/236/48828: loss 2.5046, time 105.17ms\n",
      "iter 300/354/48828: loss 2.4945, time 105.36ms\n",
      "iter 400/413/48828: loss 2.5021, time 106.15ms\n",
      "iter 500/531/48828: loss 2.5132, time 108.70ms\n",
      "iter 600/649/48828: loss 2.4986, time 389.23ms\n",
      "iter 700/708/48828: loss 2.5168, time 121.06ms\n",
      "iter 800/826/48828: loss 2.5201, time 105.90ms\n",
      "iter 900/944/48828: loss 2.5244, time 105.56ms\n",
      "step 1000: train loss 2.5181, val loss 2.5185\n",
      "iter 1000/1003/48828: loss 2.5260, time 3089.58ms\n",
      "iter 1100/1121/48828: loss 2.5133, time 105.42ms\n",
      "iter 1200/1239/48828: loss 2.5143, time 105.30ms\n",
      "iter 1300/1357/48828: loss 2.4925, time 110.74ms\n",
      "iter 1400/1416/48828: loss 2.5178, time 106.11ms\n",
      "iter 1500/1534/48828: loss 2.5232, time 106.56ms\n",
      "iter 1600/1652/48828: loss 2.5068, time 105.49ms\n",
      "iter 1700/1711/48828: loss 2.5112, time 104.75ms\n",
      "iter 1800/1829/48828: loss 2.5191, time 106.57ms\n",
      "iter 1900/1947/48828: loss 2.5079, time 108.96ms\n",
      "step 2000: train loss 2.5064, val loss 2.5051\n",
      "iter 2000/2006/48828: loss 2.5012, time 3328.47ms\n",
      "iter 2100/2124/48828: loss 2.5144, time 104.70ms\n",
      "iter 2200/2242/48828: loss 2.5195, time 109.56ms\n",
      "iter 2300/2301/48828: loss 2.5183, time 1508.68ms\n",
      "iter 2400/2419/48828: loss 2.5103, time 106.92ms\n",
      "iter 2500/2537/48828: loss 2.5310, time 108.73ms\n",
      "iter 2600/2655/48828: loss 2.5174, time 104.86ms\n",
      "iter 2700/2714/48828: loss 2.5248, time 382.36ms\n",
      "iter 2800/2832/48828: loss 2.5399, time 105.71ms\n",
      "iter 2900/2950/48828: loss 2.5126, time 105.10ms\n",
      "step 3000: train loss 2.5038, val loss 2.5024\n",
      "iter 3000/3009/48828: loss 2.4933, time 3505.23ms\n",
      "iter 3100/3127/48828: loss 2.5141, time 103.34ms\n",
      "iter 3200/3245/48828: loss 2.5106, time 106.33ms\n",
      "iter 3300/3304/48828: loss 2.5028, time 141.66ms\n",
      "iter 3400/3422/48828: loss 2.5347, time 104.81ms\n",
      "iter 3500/3540/48828: loss 2.5188, time 105.33ms\n",
      "iter 3600/3658/48828: loss 2.5392, time 109.33ms\n",
      "iter 3700/3717/48828: loss 2.5182, time 104.31ms\n",
      "iter 3800/3835/48828: loss 2.5063, time 104.52ms\n",
      "iter 3900/3953/48828: loss 2.5037, time 418.42ms\n",
      "step 4000: train loss 2.5338, val loss 2.5346\n",
      "iter 4000/4012/48828: loss 2.5355, time 4633.95ms\n",
      "iter 4100/4130/48828: loss 2.5285, time 105.32ms\n",
      "iter 4200/4248/48828: loss 2.5156, time 106.86ms\n",
      "iter 4300/4307/48828: loss 2.4997, time 105.34ms\n",
      "iter 4400/4425/48828: loss 2.4992, time 462.30ms\n",
      "iter 4500/4543/48828: loss 2.4987, time 104.82ms\n",
      "iter 4600/4602/48828: loss 2.4899, time 99.73ms\n",
      "iter 4700/4720/48828: loss 2.5127, time 108.58ms\n",
      "iter 4800/4838/48828: loss 2.4946, time 104.45ms\n",
      "iter 4900/4956/48828: loss 2.5244, time 167.56ms\n",
      "step 5000: train loss 2.5144, val loss 2.5139\n",
      "iter 5000/5015/48828: loss 2.5214, time 3390.53ms\n",
      "iter 5100/5133/48828: loss 2.5115, time 104.97ms\n",
      "iter 5200/5251/48828: loss 2.5185, time 110.78ms\n",
      "iter 5300/5310/48828: loss 2.5359, time 103.34ms\n",
      "iter 5400/5428/48828: loss 2.5446, time 104.66ms\n",
      "iter 5500/5546/48828: loss 2.5144, time 104.84ms\n",
      "iter 5600/5605/48828: loss 2.5312, time 104.88ms\n",
      "iter 5700/5723/48828: loss 2.5152, time 106.00ms\n",
      "iter 5800/5841/48828: loss 2.5029, time 103.91ms\n",
      "iter 5900/5959/48828: loss 2.5180, time 121.78ms\n",
      "step 6000: train loss 2.5212, val loss 2.5228\n",
      "iter 6000/6018/48828: loss 2.5232, time 3318.98ms\n",
      "iter 6100/6136/48828: loss 2.5028, time 109.24ms\n",
      "iter 6200/6254/48828: loss 2.5128, time 105.77ms\n",
      "iter 6300/6313/48828: loss 2.4946, time 104.05ms\n",
      "iter 6400/6431/48828: loss 2.5181, time 104.47ms\n",
      "iter 6500/6549/48828: loss 2.5075, time 110.68ms\n",
      "iter 6600/6608/48828: loss 2.5148, time 103.26ms\n",
      "iter 6700/6726/48828: loss 2.5236, time 105.72ms\n",
      "iter 6800/6844/48828: loss 2.5142, time 105.95ms\n",
      "iter 6900/6903/48828: loss 2.5117, time 105.49ms\n",
      "step 7000: train loss 2.5115, val loss 2.5106\n",
      "iter 7000/7021/48828: loss 2.5137, time 3094.09ms\n",
      "iter 7100/7139/48828: loss 2.5119, time 106.53ms\n",
      "iter 7200/7257/48828: loss 2.5198, time 104.85ms\n",
      "iter 7300/7316/48828: loss 2.5161, time 121.01ms\n",
      "iter 7400/7434/48828: loss 2.5206, time 104.62ms\n",
      "iter 7500/7552/48828: loss 2.5106, time 115.86ms\n",
      "iter 7600/7611/48828: loss 2.5056, time 116.31ms\n",
      "iter 7700/7729/48828: loss 2.5111, time 116.83ms\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, NUM_GENERATIONS+1):\n",
    "        current_model = model_dict[generation_id-1]\n",
    "        results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "        results_dict[generation_id] = results_i\n",
    "        trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "        model_dict[generation_id] = model_i\n",
    "\n",
    "## refactor, learning_rate = 0.05, warmup_iters=0\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/488: loss 2.7801, time 611.02ms\n",
    "# iter 100/105/488: loss 2.5840, time 63.73ms\n",
    "# iter 200/205/488: loss 2.5958, time 62.98ms\n",
    "# iter 300/305/488: loss 2.5835, time 60.15ms\n",
    "# iter 400/405/488: loss 2.5793, time 63.62ms\n",
    "\n",
    "# ## model = small\n",
    "# step 0: train loss 2.7741, val loss 2.7741\n",
    "# iter 0/5/488: loss 2.7743, time 1624.89ms\n",
    "# iter 100/105/488: loss 2.6157, time 141.39ms\n",
    "# iter 200/205/488: loss 2.6120, time 161.22ms\n",
    "# iter 300/305/488: loss 2.5983, time 203.82ms\n",
    "\n",
    "\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/4882: loss 2.7801, time 1422.53ms\n",
    "# iter 100/105/4882: loss 2.5970, time 110.71ms\n",
    "# iter 200/205/4882: loss 2.5962, time 116.96ms\n",
    "# iter 300/305/4882: loss 2.5917, time 160.95ms\n",
    "# iter 400/405/4882: loss 2.5885, time 63.37ms\n",
    "# iter 500/505/4882: loss 2.5912, time 65.25ms\n",
    "# iter 600/605/4882: loss 2.6000, time 67.49ms\n",
    "# iter 700/705/4882: loss 2.5780, time 61.43ms\n",
    "# iter 800/805/4882: loss 2.5864, time 265.56ms\n",
    "# iter 900/905/4882: loss 2.5857, time 263.09ms\n",
    "# step 1000: train loss 2.5849, val loss 2.5847\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 1000/1005/4882: loss 2.5844, time 1812.58ms\n",
    "# iter 1100/1105/4882: loss 2.5832, time 62.89ms\n",
    "# iter 1200/1205/4882: loss 2.5743, time 95.25ms\n",
    "# iter 1300/1305/4882: loss 2.5720, time 324.18ms\n",
    "# iter 1400/1405/4882: loss 2.5880, time 73.66ms\n",
    "# iter 1500/1505/4882: loss 2.5745, time 295.39ms\n",
    "# iter 1600/1605/4882: loss 2.5726, time 76.05ms\n",
    "# iter 1700/1705/4882: loss 2.5670, time 63.20ms\n",
    "# iter 1800/1805/4882: loss 2.5720, time 62.66ms\n",
    "# iter 1900/1905/4882: loss 2.5694, time 449.06ms\n",
    "# step 2000: train loss 2.5806, val loss 2.5806\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 2000/2005/4882: loss 2.5893, time 920.12ms\n",
    "# iter 2100/2105/4882: loss 2.5686, time 430.53ms\n",
    "# iter 2200/2205/4882: loss 2.5741, time 63.05ms\n",
    "# iter 2300/2305/4882: loss 2.5679, time 60.90ms\n",
    "# iter 2400/2405/4882: loss 2.5754, time 69.07ms\n",
    "# iter 2500/2505/4882: loss 2.5673, time 68.33ms\n",
    "# iter 2600/2605/4882: loss 2.5648, time 66.26ms\n",
    "# iter 2700/2705/4882: loss 2.5622, time 69.76ms\n",
    "# iter 2800/2805/4882: loss 2.5541, time 143.65ms\n",
    "# iter 2900/2905/4882: loss 2.5634, time 66.40ms\n",
    "# step 3000: train loss 2.5550, val loss 2.5547\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 3000/3005/4882: loss 2.5545, time 975.15ms\n",
    "# iter 3100/3105/4882: loss 2.5594, time 63.55ms\n",
    "# iter 3200/3205/4882: loss 2.5499, time 64.17ms\n",
    "# iter 3300/3305/4882: loss 2.5481, time 70.28ms\n",
    "# iter 3400/3405/4882: loss 2.5565, time 73.58ms\n",
    "# iter 3500/3505/4882: loss 2.5602, time 72.22ms\n",
    "# iter 3600/3605/4882: loss 2.5429, time 88.68ms\n",
    "# iter 3700/3705/4882: loss 2.5259, time 63.15ms\n",
    "# iter 3800/3805/4882: loss 2.5346, time 66.07ms\n",
    "# iter 3900/3905/4882: loss 2.5386, time 73.50ms\n",
    "# step 4000: train loss 2.5350, val loss 2.5345\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 4000/4005/4882: loss 2.5424, time 1217.41ms\n",
    "# iter 4100/4105/4882: loss 2.5290, time 101.01ms\n",
    "# iter 4200/4205/4882: loss 2.5323, time 61.94ms\n",
    "# iter 4300/4305/4882: loss 2.5250, time 72.57ms\n",
    "# iter 4400/4405/4882: loss 2.5243, time 68.38ms\n",
    "# iter 4500/4505/4882: loss 2.5331, time 73.33ms\n",
    "# iter 4600/4605/4882: loss 2.5246, time 101.00ms\n",
    "# iter 4700/4705/4882: loss 2.5336, time 67.27ms\n",
    "# iter 4800/4805/4882: loss 2.5170, time 79.40ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        # dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:5].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:6].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:7].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:8].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:9].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:10].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):\n",
    "    # prefix_list = [\n",
    "    #     (0,), \n",
    "    #     (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "    #     (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "    #     (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7)]\n",
    "    # prefix_list = [k for k in dd.items()]\n",
    "    \n",
    "    list(dd.items())[10][1]['*'].sum() > 100\n",
    "    top_k = sorted(dd.items(), key=lambda kv: kv[1]['*'].sum(), reverse=True)[:50]\n",
    "    top_k_keys = sorted(k for k, v in top_k)\n",
    "    \n",
    "    prefix_list = top_k_keys\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tournament to calcualte ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from contextlib import asynccontextmanager\n",
    "from rgi.rgizero.tournament import Tournament\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_player_factory(model, simulations, game, device, block_size, action_vocab, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a shared evaluator and returns a factory function that produces \n",
    "    new AlphazeroPlayer instances using that shared evaluator.\n",
    "    \"\"\"\n",
    "    # 1. Setup the shared evaluator\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(\n",
    "        model, \n",
    "        device=device, \n",
    "        block_size=block_size, \n",
    "        vocab=action_vocab\n",
    "    )\n",
    "    async_evaluator = AsyncNetworkEvaluator(\n",
    "        base_evaluator=serial_evaluator, \n",
    "        max_batch_size=max_batch_size, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 2. Start the evaluator background task\n",
    "    await async_evaluator.start()\n",
    "    \n",
    "    try:\n",
    "        # 3. Define the factory. This is called by Tournament for every game.\n",
    "        # It creates a NEW player instance but uses the SHARED async_evaluator.\n",
    "        def player_factory():\n",
    "            # Create a fresh RNG for each game/player instance\n",
    "            rng = np.random.default_rng(np.random.randint(0, 2**31))\n",
    "            return AlphazeroPlayer(\n",
    "                game, \n",
    "                async_evaluator, \n",
    "                rng=rng, \n",
    "                add_noise=True, \n",
    "                simulations=simulations\n",
    "            )\n",
    "            \n",
    "        yield player_factory\n",
    "        \n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        await async_evaluator.stop()\n",
    "\n",
    "async def main():\n",
    "    # ... setup game, models, etc ...\n",
    "    \n",
    "    # Use async with to manage the lifecycle of the evaluators\n",
    "    async with (\n",
    "        create_player_factory(model_dict[0], 100, game, device, block_size, action_vocab, 10) as factory_gen0_100,\n",
    "        create_player_factory(model_dict[1], 100, game, device, block_size, action_vocab, 10) as factory_gen1_100,\n",
    "        create_player_factory(model_dict[2], 100, game, device, block_size, action_vocab, 10) as factory_gen2_100,\n",
    "        create_player_factory(model_dict[3], 100, game, device, block_size, action_vocab, 10) as factory_gen3_100,\n",
    "        create_player_factory(model_dict[4], 100, game, device, block_size, action_vocab, 10) as factory_gen4_100,\n",
    "        create_player_factory(model_dict[5], 100, game, device, block_size, action_vocab, 10) as factory_gen5_100,\n",
    "        create_player_factory(model_dict[6], 100, game, device, block_size, action_vocab, 10) as factory_gen6_100,\n",
    "        create_player_factory(model_dict[7], 100, game, device, block_size, action_vocab, 10) as factory_gen7_100,\n",
    "\n",
    "        create_player_factory(model_dict[0], 200, game, device, block_size, action_vocab, 10) as factory_gen0_200,\n",
    "        create_player_factory(model_dict[1], 200, game, device, block_size, action_vocab, 10) as factory_gen1_200,\n",
    "        create_player_factory(model_dict[2], 200, game, device, block_size, action_vocab, 10) as factory_gen2_200,\n",
    "        create_player_factory(model_dict[3], 200, game, device, block_size, action_vocab, 10) as factory_gen3_200,\n",
    "        create_player_factory(model_dict[4], 200, game, device, block_size, action_vocab, 10) as factory_gen4_200,\n",
    "        create_player_factory(model_dict[5], 200, game, device, block_size, action_vocab, 10) as factory_gen5_200,\n",
    "        create_player_factory(model_dict[6], 200, game, device, block_size, action_vocab, 10) as factory_gen6_200,\n",
    "        create_player_factory(model_dict[7], 200, game, device, block_size, action_vocab, 10) as factory_gen7_200,\n",
    "        ):\n",
    "        \n",
    "        # The dictionary now maps names to FACTORIES (Callables), not Player instances\n",
    "        player_factories = {\n",
    "            \"factory_gen0_100\": factory_gen0_100,\n",
    "            \"factory_gen1_100\": factory_gen1_100,\n",
    "            \"factory_gen2_100\": factory_gen2_100,\n",
    "            \"factory_gen3_100\": factory_gen3_100,\n",
    "            \"factory_gen4_100\": factory_gen4_100,\n",
    "            \"factory_gen5_100\": factory_gen5_100,\n",
    "            \"factory_gen6_100\": factory_gen6_100,\n",
    "            \"factory_gen7_100\": factory_gen7_100,\n",
    "\n",
    "            \"factory_gen0_200\": factory_gen0_200,\n",
    "            \"factory_gen1_200\": factory_gen1_200,\n",
    "            \"factory_gen2_200\": factory_gen2_200,\n",
    "            \"factory_gen3_200\": factory_gen3_200,\n",
    "            \"factory_gen4_200\": factory_gen4_200,\n",
    "            \"factory_gen5_200\": factory_gen5_200,\n",
    "            \"factory_gen6_200\": factory_gen6_200,\n",
    "            \"factory_gen7_200\": factory_gen7_200,\n",
    "        }\n",
    "        \n",
    "        tournament = Tournament(game, player_factories, initial_elo=1000)\n",
    "        \n",
    "        print(\"Running tournament...\")\n",
    "        await tournament.run(num_games=10_000, concurrent_games=500)\n",
    "        tournament.print_standings()\n",
    "\n",
    "await main()\n",
    "\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 10000/10000 [1:25:59<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen6_200     1140.5     1247     827-419-1      \n",
    "# 2     factory_gen2_200     1100.1     1251     693-554-4      \n",
    "# 3     factory_gen5_100     1074.4     1251     598-652-1      \n",
    "# 4     factory_gen3_200     1029.1     1252     674-573-5      \n",
    "# 5     factory_gen4_200     1027.0     1248     711-536-1      \n",
    "# 6     factory_gen0_200     1020.0     1254     444-810-0      \n",
    "# 7     factory_gen5_200     990.2      1248     742-502-4      \n",
    "# 8     factory_gen7_100     987.5      1250     650-597-3      \n",
    "# 9     factory_gen7_200     979.2      1248     768-476-4      \n",
    "# 10    factory_gen2_100     974.0      1249     522-723-4      \n",
    "# 11    factory_gen6_100     966.6      1248     684-564-0      \n",
    "# 12    factory_gen4_100     964.2      1251     557-693-1      \n",
    "# 13    factory_gen1_100     962.5      1252     547-705-0      \n",
    "# 14    factory_gen3_100     947.0      1251     528-723-0      \n",
    "# 15    factory_gen1_200     941.1      1252     630-620-2      \n",
    "# 16    factory_gen0_100     896.5      1248     410-838-0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx STOP HERE xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
