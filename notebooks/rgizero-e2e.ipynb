{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players import alphazero\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"small\"  # \"tiny\" or \"small\" or\"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {base_game.__class__.__name__}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Confirm we can self-play a game with a Random Evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 1,\n",
       " 'rewards': array([1., 0.], dtype=float32),\n",
       " 'action_history': [4, 2, 6, 2, 5, 6, 6, 3, 7],\n",
       " 'legal_policies': [array([0.18874997, 0.14375001, 0.16500002, 0.12625   , 0.0125    , 0.19625002, 0.16750005], dtype=float32),\n",
       "  array([0.20250002, 0.19125   , 0.14125001, 0.22874996, 0.08125002, 0.005     , 0.15      ], dtype=float32),\n",
       "  array([0.115     , 0.02625   , 0.20749995, 0.11250001, 0.13000003, 0.23      , 0.17875004], dtype=float32),\n",
       "  array([0.07874999, 0.22874996, 0.19500004, 0.13000003, 0.03      , 0.15749998, 0.18      ], dtype=float32),\n",
       "  array([0.10749998, 0.11375   , 0.10499998, 0.0325    , 0.57500005, 0.03      , 0.03625   ], dtype=float32),\n",
       "  array([0.14750001, 0.17750002, 0.13000003, 0.02375   , 0.04      , 0.18250003, 0.29874992], dtype=float32),\n",
       "  array([0.015     , 0.005     , 0.5675    , 0.04624999, 0.0125    , 0.07124999, 0.2825    ], dtype=float32),\n",
       "  array([0.2125    , 0.06250001, 0.22125003, 0.14499998, 0.01875   , 0.16500002, 0.17499998], dtype=float32),\n",
       "  array([0.03624999, 0.05874998, 0.03375   , 0.06124999, 0.015     , 0.03375   , 0.7612501 ], dtype=float32)],\n",
       " 'final_state': HistoryTrackingGameState(base_state=Connect4State(board=array([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 2, 0, 0, 0, 2, 0],\n",
       "        [0, 2, 2, 1, 1, 1, 1]], dtype=int8), current_player=2, is_terminal=True, winner=1), action_history=('START_OF_GAME', 4, 2, 6, 2, 5, 6, 6, 3, 7)),\n",
       " 'legal_action_idx': [array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6])]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer, play_game, NetworkEvaluatorResult, NetworkEvaluator\n",
    "from typing import override, Any\n",
    "\n",
    "class RandomEvaluator(NetworkEvaluator):\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    @override\n",
    "    def evaluate(self, game, state, legal_actions: list[Any]):\n",
    "        policy = self.rng.random(len(legal_actions))\n",
    "        values = self.rng.random(game.num_players(state))\n",
    "        return NetworkEvaluatorResult(policy, values)\n",
    "\n",
    "evaluator = RandomEvaluator()\n",
    "player = AlphazeroPlayer(game, evaluator)\n",
    "game_result = play_game(game, [player, player])\n",
    "\n",
    "game_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ActionHistoryTransformer(\n",
       "  (action_embedding): Embedding(8, 32)\n",
       "  (transformer): Transformer(\n",
       "    (wpe): Embedding(44, 32)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=32, out_features=96, bias=False)\n",
       "          (c_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (policy_value_head): PolicyValueHead(\n",
       "    (policy): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "# Connect5 to make it harder to connect! This helps test variable policy and longer games.\n",
    "base_game, max_game_length = Connect4Game(connect_length=5), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(base_game.all_actions()))\n",
    "\n",
    "n_max_context = max_game_length + 2\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {base_game.__class__.__name__}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "# Make model initialization deterministic\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "model = ActionHistoryTransformer(config=model_config, action_vocab_size=vocab.vocab_size, num_players=game.num_players(state_0))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_size=small, simulations=50\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n",
      "game length: 22, simulations=50\n",
      "[6, 3, 6, 5, 1, 7, 6, 6, 1, 3, 3, 7, 5, 6, 4, 2, 4, 1, 6, 4, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, QueuedNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "\n",
    "async def play_single_deterministic_game_nnet_async(seed, player=None, player_factory=None, verbose=False, simulations=800):\n",
    "    player = player or player_factory()\n",
    "    game_result = await play_game_async(game, [player, player])\n",
    "    if verbose:\n",
    "        print(f'game length: {len(game_result[\"action_history\"])}, simulations={player.simulations}')\n",
    "        print(game_result['action_history'])\n",
    "    return game_result\n",
    "\n",
    "async def play_multiple_deterministic_games_nnet_async(num_games: int, player_factory_factory=None, **kwargs):\n",
    "    if player_factory_factory:\n",
    "        player_factory = await player_factory_factory()\n",
    "        kwargs['player_factory'] = player_factory\n",
    "    tasks = [play_single_deterministic_game_nnet_async(**kwargs) for _ in range(num_games)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "PARALLEL_PLAY_BENCHMARK = True\n",
    "if PARALLEL_PLAY_BENCHMARK:\n",
    "    simulations = NUM_SIMULATIONS\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(f\"model_size={MODEL_SIZE}, simulations={simulations}\")\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=vocab)\n",
    "    queued_evaluator = QueuedNetworkEvaluator(serial_evaluator, max_batch_size=1024, max_latency_ms=0, verbose=False)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, start=True, verbose=False)\n",
    "    async_evaluator_factory = lambda: AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, start=True, verbose=False)\n",
    "    await async_evaluator.start()\n",
    "    serial_player_factory = lambda: AlphazeroPlayer(game, serial_evaluator, rng=np.random.default_rng(seed), simulations=simulations)\n",
    "    queued_player_factory = lambda: AlphazeroPlayer(game, queued_evaluator, rng=np.random.default_rng(seed), simulations=simulations)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(seed), simulations=simulations)\n",
    "    async def async_player_factory_factory():\n",
    "        async_evaluator = async_evaluator_factory()\n",
    "        await async_evaluator.start()\n",
    "        return lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(seed), simulations=simulations)\n",
    "\n",
    "    results = asyncio.run(play_multiple_deterministic_games_nnet_async(num_games=50, seed=42, player_factory=async_player_factory, verbose=True)) # 10.4s\n",
    "\n",
    "    await async_evaluator.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: Vocab(vocab_size=8, itos=('START_OF_GAME', 1, 2, 3, 4, 5, 6, 7), stoi={'START_OF_GAME': 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7})\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "\n",
    "all_actions = game.all_actions()\n",
    "num_players = game.num_players(state_0)\n",
    "\n",
    "vocab = Vocab(itos=(TOKENS.START_OF_GAME,) + all_actions)\n",
    "print(f\"Vocab: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "td_builder = TrajectoryDatasetBuilder(vocab)\n",
    "add_trajectory(game_result, vocab, td_builder)\n",
    "\n",
    "td_builder.save(DATA_DIR, 'train_1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrajectoryTuple(action=tensor([4, 2, 6, 2, 5]), policy=tensor([[0.0000, 0.1887, 0.1438, 0.1650, 0.1262, 0.0125, 0.1963, 0.1675],\n",
       "        [0.0000, 0.2025, 0.1912, 0.1413, 0.2287, 0.0813, 0.0050, 0.1500],\n",
       "        [0.0000, 0.1150, 0.0263, 0.2075, 0.1125, 0.1300, 0.2300, 0.1788],\n",
       "        [0.0000, 0.0787, 0.2287, 0.1950, 0.1300, 0.0300, 0.1575, 0.1800],\n",
       "        [0.0000, 0.1075, 0.1137, 0.1050, 0.0325, 0.5750, 0.0300, 0.0362]],\n",
       "       dtype=torch.float64), value=tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TrajectoryDataset(DATA_DIR, 'train_1000', 5)\n",
    "td[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[4, 2, 6, 2, 5]]), tensor([[[0.0000, 0.1887, 0.1438, 0.1650, 0.1262, 0.0125, 0.1963, 0.1675],\n",
      "         [0.0000, 0.2025, 0.1912, 0.1413, 0.2287, 0.0813, 0.0050, 0.1500],\n",
      "         [0.0000, 0.1150, 0.0263, 0.2075, 0.1125, 0.1300, 0.2300, 0.1788],\n",
      "         [0.0000, 0.0787, 0.2287, 0.1950, 0.1300, 0.0300, 0.1575, 0.1800],\n",
      "         [0.0000, 0.1075, 0.1137, 0.1050, 0.0325, 0.5750, 0.0300, 0.0362]]],\n",
      "       dtype=torch.float64), tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]]))\n"
     ]
    }
   ],
   "source": [
    "loader = build_trajectory_loader(\n",
    "    DATA_DIR, 'train_1000', block_size=5, batch_size=1,\n",
    "    device_is_cuda=False, workers=4)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model (random weights) and play a single game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tiny_config: TransformerConfig = TransformerConfig(n_max_context=100, n_layer=2, n_head=2, n_embd=8)\n",
    "tiny_model = ActionHistoryTransformer(config=tiny_config, action_vocab_size=vocab.vocab_size, num_players=num_players)\n",
    "tiny_model.to(device)\n",
    "tiny_evaluator = ActionHistoryTransformerEvaluator(tiny_model, device=device, block_size=5, vocab=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkEvaluatorResult(legal_policy=array([0.13419195, 0.14256692, 0.14048576, 0.15470095, 0.13918312, 0.14442852, 0.1444428 ], dtype=float32), player_values=array([ 0.03081834, -0.0308184 ], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_actions = game.legal_actions(state_0)\n",
    "tiny_evaluator.evaluate(game, state_0, legal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 1,\n",
       " 'rewards': array([1., 0.], dtype=float32),\n",
       " 'action_history': [2, 6, 4, 1, 3, 7, 5],\n",
       " 'legal_policies': [array([0.13124998, 0.145     , 0.13875002, 0.15499999, 0.135     , 0.14625002, 0.14874996], dtype=float32),\n",
       "  array([0.14499998, 0.14625001, 0.13875   , 0.14874996, 0.14125   , 0.14125   , 0.13875   ], dtype=float32),\n",
       "  array([0.13499999, 0.14249998, 0.13625002, 0.15125   , 0.13625002, 0.15125   , 0.1475    ], dtype=float32),\n",
       "  array([0.13625   , 0.13625   , 0.13875   , 0.16375   , 0.14999999, 0.14125   , 0.13374996], dtype=float32),\n",
       "  array([0.09624999, 0.09874998, 0.45499998, 0.09750002, 0.07874998, 0.09      , 0.08375002], dtype=float32),\n",
       "  array([0.14375   , 0.15      , 0.14874998, 0.15875003, 0.1025    , 0.15125002, 0.14499998], dtype=float32),\n",
       "  array([0.00375   , 0.005     , 0.005     , 0.005     , 0.97124994, 0.005     , 0.005     ], dtype=float32)],\n",
       " 'final_state': HistoryTrackingGameState(base_state=Connect4State(board=array([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 1, 1, 1, 2, 2]], dtype=int8), current_player=2, is_terminal=True, winner=1), action_history=('START_OF_GAME', 2, 6, 4, 1, 3, 7, 5)),\n",
       " 'legal_action_idx': [array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6])]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_player = AlphazeroPlayer(game, tiny_evaluator)\n",
    "tiny_game_result = play_game(game, [tiny_player, tiny_player])\n",
    "\n",
    "tiny_game_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_row: tensor([[0.4045, 0.5594]])\n",
      "b_row: tensor([[0.9683, 0.1765]])\n",
      "loss: 0.8582934141159058\n",
      "a_tiled: tensor([[0.4045, 0.5594],\n",
      "        [0.4045, 0.5594],\n",
      "        [0.4045, 0.5594]])\n",
      "b_tiled: tensor([[0.9683, 0.1765],\n",
      "        [0.9683, 0.1765],\n",
      "        [0.9683, 0.1765]])\n",
      "loss: 0.858293354511261\n",
      "loss: 2.5748801231384277\n"
     ]
    }
   ],
   "source": [
    "a_row = torch.rand(1, 2)\n",
    "b_row = torch.rand(1, 2)\n",
    "print(f'a_row: {a_row}')\n",
    "print(f'b_row: {b_row}')\n",
    "print(f'loss: {F.cross_entropy(a_row, b_row)}')\n",
    "\n",
    "tile_shape = (3, 1)\n",
    "a_tiled = torch.tile(a_row, tile_shape)\n",
    "b_tiled = torch.tile(b_row, tile_shape)\n",
    "print(f'a_tiled: {a_tiled}')\n",
    "print(f'b_tiled: {b_tiled}')\n",
    "print(f'loss: {F.cross_entropy(a_tiled, b_tiled)}')\n",
    "\n",
    "print(f'loss: {F.cross_entropy(a_tiled, b_tiled, reduction=\"sum\")}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
