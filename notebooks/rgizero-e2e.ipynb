{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"tiny\"  # \"tiny\" or \"small\" or \"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "RUN_GENERATIONS = True\n",
    "RUN_TOURNAMENT = False\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 10_000\n",
    "MAX_TRAINING_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 2048\n",
    "MAX_TRAINING_ITERS = 1_000_000 // TRAIN_BATCH_SIZE\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 20\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MODEL_SIZE = \"small\"\n",
    "MAX_TRAINING_ITERS = 100_000_000 // TRAIN_BATCH_SIZE\n",
    "MAX_TRAINING_EPOCHS = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        win2_pct = 100 * counts[2] / total if total > 0 else 0\n",
    "        draw_pct = 100 * counts[None] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}, win[2]={win2_pct:.2f}% draw={draw_pct:.2f}%\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "# TODO: Use MODEL_SIZE!\n",
    "# model_config = model_config_dict[\"small\"] # Override to see if we can fit better.\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer], max_concurrent_games: int = 1000):\n",
    "    sem = asyncio.Semaphore(max_concurrent_games)\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        async with sem:\n",
    "            t0 = time.time()\n",
    "            player = player_factory()\n",
    "            game_result = await play_game_async(game, [player, player])\n",
    "            t1 = time.time()\n",
    "            game_result['time'] = t1 - t0\n",
    "            return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS, max_concurrent_games=1024):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=max_concurrent_games, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory, max_concurrent_games=max_concurrent_games)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}, draw={100*winner_stats[None]/sum(winner_stats.values()):.2f}%\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}, win[2]={100*counts[2]/sum(counts.values()):.2f}% draw={100*counts[None]/sum(counts.values()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = MAX_TRAINING_EPOCHS,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "\n",
    "    learning_rate = LEARNING_RATE,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = MAX_TRAINING_ITERS,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n",
    "        device=device, workers=num_workers, shuffle=True)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        # TODO: We're continuing training on a previosu model here ... should we train a new model from scratch?\n",
    "        print(train_config)\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 140493\n",
      "  Avg trajectory length: 14.05\n",
      "  Trajectory length - min: 7, max: 42, mean: 14.05\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=61.93% win[2]=38.06%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=1564 win[1]=51.92% counts=Counter({1: 812, 2: 752}), win[2]=48.08% draw=0.00%\n",
      "  a=2: n=1344 win[1]=57.37% counts=Counter({1: 771, 2: 573}), win[2]=42.63% draw=0.00%\n",
      "  a=3: n=1297 win[1]=65.23% counts=Counter({1: 846, 2: 451}), win[2]=34.77% draw=0.00%\n",
      "  a=4: n=1493 win[1]=76.36% counts=Counter({1: 1140, 2: 353}), win[2]=23.64% draw=0.00%\n",
      "  a=5: n=1525 win[1]=66.49% counts=Counter({1: 1014, 2: 511}), win[2]=33.51% draw=0.00%\n",
      "  a=6: n=1401 win[1]=62.03% counts=Counter({1: 869, 2: 532}), win[2]=37.97% draw=0.00%\n",
      "  a=7: n=1376 win[1]=53.85% counts=Counter({1: 741, 2: 634, None: 1}), win[2]=46.08% draw=0.07%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-2\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 136161\n",
      "  Avg trajectory length: 13.62\n",
      "  Trajectory length - min: 7, max: 38, mean: 13.62\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=46.45% win[2]=53.55%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=933 win[1]=34.19% counts=Counter({2: 614, 1: 319}), win[2]=65.81% draw=0.00%\n",
      "  a=2: n=328 win[1]=20.12% counts=Counter({2: 262, 1: 66}), win[2]=79.88% draw=0.00%\n",
      "  a=3: n=1647 win[1]=43.90% counts=Counter({2: 924, 1: 723}), win[2]=56.10% draw=0.00%\n",
      "  a=4: n=4260 win[1]=57.54% counts=Counter({1: 2451, 2: 1809}), win[2]=42.46% draw=0.00%\n",
      "  a=5: n=1508 win[1]=41.71% counts=Counter({2: 879, 1: 629}), win[2]=58.29% draw=0.00%\n",
      "  a=6: n=330 win[1]=26.06% counts=Counter({2: 244, 1: 86}), win[2]=73.94% draw=0.00%\n",
      "  a=7: n=994 win[1]=37.32% counts=Counter({2: 623, 1: 371}), win[2]=62.68% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-2.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 141481\n",
      "  Avg trajectory length: 14.15\n",
      "  Trajectory length - min: 7, max: 42, mean: 14.15\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=52.01% win[2]=47.96%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=43.62% counts=Counter({2: 53, 1: 41}), win[2]=56.38% draw=0.00%\n",
      "  a=2: n=109 win[1]=32.11% counts=Counter({2: 74, 1: 35}), win[2]=67.89% draw=0.00%\n",
      "  a=3: n=103 win[1]=22.33% counts=Counter({2: 80, 1: 23}), win[2]=77.67% draw=0.00%\n",
      "  a=4: n=9218 win[1]=53.38% counts=Counter({1: 4921, 2: 4295, None: 2}), win[2]=46.59% draw=0.02%\n",
      "  a=5: n=132 win[1]=50.00% counts=Counter({1: 66, 2: 65, None: 1}), win[2]=49.24% draw=0.76%\n",
      "  a=6: n=148 win[1]=34.46% counts=Counter({2: 97, 1: 51}), win[2]=65.54% draw=0.00%\n",
      "  a=7: n=196 win[1]=32.65% counts=Counter({2: 132, 1: 64}), win[2]=67.35% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-3.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-4\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 139426\n",
      "  Avg trajectory length: 13.94\n",
      "  Trajectory length - min: 7, max: 39, mean: 13.94\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=45.70% win[2]=54.30%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=38.85% counts=Counter({2: 85, 1: 54}), win[2]=61.15% draw=0.00%\n",
      "  a=2: n=7686 win[1]=43.98% counts=Counter({2: 4306, 1: 3380}), win[2]=56.02% draw=0.00%\n",
      "  a=3: n=349 win[1]=42.69% counts=Counter({2: 200, 1: 149}), win[2]=57.31% draw=0.00%\n",
      "  a=4: n=882 win[1]=64.06% counts=Counter({1: 565, 2: 317}), win[2]=35.94% draw=0.00%\n",
      "  a=5: n=319 win[1]=57.68% counts=Counter({1: 184, 2: 135}), win[2]=42.32% draw=0.00%\n",
      "  a=6: n= 94 win[1]=39.36% counts=Counter({2: 57, 1: 37}), win[2]=60.64% draw=0.00%\n",
      "  a=7: n=531 win[1]=37.85% counts=Counter({2: 330, 1: 201}), win[2]=62.15% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-4.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-5\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 137305\n",
      "  Avg trajectory length: 13.73\n",
      "  Trajectory length - min: 7, max: 42, mean: 13.73\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=58.39% win[2]=41.61%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=462 win[1]=37.66% counts=Counter({2: 288, 1: 174}), win[2]=62.34% draw=0.00%\n",
      "  a=2: n=1005 win[1]=54.63% counts=Counter({1: 549, 2: 456}), win[2]=45.37% draw=0.00%\n",
      "  a=3: n=1499 win[1]=48.10% counts=Counter({2: 778, 1: 721}), win[2]=51.90% draw=0.00%\n",
      "  a=4: n=5503 win[1]=67.85% counts=Counter({1: 3734, 2: 1769}), win[2]=32.15% draw=0.00%\n",
      "  a=5: n=864 win[1]=41.32% counts=Counter({2: 507, 1: 357}), win[2]=58.68% draw=0.00%\n",
      "  a=6: n=191 win[1]=65.45% counts=Counter({1: 125, 2: 66}), win[2]=34.55% draw=0.00%\n",
      "  a=7: n=476 win[1]=37.61% counts=Counter({2: 297, 1: 179}), win[2]=62.39% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-5.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 6 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-6\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 123991\n",
      "  Avg trajectory length: 12.40\n",
      "  Trajectory length - min: 7, max: 39, mean: 12.40\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=67.09% win[2]=32.91%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=379 win[1]=42.48% counts=Counter({2: 218, 1: 161}), win[2]=57.52% draw=0.00%\n",
      "  a=2: n=237 win[1]=37.97% counts=Counter({2: 147, 1: 90}), win[2]=62.03% draw=0.00%\n",
      "  a=3: n=2182 win[1]=78.92% counts=Counter({1: 1722, 2: 460}), win[2]=21.08% draw=0.00%\n",
      "  a=4: n=2659 win[1]=78.19% counts=Counter({1: 2079, 2: 580}), win[2]=21.81% draw=0.00%\n",
      "  a=5: n=1111 win[1]=52.57% counts=Counter({1: 584, 2: 527}), win[2]=47.43% draw=0.00%\n",
      "  a=6: n=3288 win[1]=60.64% counts=Counter({1: 1994, 2: 1294}), win[2]=39.36% draw=0.00%\n",
      "  a=7: n=144 win[1]=54.86% counts=Counter({1: 79, 2: 65}), win[2]=45.14% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-6.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 7 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-7\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 129576\n",
      "  Avg trajectory length: 12.96\n",
      "  Trajectory length - min: 7, max: 42, mean: 12.96\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=64.54% win[2]=35.45%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=306 win[1]=29.74% counts=Counter({2: 215, 1: 91}), win[2]=70.26% draw=0.00%\n",
      "  a=2: n=797 win[1]=40.65% counts=Counter({2: 473, 1: 324}), win[2]=59.35% draw=0.00%\n",
      "  a=3: n=258 win[1]=55.43% counts=Counter({1: 143, 2: 115}), win[2]=44.57% draw=0.00%\n",
      "  a=4: n=7409 win[1]=70.45% counts=Counter({1: 5220, 2: 2188, None: 1}), win[2]=29.53% draw=0.01%\n",
      "  a=5: n=754 win[1]=51.06% counts=Counter({1: 385, 2: 369}), win[2]=48.94% draw=0.00%\n",
      "  a=6: n=237 win[1]=55.27% counts=Counter({1: 131, 2: 106}), win[2]=44.73% draw=0.00%\n",
      "  a=7: n=239 win[1]=66.95% counts=Counter({1: 160, 2: 79}), win[2]=33.05% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-7.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 8 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-8\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 132905\n",
      "  Avg trajectory length: 13.29\n",
      "  Trajectory length - min: 7, max: 40, mean: 13.29\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=50.73% win[2]=49.27%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=44.60% counts=Counter({2: 77, 1: 62}), win[2]=55.40% draw=0.00%\n",
      "  a=2: n=2769 win[1]=42.58% counts=Counter({2: 1590, 1: 1179}), win[2]=57.42% draw=0.00%\n",
      "  a=3: n=379 win[1]=50.66% counts=Counter({1: 192, 2: 187}), win[2]=49.34% draw=0.00%\n",
      "  a=4: n=2369 win[1]=74.00% counts=Counter({1: 1753, 2: 616}), win[2]=26.00% draw=0.00%\n",
      "  a=5: n=1463 win[1]=64.18% counts=Counter({1: 939, 2: 524}), win[2]=35.82% draw=0.00%\n",
      "  a=6: n=415 win[1]=57.35% counts=Counter({1: 238, 2: 177}), win[2]=42.65% draw=0.00%\n",
      "  a=7: n=2466 win[1]=28.79% counts=Counter({2: 1756, 1: 710}), win[2]=71.21% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-8.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 9 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-9\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 130438\n",
      "  Avg trajectory length: 13.04\n",
      "  Trajectory length - min: 7, max: 42, mean: 13.04\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=57.54% win[2]=42.45%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=422 win[1]=25.12% counts=Counter({2: 316, 1: 106}), win[2]=74.88% draw=0.00%\n",
      "  a=2: n=455 win[1]=26.59% counts=Counter({2: 334, 1: 121}), win[2]=73.41% draw=0.00%\n",
      "  a=3: n=1830 win[1]=40.66% counts=Counter({2: 1086, 1: 744}), win[2]=59.34% draw=0.00%\n",
      "  a=4: n=4827 win[1]=75.66% counts=Counter({1: 3652, 2: 1175}), win[2]=24.34% draw=0.00%\n",
      "  a=5: n=774 win[1]=36.43% counts=Counter({2: 492, 1: 282}), win[2]=63.57% draw=0.00%\n",
      "  a=6: n=1261 win[1]=57.97% counts=Counter({1: 731, 2: 529, None: 1}), win[2]=41.95% draw=0.08%\n",
      "  a=7: n=431 win[1]=27.38% counts=Counter({2: 313, 1: 118}), win[2]=72.62% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-9.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 10 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-10\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 129854\n",
      "  Avg trajectory length: 12.99\n",
      "  Trajectory length - min: 7, max: 40, mean: 12.99\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=40.74% win[2]=59.26%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=987 win[1]=23.20% counts=Counter({2: 758, 1: 229}), win[2]=76.80% draw=0.00%\n",
      "  a=2: n=769 win[1]=45.25% counts=Counter({2: 421, 1: 348}), win[2]=54.75% draw=0.00%\n",
      "  a=3: n=1000 win[1]=32.50% counts=Counter({2: 675, 1: 325}), win[2]=67.50% draw=0.00%\n",
      "  a=4: n=1449 win[1]=75.22% counts=Counter({1: 1090, 2: 359}), win[2]=24.78% draw=0.00%\n",
      "  a=5: n=2709 win[1]=39.83% counts=Counter({2: 1630, 1: 1079}), win[2]=60.17% draw=0.00%\n",
      "  a=6: n=2370 win[1]=36.41% counts=Counter({2: 1507, 1: 863}), win[2]=63.59% draw=0.00%\n",
      "  a=7: n=716 win[1]=19.55% counts=Counter({2: 576, 1: 140}), win[2]=80.45% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-10.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 11 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-11\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 94569\n",
      "  Avg trajectory length: 9.46\n",
      "  Trajectory length - min: 7, max: 36, mean: 9.46\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=88.11% win[2]=11.89%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=38.30% counts=Counter({2: 58, 1: 36}), win[2]=61.70% draw=0.00%\n",
      "  a=2: n=212 win[1]=49.53% counts=Counter({2: 107, 1: 105}), win[2]=50.47% draw=0.00%\n",
      "  a=3: n=731 win[1]=66.07% counts=Counter({1: 483, 2: 248}), win[2]=33.93% draw=0.00%\n",
      "  a=4: n=8296 win[1]=93.66% counts=Counter({1: 7770, 2: 526}), win[2]=6.34% draw=0.00%\n",
      "  a=5: n=191 win[1]=54.97% counts=Counter({1: 105, 2: 86}), win[2]=45.03% draw=0.00%\n",
      "  a=6: n=387 win[1]=73.39% counts=Counter({1: 284, 2: 103}), win[2]=26.61% draw=0.00%\n",
      "  a=7: n= 89 win[1]=31.46% counts=Counter({2: 61, 1: 28}), win[2]=68.54% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-11.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 12 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-12\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 119131\n",
      "  Avg trajectory length: 11.91\n",
      "  Trajectory length - min: 7, max: 40, mean: 11.91\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=80.75% win[2]=19.25%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=36.17% counts=Counter({2: 60, 1: 34}), win[2]=63.83% draw=0.00%\n",
      "  a=2: n=212 win[1]=58.96% counts=Counter({1: 125, 2: 87}), win[2]=41.04% draw=0.00%\n",
      "  a=3: n=1213 win[1]=72.38% counts=Counter({1: 878, 2: 335}), win[2]=27.62% draw=0.00%\n",
      "  a=4: n=3363 win[1]=86.41% counts=Counter({1: 2906, 2: 457}), win[2]=13.59% draw=0.00%\n",
      "  a=5: n=2999 win[1]=81.99% counts=Counter({1: 2459, 2: 540}), win[2]=18.01% draw=0.00%\n",
      "  a=6: n=2030 win[1]=80.20% counts=Counter({1: 1628, 2: 402}), win[2]=19.80% draw=0.00%\n",
      "  a=7: n= 89 win[1]=50.56% counts=Counter({1: 45, 2: 44}), win[2]=49.44% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-12.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 13 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-13\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 133159\n",
      "  Avg trajectory length: 13.32\n",
      "  Trajectory length - min: 7, max: 42, mean: 13.32\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=70.93% win[2]=29.06%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 40 win[1]=15.00% counts=Counter({2: 34, 1: 6}), win[2]=85.00% draw=0.00%\n",
      "  a=2: n=163 win[1]=36.81% counts=Counter({2: 103, 1: 60}), win[2]=63.19% draw=0.00%\n",
      "  a=3: n=176 win[1]=27.84% counts=Counter({2: 126, 1: 49, None: 1}), win[2]=71.59% draw=0.57%\n",
      "  a=4: n=9090 win[1]=75.07% counts=Counter({1: 6824, 2: 2266}), win[2]=24.93% draw=0.00%\n",
      "  a=5: n=335 win[1]=27.16% counts=Counter({2: 244, 1: 91}), win[2]=72.84% draw=0.00%\n",
      "  a=6: n=151 win[1]=35.10% counts=Counter({2: 98, 1: 53}), win[2]=64.90% draw=0.00%\n",
      "  a=7: n= 45 win[1]=22.22% counts=Counter({2: 35, 1: 10}), win[2]=77.78% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-13.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 14 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-14\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 130316\n",
      "  Avg trajectory length: 13.03\n",
      "  Trajectory length - min: 7, max: 42, mean: 13.03\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=57.78% win[2]=42.21%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=29.50% counts=Counter({2: 98, 1: 41}), win[2]=70.50% draw=0.00%\n",
      "  a=2: n=323 win[1]=44.58% counts=Counter({2: 179, 1: 144}), win[2]=55.42% draw=0.00%\n",
      "  a=3: n=799 win[1]=38.30% counts=Counter({2: 493, 1: 306}), win[2]=61.70% draw=0.00%\n",
      "  a=4: n=656 win[1]=75.91% counts=Counter({1: 498, 2: 158}), win[2]=24.09% draw=0.00%\n",
      "  a=5: n=2208 win[1]=54.44% counts=Counter({1: 1202, 2: 1005, None: 1}), win[2]=45.52% draw=0.05%\n",
      "  a=6: n=5576 win[1]=62.91% counts=Counter({1: 3508, 2: 2068}), win[2]=37.09% draw=0.00%\n",
      "  a=7: n=299 win[1]=26.42% counts=Counter({2: 220, 1: 79}), win[2]=73.58% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-14.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 15 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-15\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 120071\n",
      "  Avg trajectory length: 12.01\n",
      "  Trajectory length - min: 7, max: 37, mean: 12.01\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=82.39% win[2]=17.61%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=53.19% counts=Counter({1: 50, 2: 44}), win[2]=46.81% draw=0.00%\n",
      "  a=2: n=328 win[1]=66.77% counts=Counter({1: 219, 2: 109}), win[2]=33.23% draw=0.00%\n",
      "  a=3: n=839 win[1]=76.16% counts=Counter({1: 639, 2: 200}), win[2]=23.84% draw=0.00%\n",
      "  a=4: n=7307 win[1]=86.41% counts=Counter({1: 6314, 2: 993}), win[2]=13.59% draw=0.00%\n",
      "  a=5: n=849 win[1]=66.31% counts=Counter({1: 563, 2: 286}), win[2]=33.69% draw=0.00%\n",
      "  a=6: n=439 win[1]=84.28% counts=Counter({1: 370, 2: 69}), win[2]=15.72% draw=0.00%\n",
      "  a=7: n=144 win[1]=58.33% counts=Counter({1: 84, 2: 60}), win[2]=41.67% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-15.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 16 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-16\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 126573\n",
      "  Avg trajectory length: 12.66\n",
      "  Trajectory length - min: 7, max: 34, mean: 12.66\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=68.79% win[2]=31.21%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=37.41% counts=Counter({2: 87, 1: 52}), win[2]=62.59% draw=0.00%\n",
      "  a=2: n=240 win[1]=59.58% counts=Counter({1: 143, 2: 97}), win[2]=40.42% draw=0.00%\n",
      "  a=3: n=498 win[1]=63.65% counts=Counter({1: 317, 2: 181}), win[2]=36.35% draw=0.00%\n",
      "  a=4: n=879 win[1]=82.37% counts=Counter({1: 724, 2: 155}), win[2]=17.63% draw=0.00%\n",
      "  a=5: n=780 win[1]=75.38% counts=Counter({1: 588, 2: 192}), win[2]=24.62% draw=0.00%\n",
      "  a=6: n=7320 win[1]=68.31% counts=Counter({1: 5000, 2: 2320}), win[2]=31.69% draw=0.00%\n",
      "  a=7: n=144 win[1]=38.19% counts=Counter({2: 89, 1: 55}), win[2]=61.81% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-16.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 17 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-17\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 116939\n",
      "  Avg trajectory length: 11.69\n",
      "  Trajectory length - min: 7, max: 36, mean: 11.69\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=78.35% win[2]=21.65%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=23.02% counts=Counter({2: 107, 1: 32}), win[2]=76.98% draw=0.00%\n",
      "  a=2: n=240 win[1]=57.08% counts=Counter({1: 137, 2: 103}), win[2]=42.92% draw=0.00%\n",
      "  a=3: n=388 win[1]=70.36% counts=Counter({1: 273, 2: 115}), win[2]=29.64% draw=0.00%\n",
      "  a=4: n=594 win[1]=85.52% counts=Counter({1: 508, 2: 86}), win[2]=14.48% draw=0.00%\n",
      "  a=5: n=724 win[1]=78.31% counts=Counter({1: 567, 2: 157}), win[2]=21.69% draw=0.00%\n",
      "  a=6: n=7771 win[1]=80.62% counts=Counter({1: 6265, 2: 1506}), win[2]=19.38% draw=0.00%\n",
      "  a=7: n=144 win[1]=36.81% counts=Counter({2: 91, 1: 53}), win[2]=63.19% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-17.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 18 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-18\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 125424\n",
      "  Avg trajectory length: 12.54\n",
      "  Trajectory length - min: 7, max: 42, mean: 12.54\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=67.22% win[2]=32.77%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=37.23% counts=Counter({2: 59, 1: 35}), win[2]=62.77% draw=0.00%\n",
      "  a=2: n= 45 win[1]=35.56% counts=Counter({2: 29, 1: 16}), win[2]=64.44% draw=0.00%\n",
      "  a=3: n=964 win[1]=75.41% counts=Counter({1: 727, 2: 236, None: 1}), win[2]=24.48% draw=0.10%\n",
      "  a=4: n=2548 win[1]=80.02% counts=Counter({1: 2039, 2: 509}), win[2]=19.98% draw=0.00%\n",
      "  a=5: n=2625 win[1]=64.53% counts=Counter({1: 1694, 2: 931}), win[2]=35.47% draw=0.00%\n",
      "  a=6: n=3528 win[1]=61.05% counts=Counter({1: 2154, 2: 1374}), win[2]=38.95% draw=0.00%\n",
      "  a=7: n=196 win[1]=29.08% counts=Counter({2: 139, 1: 57}), win[2]=70.92% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-18.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 19 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-19\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 125489\n",
      "  Avg trajectory length: 12.55\n",
      "  Trajectory length - min: 7, max: 36, mean: 12.55\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=64.81% win[2]=35.19%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=203 win[1]=33.00% counts=Counter({2: 136, 1: 67}), win[2]=67.00% draw=0.00%\n",
      "  a=2: n=730 win[1]=42.60% counts=Counter({2: 419, 1: 311}), win[2]=57.40% draw=0.00%\n",
      "  a=3: n=768 win[1]=66.54% counts=Counter({1: 511, 2: 257}), win[2]=33.46% draw=0.00%\n",
      "  a=4: n=5985 win[1]=72.97% counts=Counter({1: 4367, 2: 1618}), win[2]=27.03% draw=0.00%\n",
      "  a=5: n=938 win[1]=60.87% counts=Counter({1: 571, 2: 367}), win[2]=39.13% draw=0.00%\n",
      "  a=6: n=1232 win[1]=49.68% counts=Counter({2: 620, 1: 612}), win[2]=50.32% draw=0.00%\n",
      "  a=7: n=144 win[1]=29.17% counts=Counter({2: 102, 1: 42}), win[2]=70.83% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-19.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 20 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-20\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 132275\n",
      "  Avg trajectory length: 13.23\n",
      "  Trajectory length - min: 7, max: 40, mean: 13.23\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=56.71% win[2]=43.29%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=306 win[1]=25.16% counts=Counter({2: 229, 1: 77}), win[2]=74.84% draw=0.00%\n",
      "  a=2: n=256 win[1]=37.11% counts=Counter({2: 161, 1: 95}), win[2]=62.89% draw=0.00%\n",
      "  a=3: n=601 win[1]=29.45% counts=Counter({2: 424, 1: 177}), win[2]=70.55% draw=0.00%\n",
      "  a=4: n=7363 win[1]=65.96% counts=Counter({1: 4857, 2: 2506}), win[2]=34.04% draw=0.00%\n",
      "  a=5: n=660 win[1]=33.03% counts=Counter({2: 442, 1: 218}), win[2]=66.97% draw=0.00%\n",
      "  a=6: n=515 win[1]=35.15% counts=Counter({2: 334, 1: 181}), win[2]=64.85% draw=0.00%\n",
      "  a=7: n=299 win[1]=22.07% counts=Counter({2: 233, 1: 66}), win[2]=77.93% draw=0.00%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-20.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, NUM_GENERATIONS+1):\n",
    "        current_model = model_dict[generation_id-1]\n",
    "        results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "        results_dict[generation_id] = results_i\n",
    "        trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "        model_dict[generation_id] = model_i\n",
    "\n",
    "## refactor, learning_rate = 0.05, warmup_iters=0\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/488: loss 2.7801, time 611.02ms\n",
    "# iter 100/105/488: loss 2.5840, time 63.73ms\n",
    "# iter 200/205/488: loss 2.5958, time 62.98ms\n",
    "# iter 300/305/488: loss 2.5835, time 60.15ms\n",
    "# iter 400/405/488: loss 2.5793, time 63.62ms\n",
    "\n",
    "# ## model = small\n",
    "# step 0: train loss 2.7741, val loss 2.7741\n",
    "# iter 0/5/488: loss 2.7743, time 1624.89ms\n",
    "# iter 100/105/488: loss 2.6157, time 141.39ms\n",
    "# iter 200/205/488: loss 2.6120, time 161.22ms\n",
    "# iter 300/305/488: loss 2.5983, time 203.82ms\n",
    "\n",
    "\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/4882: loss 2.7801, time 1422.53ms\n",
    "# iter 100/105/4882: loss 2.5970, time 110.71ms\n",
    "# iter 200/205/4882: loss 2.5962, time 116.96ms\n",
    "# iter 300/305/4882: loss 2.5917, time 160.95ms\n",
    "# iter 400/405/4882: loss 2.5885, time 63.37ms\n",
    "# iter 500/505/4882: loss 2.5912, time 65.25ms\n",
    "# iter 600/605/4882: loss 2.6000, time 67.49ms\n",
    "# iter 700/705/4882: loss 2.5780, time 61.43ms\n",
    "# iter 800/805/4882: loss 2.5864, time 265.56ms\n",
    "# iter 900/905/4882: loss 2.5857, time 263.09ms\n",
    "# step 1000: train loss 2.5849, val loss 2.5847\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 1000/1005/4882: loss 2.5844, time 1812.58ms\n",
    "# iter 1100/1105/4882: loss 2.5832, time 62.89ms\n",
    "# iter 1200/1205/4882: loss 2.5743, time 95.25ms\n",
    "# iter 1300/1305/4882: loss 2.5720, time 324.18ms\n",
    "# iter 1400/1405/4882: loss 2.5880, time 73.66ms\n",
    "# iter 1500/1505/4882: loss 2.5745, time 295.39ms\n",
    "# iter 1600/1605/4882: loss 2.5726, time 76.05ms\n",
    "# iter 1700/1705/4882: loss 2.5670, time 63.20ms\n",
    "# iter 1800/1805/4882: loss 2.5720, time 62.66ms\n",
    "# iter 1900/1905/4882: loss 2.5694, time 449.06ms\n",
    "# step 2000: train loss 2.5806, val loss 2.5806\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 2000/2005/4882: loss 2.5893, time 920.12ms\n",
    "# iter 2100/2105/4882: loss 2.5686, time 430.53ms\n",
    "# iter 2200/2205/4882: loss 2.5741, time 63.05ms\n",
    "# iter 2300/2305/4882: loss 2.5679, time 60.90ms\n",
    "# iter 2400/2405/4882: loss 2.5754, time 69.07ms\n",
    "# iter 2500/2505/4882: loss 2.5673, time 68.33ms\n",
    "# iter 2600/2605/4882: loss 2.5648, time 66.26ms\n",
    "# iter 2700/2705/4882: loss 2.5622, time 69.76ms\n",
    "# iter 2800/2805/4882: loss 2.5541, time 143.65ms\n",
    "# iter 2900/2905/4882: loss 2.5634, time 66.40ms\n",
    "# step 3000: train loss 2.5550, val loss 2.5547\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 3000/3005/4882: loss 2.5545, time 975.15ms\n",
    "# iter 3100/3105/4882: loss 2.5594, time 63.55ms\n",
    "# iter 3200/3205/4882: loss 2.5499, time 64.17ms\n",
    "# iter 3300/3305/4882: loss 2.5481, time 70.28ms\n",
    "# iter 3400/3405/4882: loss 2.5565, time 73.58ms\n",
    "# iter 3500/3505/4882: loss 2.5602, time 72.22ms\n",
    "# iter 3600/3605/4882: loss 2.5429, time 88.68ms\n",
    "# iter 3700/3705/4882: loss 2.5259, time 63.15ms\n",
    "# iter 3800/3805/4882: loss 2.5346, time 66.07ms\n",
    "# iter 3900/3905/4882: loss 2.5386, time 73.50ms\n",
    "# step 4000: train loss 2.5350, val loss 2.5345\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 4000/4005/4882: loss 2.5424, time 1217.41ms\n",
    "# iter 4100/4105/4882: loss 2.5290, time 101.01ms\n",
    "# iter 4200/4205/4882: loss 2.5323, time 61.94ms\n",
    "# iter 4300/4305/4882: loss 2.5250, time 72.57ms\n",
    "# iter 4400/4405/4882: loss 2.5243, time 68.38ms\n",
    "# iter 4500/4505/4882: loss 2.5331, time 73.33ms\n",
    "# iter 4600/4605/4882: loss 2.5246, time 101.00ms\n",
    "# iter 4700/4705/4882: loss 2.5336, time 67.27ms\n",
    "# iter 4800/4805/4882: loss 2.5170, time 79.40ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dd) = 352822\n"
     ]
    }
   ],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        # dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:5].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:6].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:7].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:8].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:9].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:10].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([126100.5000,  73899.5000]), win_pct=63.05%, sum=200000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "gen=6: tensor([6709., 3291.]), win_pct=67.09%, sum=10000.0\n",
      "gen=7: tensor([6454.5000, 3545.5000]), win_pct=64.54%, sum=10000.0\n",
      "gen=8: tensor([5073., 4927.]), win_pct=50.73%, sum=10000.0\n",
      "gen=9: tensor([5754.5000, 4245.5000]), win_pct=57.54%, sum=10000.0\n",
      "gen=10: tensor([4074., 5926.]), win_pct=40.74%, sum=10000.0\n",
      "gen=11: tensor([8811., 1189.]), win_pct=88.11%, sum=10000.0\n",
      "gen=12: tensor([8075., 1925.]), win_pct=80.75%, sum=10000.0\n",
      "gen=13: tensor([7093.5000, 2906.5000]), win_pct=70.93%, sum=10000.0\n",
      "gen=14: tensor([5778.5000, 4221.5000]), win_pct=57.78%, sum=10000.0\n",
      "gen=15: tensor([8239., 1761.]), win_pct=82.39%, sum=10000.0\n",
      "gen=16: tensor([6879., 3121.]), win_pct=68.79%, sum=10000.0\n",
      "gen=17: tensor([7835., 2165.]), win_pct=78.35%, sum=10000.0\n",
      "gen=18: tensor([6722.5000, 3277.5000]), win_pct=67.22%, sum=10000.0\n",
      "gen=19: tensor([6481., 3519.]), win_pct=64.81%, sum=10000.0\n",
      "gen=20: tensor([5671., 4329.]), win_pct=56.71%, sum=10000.0\n",
      "player_probs=[0.4990083 0.5009917]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([ 8218., 10030.]), win_pct=45.04%, sum=18248.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "gen=6: tensor([ 90., 147.]), win_pct=37.97%, sum=237.0\n",
      "gen=7: tensor([324., 473.]), win_pct=40.65%, sum=797.0\n",
      "gen=8: tensor([1179., 1590.]), win_pct=42.58%, sum=2769.0\n",
      "gen=9: tensor([121., 334.]), win_pct=26.59%, sum=455.0\n",
      "gen=10: tensor([348., 421.]), win_pct=45.25%, sum=769.0\n",
      "gen=11: tensor([105., 107.]), win_pct=49.53%, sum=212.0\n",
      "gen=12: tensor([125.,  87.]), win_pct=58.96%, sum=212.0\n",
      "gen=13: tensor([ 60., 103.]), win_pct=36.81%, sum=163.0\n",
      "gen=14: tensor([144., 179.]), win_pct=44.58%, sum=323.0\n",
      "gen=15: tensor([219., 109.]), win_pct=66.77%, sum=328.0\n",
      "gen=16: tensor([143.,  97.]), win_pct=59.58%, sum=240.0\n",
      "gen=17: tensor([137., 103.]), win_pct=57.08%, sum=240.0\n",
      "gen=18: tensor([16., 29.]), win_pct=35.56%, sum=45.0\n",
      "gen=19: tensor([311., 419.]), win_pct=42.60%, sum=730.0\n",
      "gen=20: tensor([ 95., 161.]), win_pct=37.11%, sum=256.0\n",
      "player_probs=[0.42821875 0.5717813 ]\n",
      "\n",
      "prefix=(0, 2, 3)\n",
      "gen=*: tensor([3163., 4777.]), win_pct=39.84%, sum=7940.0\n",
      "gen=1: tensor([75., 90.]), win_pct=45.45%, sum=165.0\n",
      "gen=2: tensor([ 6., 30.]), win_pct=16.67%, sum=36.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([2022., 3652.]), win_pct=35.64%, sum=5674.0\n",
      "gen=5: tensor([469., 398.]), win_pct=54.09%, sum=867.0\n",
      "gen=6: tensor([7., 2.]), win_pct=77.78%, sum=9.0\n",
      "gen=7: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=8: tensor([ 91., 108.]), win_pct=45.73%, sum=199.0\n",
      "gen=9: tensor([25., 12.]), win_pct=67.57%, sum=37.0\n",
      "gen=10: tensor([284., 379.]), win_pct=42.84%, sum=663.0\n",
      "gen=11: tensor([4., 2.]), win_pct=66.67%, sum=6.0\n",
      "gen=12: tensor([19., 11.]), win_pct=63.33%, sum=30.0\n",
      "gen=14: tensor([29., 23.]), win_pct=55.77%, sum=52.0\n",
      "gen=15: tensor([45., 23.]), win_pct=66.18%, sum=68.0\n",
      "gen=16: tensor([13.,  2.]), win_pct=86.67%, sum=15.0\n",
      "gen=17: tensor([19., 13.]), win_pct=59.38%, sum=32.0\n",
      "gen=18: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=19: tensor([18.,  7.]), win_pct=72.00%, sum=25.0\n",
      "gen=20: tensor([14., 11.]), win_pct=56.00%, sum=25.0\n",
      "player_probs=[0.28790256 0.71209747]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([9949., 7572.]), win_pct=56.78%, sum=17521.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "gen=6: tensor([1722.,  460.]), win_pct=78.92%, sum=2182.0\n",
      "gen=7: tensor([143., 115.]), win_pct=55.43%, sum=258.0\n",
      "gen=8: tensor([192., 187.]), win_pct=50.66%, sum=379.0\n",
      "gen=9: tensor([ 744., 1086.]), win_pct=40.66%, sum=1830.0\n",
      "gen=10: tensor([325., 675.]), win_pct=32.50%, sum=1000.0\n",
      "gen=11: tensor([483., 248.]), win_pct=66.07%, sum=731.0\n",
      "gen=12: tensor([878., 335.]), win_pct=72.38%, sum=1213.0\n",
      "gen=13: tensor([ 49.5000, 126.5000]), win_pct=28.12%, sum=176.0\n",
      "gen=14: tensor([306., 493.]), win_pct=38.30%, sum=799.0\n",
      "gen=15: tensor([639., 200.]), win_pct=76.16%, sum=839.0\n",
      "gen=16: tensor([317., 181.]), win_pct=63.65%, sum=498.0\n",
      "gen=17: tensor([273., 115.]), win_pct=70.36%, sum=388.0\n",
      "gen=18: tensor([727.5000, 236.5000]), win_pct=75.47%, sum=964.0\n",
      "gen=19: tensor([511., 257.]), win_pct=66.54%, sum=768.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.40098962 0.5990104 ]\n",
      "\n",
      "prefix=(0, 3, 4)\n",
      "gen=*: tensor([4546., 6046.]), win_pct=42.92%, sum=10592.0\n",
      "gen=1: tensor([ 99., 103.]), win_pct=49.01%, sum=202.0\n",
      "gen=2: tensor([458., 821.]), win_pct=35.81%, sum=1279.0\n",
      "gen=3: tensor([19., 79.]), win_pct=19.39%, sum=98.0\n",
      "gen=4: tensor([116., 168.]), win_pct=40.85%, sum=284.0\n",
      "gen=5: tensor([656., 746.]), win_pct=46.79%, sum=1402.0\n",
      "gen=6: tensor([18., 14.]), win_pct=56.25%, sum=32.0\n",
      "gen=7: tensor([128., 107.]), win_pct=54.47%, sum=235.0\n",
      "gen=8: tensor([131., 160.]), win_pct=45.02%, sum=291.0\n",
      "gen=9: tensor([ 565., 1030.]), win_pct=35.42%, sum=1595.0\n",
      "gen=10: tensor([225., 637.]), win_pct=26.10%, sum=862.0\n",
      "gen=11: tensor([408., 230.]), win_pct=63.95%, sum=638.0\n",
      "gen=12: tensor([261., 206.]), win_pct=55.89%, sum=467.0\n",
      "gen=13: tensor([ 38.5000, 124.5000]), win_pct=23.62%, sum=163.0\n",
      "gen=14: tensor([228., 467.]), win_pct=32.81%, sum=695.0\n",
      "gen=15: tensor([365., 165.]), win_pct=68.87%, sum=530.0\n",
      "gen=16: tensor([216., 171.]), win_pct=55.81%, sum=387.0\n",
      "gen=17: tensor([135.,  92.]), win_pct=59.47%, sum=227.0\n",
      "gen=18: tensor([136.5000, 143.5000]), win_pct=48.75%, sum=280.0\n",
      "gen=19: tensor([205., 173.]), win_pct=54.23%, sum=378.0\n",
      "gen=20: tensor([138., 409.]), win_pct=25.23%, sum=547.0\n",
      "player_probs=[0.2915901 0.7084099]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([63413.5000, 22736.5000]), win_pct=73.61%, sum=86150.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "gen=6: tensor([2079.,  580.]), win_pct=78.19%, sum=2659.0\n",
      "gen=7: tensor([5220.5000, 2188.5000]), win_pct=70.46%, sum=7409.0\n",
      "gen=8: tensor([1753.,  616.]), win_pct=74.00%, sum=2369.0\n",
      "gen=9: tensor([3652., 1175.]), win_pct=75.66%, sum=4827.0\n",
      "gen=10: tensor([1090.,  359.]), win_pct=75.22%, sum=1449.0\n",
      "gen=11: tensor([7770.,  526.]), win_pct=93.66%, sum=8296.0\n",
      "gen=12: tensor([2906.,  457.]), win_pct=86.41%, sum=3363.0\n",
      "gen=13: tensor([6824., 2266.]), win_pct=75.07%, sum=9090.0\n",
      "gen=14: tensor([498., 158.]), win_pct=75.91%, sum=656.0\n",
      "gen=15: tensor([6314.,  993.]), win_pct=86.41%, sum=7307.0\n",
      "gen=16: tensor([724., 155.]), win_pct=82.37%, sum=879.0\n",
      "gen=17: tensor([508.,  86.]), win_pct=85.52%, sum=594.0\n",
      "gen=18: tensor([2039.,  509.]), win_pct=80.02%, sum=2548.0\n",
      "gen=19: tensor([4367., 1618.]), win_pct=72.97%, sum=5985.0\n",
      "gen=20: tensor([4857., 2506.]), win_pct=65.96%, sum=7363.0\n",
      "player_probs=[0.64143634 0.3585637 ]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([5812., 1448.]), win_pct=80.06%, sum=7260.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "gen=6: tensor([538., 166.]), win_pct=76.42%, sum=704.0\n",
      "gen=7: tensor([79., 24.]), win_pct=76.70%, sum=103.0\n",
      "gen=8: tensor([35., 11.]), win_pct=76.09%, sum=46.0\n",
      "gen=9: tensor([315.,  65.]), win_pct=82.89%, sum=380.0\n",
      "gen=10: tensor([33., 16.]), win_pct=67.35%, sum=49.0\n",
      "gen=11: tensor([246.,  41.]), win_pct=85.71%, sum=287.0\n",
      "gen=12: tensor([967., 228.]), win_pct=80.92%, sum=1195.0\n",
      "gen=13: tensor([81., 15.]), win_pct=84.38%, sum=96.0\n",
      "gen=14: tensor([11.,  4.]), win_pct=73.33%, sum=15.0\n",
      "gen=15: tensor([1334.,  233.]), win_pct=85.13%, sum=1567.0\n",
      "gen=16: tensor([33.,  4.]), win_pct=89.19%, sum=37.0\n",
      "gen=17: tensor([56.,  7.]), win_pct=88.89%, sum=63.0\n",
      "gen=18: tensor([1115.,  245.]), win_pct=81.99%, sum=1360.0\n",
      "gen=19: tensor([143.,  37.]), win_pct=79.44%, sum=180.0\n",
      "gen=20: tensor([297., 129.]), win_pct=69.72%, sum=426.0\n",
      "player_probs=[0.59470844 0.40529156]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([41327.5000, 17642.5000]), win_pct=70.08%, sum=58970.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "gen=6: tensor([1297.,  349.]), win_pct=78.80%, sum=1646.0\n",
      "gen=7: tensor([3362.5000, 1705.5000]), win_pct=66.35%, sum=5068.0\n",
      "gen=8: tensor([680., 218.]), win_pct=75.72%, sum=898.0\n",
      "gen=9: tensor([2494.,  935.]), win_pct=72.73%, sum=3429.0\n",
      "gen=10: tensor([967., 317.]), win_pct=75.31%, sum=1284.0\n",
      "gen=11: tensor([6879.,  427.]), win_pct=94.16%, sum=7306.0\n",
      "gen=12: tensor([603.,  47.]), win_pct=92.77%, sum=650.0\n",
      "gen=13: tensor([4557., 1850.]), win_pct=71.13%, sum=6407.0\n",
      "gen=14: tensor([424., 146.]), win_pct=74.39%, sum=570.0\n",
      "gen=15: tensor([1828.,  290.]), win_pct=86.31%, sum=2118.0\n",
      "gen=16: tensor([489., 122.]), win_pct=80.03%, sum=611.0\n",
      "gen=17: tensor([324.,  61.]), win_pct=84.16%, sum=385.0\n",
      "gen=18: tensor([564., 197.]), win_pct=74.11%, sum=761.0\n",
      "gen=19: tensor([3401., 1391.]), win_pct=70.97%, sum=4792.0\n",
      "gen=20: tensor([3769., 2221.]), win_pct=62.92%, sum=5990.0\n",
      "player_probs=[0.41756445 0.58243555]\n",
      "\n",
      "prefix=(0, 4, 4, 3)\n",
      "gen=*: tensor([8937., 2897.]), win_pct=75.52%, sum=11834.0\n",
      "gen=1: tensor([36.,  3.]), win_pct=92.31%, sum=39.0\n",
      "gen=2: tensor([438., 159.]), win_pct=73.37%, sum=597.0\n",
      "gen=3: tensor([725., 351.]), win_pct=67.38%, sum=1076.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([1974.,  893.]), win_pct=68.85%, sum=2867.0\n",
      "gen=6: tensor([810.,  58.]), win_pct=93.32%, sum=868.0\n",
      "gen=7: tensor([690., 621.]), win_pct=52.63%, sum=1311.0\n",
      "gen=8: tensor([328.,  74.]), win_pct=81.59%, sum=402.0\n",
      "gen=9: tensor([236.,  31.]), win_pct=88.39%, sum=267.0\n",
      "gen=10: tensor([614.,  70.]), win_pct=89.77%, sum=684.0\n",
      "gen=11: tensor([387.,  13.]), win_pct=96.75%, sum=400.0\n",
      "gen=12: tensor([36.,  1.]), win_pct=97.30%, sum=37.0\n",
      "gen=13: tensor([229.,  22.]), win_pct=91.24%, sum=251.0\n",
      "gen=14: tensor([70., 35.]), win_pct=66.67%, sum=105.0\n",
      "gen=15: tensor([231.,  14.]), win_pct=94.29%, sum=245.0\n",
      "gen=16: tensor([67., 12.]), win_pct=84.81%, sum=79.0\n",
      "gen=17: tensor([51.,  7.]), win_pct=87.93%, sum=58.0\n",
      "gen=18: tensor([38., 14.]), win_pct=73.08%, sum=52.0\n",
      "gen=19: tensor([1776.,  410.]), win_pct=81.24%, sum=2186.0\n",
      "gen=20: tensor([199., 109.]), win_pct=64.61%, sum=308.0\n",
      "player_probs=[0.37139595 0.62860405]\n",
      "\n",
      "prefix=(0, 4, 4, 4)\n",
      "gen=*: tensor([14185.,  6380.]), win_pct=68.98%, sum=20565.0\n",
      "gen=1: tensor([30., 12.]), win_pct=71.43%, sum=42.0\n",
      "gen=2: tensor([728., 660.]), win_pct=52.45%, sum=1388.0\n",
      "gen=3: tensor([780., 636.]), win_pct=55.08%, sum=1416.0\n",
      "gen=4: tensor([29., 14.]), win_pct=67.44%, sum=43.0\n",
      "gen=5: tensor([442., 235.]), win_pct=65.29%, sum=677.0\n",
      "gen=6: tensor([123., 184.]), win_pct=40.07%, sum=307.0\n",
      "gen=7: tensor([792., 386.]), win_pct=67.23%, sum=1178.0\n",
      "gen=8: tensor([214.,  64.]), win_pct=76.98%, sum=278.0\n",
      "gen=9: tensor([630., 146.]), win_pct=81.19%, sum=776.0\n",
      "gen=10: tensor([55., 45.]), win_pct=55.00%, sum=100.0\n",
      "gen=11: tensor([956., 126.]), win_pct=88.35%, sum=1082.0\n",
      "gen=12: tensor([208.,  25.]), win_pct=89.27%, sum=233.0\n",
      "gen=13: tensor([3412., 1295.]), win_pct=72.49%, sum=4707.0\n",
      "gen=14: tensor([119.,  68.]), win_pct=63.64%, sum=187.0\n",
      "gen=15: tensor([1213.,  244.]), win_pct=83.25%, sum=1457.0\n",
      "gen=16: tensor([225.,  56.]), win_pct=80.07%, sum=281.0\n",
      "gen=17: tensor([169.,  31.]), win_pct=84.50%, sum=200.0\n",
      "gen=18: tensor([253.,  98.]), win_pct=72.08%, sum=351.0\n",
      "gen=19: tensor([1032.,  607.]), win_pct=62.97%, sum=1639.0\n",
      "gen=20: tensor([2775., 1448.]), win_pct=65.71%, sum=4223.0\n",
      "player_probs=[0.52655715 0.47344282]\n",
      "\n",
      "prefix=(0, 4, 4, 5)\n",
      "gen=*: tensor([4593., 3993.]), win_pct=53.49%, sum=8586.0\n",
      "gen=1: tensor([22.,  5.]), win_pct=81.48%, sum=27.0\n",
      "gen=2: tensor([255., 182.]), win_pct=58.35%, sum=437.0\n",
      "gen=3: tensor([1793., 2519.]), win_pct=41.58%, sum=4312.0\n",
      "gen=5: tensor([384., 367.]), win_pct=51.13%, sum=751.0\n",
      "gen=6: tensor([144.,  35.]), win_pct=80.45%, sum=179.0\n",
      "gen=7: tensor([472., 219.]), win_pct=68.31%, sum=691.0\n",
      "gen=8: tensor([51., 13.]), win_pct=79.69%, sum=64.0\n",
      "gen=9: tensor([28., 13.]), win_pct=68.29%, sum=41.0\n",
      "gen=10: tensor([132.,  49.]), win_pct=72.93%, sum=181.0\n",
      "gen=11: tensor([128.,  40.]), win_pct=76.19%, sum=168.0\n",
      "gen=12: tensor([111.,  14.]), win_pct=88.80%, sum=125.0\n",
      "gen=13: tensor([412., 232.]), win_pct=63.98%, sum=644.0\n",
      "gen=14: tensor([25.,  4.]), win_pct=86.21%, sum=29.0\n",
      "gen=15: tensor([50.,  9.]), win_pct=84.75%, sum=59.0\n",
      "gen=16: tensor([51., 14.]), win_pct=78.46%, sum=65.0\n",
      "gen=17: tensor([13.,  6.]), win_pct=68.42%, sum=19.0\n",
      "gen=18: tensor([54., 22.]), win_pct=71.05%, sum=76.0\n",
      "gen=19: tensor([281., 152.]), win_pct=64.90%, sum=433.0\n",
      "gen=20: tensor([187.,  98.]), win_pct=65.61%, sum=285.0\n",
      "player_probs=[0.3431059 0.656894 ]\n",
      "\n",
      "prefix=(0, 4, 4, 6)\n",
      "gen=*: tensor([9904., 1662.]), win_pct=85.63%, sum=11566.0\n",
      "gen=1: tensor([24.,  7.]), win_pct=77.42%, sum=31.0\n",
      "gen=2: tensor([122.,  86.]), win_pct=58.65%, sum=208.0\n",
      "gen=3: tensor([149.,  58.]), win_pct=71.98%, sum=207.0\n",
      "gen=5: tensor([19.,  2.]), win_pct=90.48%, sum=21.0\n",
      "gen=6: tensor([43., 11.]), win_pct=79.63%, sum=54.0\n",
      "gen=7: tensor([1033.,  150.]), win_pct=87.32%, sum=1183.0\n",
      "gen=8: tensor([66., 24.]), win_pct=73.33%, sum=90.0\n",
      "gen=9: tensor([1419.,  643.]), win_pct=68.82%, sum=2062.0\n",
      "gen=10: tensor([70., 23.]), win_pct=75.27%, sum=93.0\n",
      "gen=11: tensor([5241.,  172.]), win_pct=96.82%, sum=5413.0\n",
      "gen=12: tensor([203.,   2.]), win_pct=99.02%, sum=205.0\n",
      "gen=13: tensor([397., 211.]), win_pct=65.30%, sum=608.0\n",
      "gen=14: tensor([190.,  16.]), win_pct=92.23%, sum=206.0\n",
      "gen=15: tensor([282.,   4.]), win_pct=98.60%, sum=286.0\n",
      "gen=16: tensor([93., 13.]), win_pct=87.74%, sum=106.0\n",
      "gen=17: tensor([58.,  4.]), win_pct=93.55%, sum=62.0\n",
      "gen=18: tensor([197.,  47.]), win_pct=80.74%, sum=244.0\n",
      "gen=19: tensor([144.,  71.]), win_pct=66.98%, sum=215.0\n",
      "gen=20: tensor([154., 118.]), win_pct=56.62%, sum=272.0\n",
      "player_probs=[0.37317386 0.62682617]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([13578.,  9890.]), win_pct=57.86%, sum=23468.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "gen=6: tensor([584., 527.]), win_pct=52.57%, sum=1111.0\n",
      "gen=7: tensor([385., 369.]), win_pct=51.06%, sum=754.0\n",
      "gen=8: tensor([939., 524.]), win_pct=64.18%, sum=1463.0\n",
      "gen=9: tensor([282., 492.]), win_pct=36.43%, sum=774.0\n",
      "gen=10: tensor([1079., 1630.]), win_pct=39.83%, sum=2709.0\n",
      "gen=11: tensor([105.,  86.]), win_pct=54.97%, sum=191.0\n",
      "gen=12: tensor([2459.,  540.]), win_pct=81.99%, sum=2999.0\n",
      "gen=13: tensor([ 91., 244.]), win_pct=27.16%, sum=335.0\n",
      "gen=14: tensor([1202.5000, 1005.5000]), win_pct=54.46%, sum=2208.0\n",
      "gen=15: tensor([563., 286.]), win_pct=66.31%, sum=849.0\n",
      "gen=16: tensor([588., 192.]), win_pct=75.38%, sum=780.0\n",
      "gen=17: tensor([567., 157.]), win_pct=78.31%, sum=724.0\n",
      "gen=18: tensor([1694.,  931.]), win_pct=64.53%, sum=2625.0\n",
      "gen=19: tensor([571., 367.]), win_pct=60.87%, sum=938.0\n",
      "gen=20: tensor([218., 442.]), win_pct=33.03%, sum=660.0\n",
      "player_probs=[0.4678061  0.53219396]\n",
      "\n",
      "prefix=(0, 5, 4)\n",
      "gen=*: tensor([6657., 7845.]), win_pct=45.90%, sum=14502.0\n",
      "gen=1: tensor([102., 113.]), win_pct=47.44%, sum=215.0\n",
      "gen=2: tensor([450., 802.]), win_pct=35.94%, sum=1252.0\n",
      "gen=3: tensor([62.5000, 64.5000]), win_pct=49.21%, sum=127.0\n",
      "gen=4: tensor([124., 116.]), win_pct=51.67%, sum=240.0\n",
      "gen=5: tensor([334., 486.]), win_pct=40.73%, sum=820.0\n",
      "gen=6: tensor([203., 396.]), win_pct=33.89%, sum=599.0\n",
      "gen=7: tensor([314., 357.]), win_pct=46.80%, sum=671.0\n",
      "gen=8: tensor([101., 119.]), win_pct=45.91%, sum=220.0\n",
      "gen=9: tensor([212., 455.]), win_pct=31.78%, sum=667.0\n",
      "gen=10: tensor([ 832., 1528.]), win_pct=35.25%, sum=2360.0\n",
      "gen=11: tensor([84., 85.]), win_pct=49.70%, sum=169.0\n",
      "gen=12: tensor([499., 148.]), win_pct=77.13%, sum=647.0\n",
      "gen=13: tensor([ 73., 240.]), win_pct=23.32%, sum=313.0\n",
      "gen=14: tensor([933.5000, 974.5000]), win_pct=48.93%, sum=1908.0\n",
      "gen=15: tensor([427., 263.]), win_pct=61.88%, sum=690.0\n",
      "gen=16: tensor([328., 154.]), win_pct=68.05%, sum=482.0\n",
      "gen=17: tensor([91., 39.]), win_pct=70.00%, sum=130.0\n",
      "gen=18: tensor([882., 760.]), win_pct=53.71%, sum=1642.0\n",
      "gen=19: tensor([428., 321.]), win_pct=57.14%, sum=749.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.29594842 0.70405155]\n",
      "\n",
      "prefix=(0, 5, 4, 4)\n",
      "gen=*: tensor([4227.5000, 3571.5000]), win_pct=54.21%, sum=7799.0\n",
      "gen=1: tensor([16., 12.]), win_pct=57.14%, sum=28.0\n",
      "gen=2: tensor([347., 560.]), win_pct=38.26%, sum=907.0\n",
      "gen=3: tensor([3., 4.]), win_pct=42.86%, sum=7.0\n",
      "gen=4: tensor([3., 6.]), win_pct=33.33%, sum=9.0\n",
      "gen=5: tensor([262., 347.]), win_pct=43.02%, sum=609.0\n",
      "gen=6: tensor([ 78., 153.]), win_pct=33.77%, sum=231.0\n",
      "gen=7: tensor([260., 188.]), win_pct=58.04%, sum=448.0\n",
      "gen=8: tensor([94., 95.]), win_pct=49.74%, sum=189.0\n",
      "gen=9: tensor([16.,  7.]), win_pct=69.57%, sum=23.0\n",
      "gen=10: tensor([221., 173.]), win_pct=56.09%, sum=394.0\n",
      "gen=11: tensor([73., 60.]), win_pct=54.89%, sum=133.0\n",
      "gen=12: tensor([361.,  95.]), win_pct=79.17%, sum=456.0\n",
      "gen=13: tensor([ 63., 207.]), win_pct=23.33%, sum=270.0\n",
      "gen=14: tensor([679.5000, 520.5000]), win_pct=56.62%, sum=1200.0\n",
      "gen=15: tensor([302., 142.]), win_pct=68.02%, sum=444.0\n",
      "gen=16: tensor([260.,  96.]), win_pct=73.03%, sum=356.0\n",
      "gen=17: tensor([69., 23.]), win_pct=75.00%, sum=92.0\n",
      "gen=18: tensor([766., 559.]), win_pct=57.81%, sum=1325.0\n",
      "gen=19: tensor([266., 161.]), win_pct=62.30%, sum=427.0\n",
      "gen=20: tensor([ 88., 163.]), win_pct=35.06%, sum=251.0\n",
      "player_probs=[0.3536896 0.6463104]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([25180.5000, 13503.5000]), win_pct=65.09%, sum=38684.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "gen=6: tensor([1994., 1294.]), win_pct=60.64%, sum=3288.0\n",
      "gen=7: tensor([131., 106.]), win_pct=55.27%, sum=237.0\n",
      "gen=8: tensor([238., 177.]), win_pct=57.35%, sum=415.0\n",
      "gen=9: tensor([731.5000, 529.5000]), win_pct=58.01%, sum=1261.0\n",
      "gen=10: tensor([ 863., 1507.]), win_pct=36.41%, sum=2370.0\n",
      "gen=11: tensor([284., 103.]), win_pct=73.39%, sum=387.0\n",
      "gen=12: tensor([1628.,  402.]), win_pct=80.20%, sum=2030.0\n",
      "gen=13: tensor([53., 98.]), win_pct=35.10%, sum=151.0\n",
      "gen=14: tensor([3508., 2068.]), win_pct=62.91%, sum=5576.0\n",
      "gen=15: tensor([370.,  69.]), win_pct=84.28%, sum=439.0\n",
      "gen=16: tensor([5000., 2320.]), win_pct=68.31%, sum=7320.0\n",
      "gen=17: tensor([6265., 1506.]), win_pct=80.62%, sum=7771.0\n",
      "gen=18: tensor([2154., 1374.]), win_pct=61.05%, sum=3528.0\n",
      "gen=19: tensor([612., 620.]), win_pct=49.68%, sum=1232.0\n",
      "gen=20: tensor([181., 334.]), win_pct=35.15%, sum=515.0\n",
      "player_probs=[0.49599618 0.5040038 ]\n",
      "\n",
      "prefix=(0, 6, 4)\n",
      "gen=*: tensor([9921.5000, 8800.5000]), win_pct=52.99%, sum=18722.0\n",
      "gen=1: tensor([ 94., 117.]), win_pct=44.55%, sum=211.0\n",
      "gen=2: tensor([ 69., 225.]), win_pct=23.47%, sum=294.0\n",
      "gen=3: tensor([45., 89.]), win_pct=33.58%, sum=134.0\n",
      "gen=4: tensor([24., 46.]), win_pct=34.29%, sum=70.0\n",
      "gen=5: tensor([89., 39.]), win_pct=69.53%, sum=128.0\n",
      "gen=6: tensor([74., 71.]), win_pct=51.03%, sum=145.0\n",
      "gen=7: tensor([109.,  98.]), win_pct=52.66%, sum=207.0\n",
      "gen=8: tensor([57., 29.]), win_pct=66.28%, sum=86.0\n",
      "gen=9: tensor([218.5000, 115.5000]), win_pct=65.42%, sum=334.0\n",
      "gen=10: tensor([ 621., 1373.]), win_pct=31.14%, sum=1994.0\n",
      "gen=11: tensor([218.,  96.]), win_pct=69.43%, sum=314.0\n",
      "gen=12: tensor([244., 136.]), win_pct=64.21%, sum=380.0\n",
      "gen=13: tensor([42., 95.]), win_pct=30.66%, sum=137.0\n",
      "gen=14: tensor([1581., 1585.]), win_pct=49.94%, sum=3166.0\n",
      "gen=15: tensor([68., 30.]), win_pct=69.39%, sum=98.0\n",
      "gen=16: tensor([3349., 2052.]), win_pct=62.01%, sum=5401.0\n",
      "gen=17: tensor([1336.,  635.]), win_pct=67.78%, sum=1971.0\n",
      "gen=18: tensor([1114., 1100.]), win_pct=50.32%, sum=2214.0\n",
      "gen=19: tensor([430., 561.]), win_pct=43.39%, sum=991.0\n",
      "gen=20: tensor([139., 308.]), win_pct=31.10%, sum=447.0\n",
      "player_probs=[0.30287927 0.6971207 ]\n",
      "\n",
      "prefix=(0, 6, 4, 4)\n",
      "gen=*: tensor([4925., 3458.]), win_pct=58.75%, sum=8383.0\n",
      "gen=1: tensor([17., 19.]), win_pct=47.22%, sum=36.0\n",
      "gen=2: tensor([27., 88.]), win_pct=23.48%, sum=115.0\n",
      "gen=3: tensor([27., 40.]), win_pct=40.30%, sum=67.0\n",
      "gen=4: tensor([1., 1.]), win_pct=50.00%, sum=2.0\n",
      "gen=5: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=6: tensor([54., 46.]), win_pct=54.00%, sum=100.0\n",
      "gen=7: tensor([49., 73.]), win_pct=40.16%, sum=122.0\n",
      "gen=8: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=9: tensor([3., 2.]), win_pct=60.00%, sum=5.0\n",
      "gen=10: tensor([ 85., 124.]), win_pct=40.67%, sum=209.0\n",
      "gen=11: tensor([39., 15.]), win_pct=72.22%, sum=54.0\n",
      "gen=12: tensor([176.,  81.]), win_pct=68.48%, sum=257.0\n",
      "gen=13: tensor([36., 71.]), win_pct=33.64%, sum=107.0\n",
      "gen=14: tensor([97., 67.]), win_pct=59.15%, sum=164.0\n",
      "gen=15: tensor([55., 21.]), win_pct=72.37%, sum=76.0\n",
      "gen=16: tensor([2200., 1187.]), win_pct=64.95%, sum=3387.0\n",
      "gen=17: tensor([1015.,  472.]), win_pct=68.26%, sum=1487.0\n",
      "gen=18: tensor([739., 707.]), win_pct=51.11%, sum=1446.0\n",
      "gen=19: tensor([205., 312.]), win_pct=39.65%, sum=517.0\n",
      "gen=20: tensor([ 94., 126.]), win_pct=42.73%, sum=220.0\n",
      "player_probs=[0.38601965 0.6139803 ]\n",
      "\n",
      "prefix=(0, 6, 6)\n",
      "gen=*: tensor([5692., 2272.]), win_pct=71.47%, sum=7964.0\n",
      "gen=1: tensor([119.,  76.]), win_pct=61.03%, sum=195.0\n",
      "gen=2: tensor([1., 4.]), win_pct=20.00%, sum=5.0\n",
      "gen=4: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=5: tensor([21., 22.]), win_pct=48.84%, sum=43.0\n",
      "gen=6: tensor([1593., 1107.]), win_pct=59.00%, sum=2700.0\n",
      "gen=7: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=8: tensor([63., 58.]), win_pct=52.07%, sum=121.0\n",
      "gen=9: tensor([225., 323.]), win_pct=41.06%, sum=548.0\n",
      "gen=10: tensor([37., 32.]), win_pct=53.62%, sum=69.0\n",
      "gen=11: tensor([23.,  0.]), win_pct=100.00%, sum=23.0\n",
      "gen=12: tensor([301.,  30.]), win_pct=90.94%, sum=331.0\n",
      "gen=13: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=14: tensor([937., 314.]), win_pct=74.90%, sum=1251.0\n",
      "gen=15: tensor([53.,  6.]), win_pct=89.83%, sum=59.0\n",
      "gen=16: tensor([264.,  35.]), win_pct=88.29%, sum=299.0\n",
      "gen=17: tensor([1825.,  218.]), win_pct=89.33%, sum=2043.0\n",
      "gen=18: tensor([141.,  13.]), win_pct=91.56%, sum=154.0\n",
      "gen=19: tensor([69., 21.]), win_pct=76.67%, sum=90.0\n",
      "gen=20: tensor([11.,  7.]), win_pct=61.11%, sum=18.0\n",
      "player_probs=[0.46239752 0.5376025 ]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([3282.5000, 5879.5000]), win_pct=35.83%, sum=9162.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "gen=6: tensor([79., 65.]), win_pct=54.86%, sum=144.0\n",
      "gen=7: tensor([160.,  79.]), win_pct=66.95%, sum=239.0\n",
      "gen=8: tensor([ 710., 1756.]), win_pct=28.79%, sum=2466.0\n",
      "gen=9: tensor([118., 313.]), win_pct=27.38%, sum=431.0\n",
      "gen=10: tensor([140., 576.]), win_pct=19.55%, sum=716.0\n",
      "gen=11: tensor([28., 61.]), win_pct=31.46%, sum=89.0\n",
      "gen=12: tensor([45., 44.]), win_pct=50.56%, sum=89.0\n",
      "gen=13: tensor([10., 35.]), win_pct=22.22%, sum=45.0\n",
      "gen=14: tensor([ 79., 220.]), win_pct=26.42%, sum=299.0\n",
      "gen=15: tensor([84., 60.]), win_pct=58.33%, sum=144.0\n",
      "gen=16: tensor([55., 89.]), win_pct=38.19%, sum=144.0\n",
      "gen=17: tensor([53., 91.]), win_pct=36.81%, sum=144.0\n",
      "gen=18: tensor([ 57., 139.]), win_pct=29.08%, sum=196.0\n",
      "gen=19: tensor([ 42., 102.]), win_pct=29.17%, sum=144.0\n",
      "gen=20: tensor([ 66., 233.]), win_pct=22.07%, sum=299.0\n",
      "player_probs=[0.44871598 0.551284  ]\n"
     ]
    }
   ],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):    \n",
    "    list(dd.items())[10][1]['*'].sum() > 100\n",
    "    top_k = sorted(dd.items(), key=lambda kv: kv[1]['*'].sum(), reverse=True)[:20]\n",
    "    top_k_keys = sorted(k for k, v in top_k)\n",
    "    \n",
    "    prefix_list = top_k_keys\n",
    "\n",
    "    # prefix_list = [\n",
    "    #     (0,), \n",
    "    #     (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "    #     (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "    #     (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7),\n",
    "    # ]\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Model 0\n",
      "Parameter containing:\n",
      "tensor([[-0.0504,  0.0093, -0.0226,  0.0165,  0.0072, -0.0277,  0.0177, -0.0237,\n",
      "          0.0188, -0.0143,  0.0269, -0.0013, -0.0198,  0.0144, -0.0243,  0.0237,\n",
      "         -0.0140,  0.0181,  0.0318, -0.0157,  0.0148, -0.0034, -0.0324,  0.0208,\n",
      "         -0.0291, -0.0305,  0.0342, -0.0172,  0.0260, -0.0208, -0.0124,  0.0096],\n",
      "        [-0.0126,  0.0072,  0.0104, -0.0769, -0.0078, -0.0041, -0.0216,  0.0517,\n",
      "          0.0385, -0.0091, -0.0166,  0.0048,  0.0219, -0.0037, -0.0307,  0.0049,\n",
      "         -0.0108,  0.0062,  0.0002, -0.0093, -0.0116, -0.0281, -0.0046, -0.0189,\n",
      "         -0.0464, -0.0034, -0.0101, -0.0152, -0.0383,  0.0229, -0.0058, -0.0105],\n",
      "        [-0.0208, -0.0252,  0.0038, -0.0199, -0.0087, -0.0136, -0.0072,  0.0004,\n",
      "         -0.0083,  0.0022, -0.0049,  0.0290,  0.0091, -0.0186, -0.0067,  0.0133,\n",
      "         -0.0194, -0.0004,  0.0097,  0.0346, -0.0028, -0.0301,  0.0015,  0.0175,\n",
      "         -0.0117,  0.0087,  0.0185, -0.0033,  0.0177,  0.0148,  0.0138,  0.0008],\n",
      "        [ 0.0108,  0.0246,  0.0061,  0.0229,  0.0009, -0.0005, -0.0093,  0.0311,\n",
      "          0.0120,  0.0404, -0.0377, -0.0150,  0.0187, -0.0209,  0.0130, -0.0071,\n",
      "         -0.0059, -0.0070,  0.0242,  0.0173,  0.0340,  0.0052, -0.0227,  0.0163,\n",
      "          0.0290, -0.0299,  0.0063,  0.0577, -0.0285,  0.0299, -0.0042,  0.0182],\n",
      "        [ 0.0079,  0.0110,  0.0027, -0.0343, -0.0018, -0.0030,  0.0095, -0.0341,\n",
      "         -0.0042, -0.0264,  0.0266,  0.0052,  0.0007, -0.0033, -0.0015, -0.0278,\n",
      "          0.0115, -0.0010,  0.0064, -0.0044, -0.0115,  0.0057, -0.0438, -0.0170,\n",
      "          0.0007,  0.0555, -0.0064, -0.0045,  0.0276, -0.0094, -0.0307, -0.0514],\n",
      "        [-0.0523, -0.0152, -0.0220,  0.0133, -0.0118,  0.0202,  0.0215, -0.0226,\n",
      "         -0.0149, -0.0153, -0.0252, -0.0042, -0.0080,  0.0028, -0.0132, -0.0119,\n",
      "         -0.0156, -0.0223,  0.0076,  0.0080, -0.0226, -0.0042, -0.0134, -0.0196,\n",
      "         -0.0076,  0.0077, -0.0115,  0.0192, -0.0142, -0.0291, -0.0335, -0.0355],\n",
      "        [ 0.0327, -0.0035, -0.0089, -0.0022,  0.0108, -0.0225,  0.0273,  0.0042,\n",
      "          0.0252, -0.0067,  0.0078, -0.0050, -0.0320,  0.0053, -0.0220,  0.0217,\n",
      "         -0.0161, -0.0067,  0.0006, -0.0307, -0.0253, -0.0320, -0.0005,  0.0118,\n",
      "          0.0088, -0.0154, -0.0236,  0.0061, -0.0021, -0.0011, -0.0321,  0.0027],\n",
      "        [-0.0200,  0.0198,  0.0210, -0.0304, -0.0015, -0.0245,  0.0166,  0.0256,\n",
      "         -0.0027,  0.0169, -0.0032, -0.0262,  0.0021,  0.0221,  0.0110, -0.0057,\n",
      "          0.0008, -0.0422,  0.0184, -0.0111,  0.0309,  0.0086, -0.0171,  0.0331,\n",
      "         -0.0144, -0.0149,  0.0199, -0.0022,  0.0037, -0.0067, -0.0106, -0.0266]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([126100.5000,  73899.5000]), win_pct=63.05%, sum=200000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "gen=6: tensor([6709., 3291.]), win_pct=67.09%, sum=10000.0\n",
      "gen=7: tensor([6454.5000, 3545.5000]), win_pct=64.54%, sum=10000.0\n",
      "gen=8: tensor([5073., 4927.]), win_pct=50.73%, sum=10000.0\n",
      "gen=9: tensor([5754.5000, 4245.5000]), win_pct=57.54%, sum=10000.0\n",
      "gen=10: tensor([4074., 5926.]), win_pct=40.74%, sum=10000.0\n",
      "gen=11: tensor([8811., 1189.]), win_pct=88.11%, sum=10000.0\n",
      "gen=12: tensor([8075., 1925.]), win_pct=80.75%, sum=10000.0\n",
      "gen=13: tensor([7093.5000, 2906.5000]), win_pct=70.93%, sum=10000.0\n",
      "gen=14: tensor([5778.5000, 4221.5000]), win_pct=57.78%, sum=10000.0\n",
      "gen=15: tensor([8239., 1761.]), win_pct=82.39%, sum=10000.0\n",
      "gen=16: tensor([6879., 3121.]), win_pct=68.79%, sum=10000.0\n",
      "gen=17: tensor([7835., 2165.]), win_pct=78.35%, sum=10000.0\n",
      "gen=18: tensor([6722.5000, 3277.5000]), win_pct=67.22%, sum=10000.0\n",
      "gen=19: tensor([6481., 3519.]), win_pct=64.81%, sum=10000.0\n",
      "gen=20: tensor([5671., 4329.]), win_pct=56.71%, sum=10000.0\n",
      "player_probs=[0.55144787 0.4485522 ]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([ 8218., 10030.]), win_pct=45.04%, sum=18248.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "gen=6: tensor([ 90., 147.]), win_pct=37.97%, sum=237.0\n",
      "gen=7: tensor([324., 473.]), win_pct=40.65%, sum=797.0\n",
      "gen=8: tensor([1179., 1590.]), win_pct=42.58%, sum=2769.0\n",
      "gen=9: tensor([121., 334.]), win_pct=26.59%, sum=455.0\n",
      "gen=10: tensor([348., 421.]), win_pct=45.25%, sum=769.0\n",
      "gen=11: tensor([105., 107.]), win_pct=49.53%, sum=212.0\n",
      "gen=12: tensor([125.,  87.]), win_pct=58.96%, sum=212.0\n",
      "gen=13: tensor([ 60., 103.]), win_pct=36.81%, sum=163.0\n",
      "gen=14: tensor([144., 179.]), win_pct=44.58%, sum=323.0\n",
      "gen=15: tensor([219., 109.]), win_pct=66.77%, sum=328.0\n",
      "gen=16: tensor([143.,  97.]), win_pct=59.58%, sum=240.0\n",
      "gen=17: tensor([137., 103.]), win_pct=57.08%, sum=240.0\n",
      "gen=18: tensor([16., 29.]), win_pct=35.56%, sum=45.0\n",
      "gen=19: tensor([311., 419.]), win_pct=42.60%, sum=730.0\n",
      "gen=20: tensor([ 95., 161.]), win_pct=37.11%, sum=256.0\n",
      "player_probs=[0.5518051 0.4481949]\n",
      "\n",
      "prefix=(0, 2, 3)\n",
      "gen=*: tensor([3163., 4777.]), win_pct=39.84%, sum=7940.0\n",
      "gen=1: tensor([75., 90.]), win_pct=45.45%, sum=165.0\n",
      "gen=2: tensor([ 6., 30.]), win_pct=16.67%, sum=36.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([2022., 3652.]), win_pct=35.64%, sum=5674.0\n",
      "gen=5: tensor([469., 398.]), win_pct=54.09%, sum=867.0\n",
      "gen=6: tensor([7., 2.]), win_pct=77.78%, sum=9.0\n",
      "gen=7: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=8: tensor([ 91., 108.]), win_pct=45.73%, sum=199.0\n",
      "gen=9: tensor([25., 12.]), win_pct=67.57%, sum=37.0\n",
      "gen=10: tensor([284., 379.]), win_pct=42.84%, sum=663.0\n",
      "gen=11: tensor([4., 2.]), win_pct=66.67%, sum=6.0\n",
      "gen=12: tensor([19., 11.]), win_pct=63.33%, sum=30.0\n",
      "gen=14: tensor([29., 23.]), win_pct=55.77%, sum=52.0\n",
      "gen=15: tensor([45., 23.]), win_pct=66.18%, sum=68.0\n",
      "gen=16: tensor([13.,  2.]), win_pct=86.67%, sum=15.0\n",
      "gen=17: tensor([19., 13.]), win_pct=59.38%, sum=32.0\n",
      "gen=18: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=19: tensor([18.,  7.]), win_pct=72.00%, sum=25.0\n",
      "gen=20: tensor([14., 11.]), win_pct=56.00%, sum=25.0\n",
      "player_probs=[0.55152726 0.4484728 ]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([9949., 7572.]), win_pct=56.78%, sum=17521.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "gen=6: tensor([1722.,  460.]), win_pct=78.92%, sum=2182.0\n",
      "gen=7: tensor([143., 115.]), win_pct=55.43%, sum=258.0\n",
      "gen=8: tensor([192., 187.]), win_pct=50.66%, sum=379.0\n",
      "gen=9: tensor([ 744., 1086.]), win_pct=40.66%, sum=1830.0\n",
      "gen=10: tensor([325., 675.]), win_pct=32.50%, sum=1000.0\n",
      "gen=11: tensor([483., 248.]), win_pct=66.07%, sum=731.0\n",
      "gen=12: tensor([878., 335.]), win_pct=72.38%, sum=1213.0\n",
      "gen=13: tensor([ 49.5000, 126.5000]), win_pct=28.12%, sum=176.0\n",
      "gen=14: tensor([306., 493.]), win_pct=38.30%, sum=799.0\n",
      "gen=15: tensor([639., 200.]), win_pct=76.16%, sum=839.0\n",
      "gen=16: tensor([317., 181.]), win_pct=63.65%, sum=498.0\n",
      "gen=17: tensor([273., 115.]), win_pct=70.36%, sum=388.0\n",
      "gen=18: tensor([727.5000, 236.5000]), win_pct=75.47%, sum=964.0\n",
      "gen=19: tensor([511., 257.]), win_pct=66.54%, sum=768.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.5510513 0.4489487]\n",
      "\n",
      "prefix=(0, 3, 4)\n",
      "gen=*: tensor([4546., 6046.]), win_pct=42.92%, sum=10592.0\n",
      "gen=1: tensor([ 99., 103.]), win_pct=49.01%, sum=202.0\n",
      "gen=2: tensor([458., 821.]), win_pct=35.81%, sum=1279.0\n",
      "gen=3: tensor([19., 79.]), win_pct=19.39%, sum=98.0\n",
      "gen=4: tensor([116., 168.]), win_pct=40.85%, sum=284.0\n",
      "gen=5: tensor([656., 746.]), win_pct=46.79%, sum=1402.0\n",
      "gen=6: tensor([18., 14.]), win_pct=56.25%, sum=32.0\n",
      "gen=7: tensor([128., 107.]), win_pct=54.47%, sum=235.0\n",
      "gen=8: tensor([131., 160.]), win_pct=45.02%, sum=291.0\n",
      "gen=9: tensor([ 565., 1030.]), win_pct=35.42%, sum=1595.0\n",
      "gen=10: tensor([225., 637.]), win_pct=26.10%, sum=862.0\n",
      "gen=11: tensor([408., 230.]), win_pct=63.95%, sum=638.0\n",
      "gen=12: tensor([261., 206.]), win_pct=55.89%, sum=467.0\n",
      "gen=13: tensor([ 38.5000, 124.5000]), win_pct=23.62%, sum=163.0\n",
      "gen=14: tensor([228., 467.]), win_pct=32.81%, sum=695.0\n",
      "gen=15: tensor([365., 165.]), win_pct=68.87%, sum=530.0\n",
      "gen=16: tensor([216., 171.]), win_pct=55.81%, sum=387.0\n",
      "gen=17: tensor([135.,  92.]), win_pct=59.47%, sum=227.0\n",
      "gen=18: tensor([136.5000, 143.5000]), win_pct=48.75%, sum=280.0\n",
      "gen=19: tensor([205., 173.]), win_pct=54.23%, sum=378.0\n",
      "gen=20: tensor([138., 409.]), win_pct=25.23%, sum=547.0\n",
      "player_probs=[0.55146104 0.44853896]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([63413.5000, 22736.5000]), win_pct=73.61%, sum=86150.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "gen=6: tensor([2079.,  580.]), win_pct=78.19%, sum=2659.0\n",
      "gen=7: tensor([5220.5000, 2188.5000]), win_pct=70.46%, sum=7409.0\n",
      "gen=8: tensor([1753.,  616.]), win_pct=74.00%, sum=2369.0\n",
      "gen=9: tensor([3652., 1175.]), win_pct=75.66%, sum=4827.0\n",
      "gen=10: tensor([1090.,  359.]), win_pct=75.22%, sum=1449.0\n",
      "gen=11: tensor([7770.,  526.]), win_pct=93.66%, sum=8296.0\n",
      "gen=12: tensor([2906.,  457.]), win_pct=86.41%, sum=3363.0\n",
      "gen=13: tensor([6824., 2266.]), win_pct=75.07%, sum=9090.0\n",
      "gen=14: tensor([498., 158.]), win_pct=75.91%, sum=656.0\n",
      "gen=15: tensor([6314.,  993.]), win_pct=86.41%, sum=7307.0\n",
      "gen=16: tensor([724., 155.]), win_pct=82.37%, sum=879.0\n",
      "gen=17: tensor([508.,  86.]), win_pct=85.52%, sum=594.0\n",
      "gen=18: tensor([2039.,  509.]), win_pct=80.02%, sum=2548.0\n",
      "gen=19: tensor([4367., 1618.]), win_pct=72.97%, sum=5985.0\n",
      "gen=20: tensor([4857., 2506.]), win_pct=65.96%, sum=7363.0\n",
      "player_probs=[0.5517535 0.4482465]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([5812., 1448.]), win_pct=80.06%, sum=7260.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "gen=6: tensor([538., 166.]), win_pct=76.42%, sum=704.0\n",
      "gen=7: tensor([79., 24.]), win_pct=76.70%, sum=103.0\n",
      "gen=8: tensor([35., 11.]), win_pct=76.09%, sum=46.0\n",
      "gen=9: tensor([315.,  65.]), win_pct=82.89%, sum=380.0\n",
      "gen=10: tensor([33., 16.]), win_pct=67.35%, sum=49.0\n",
      "gen=11: tensor([246.,  41.]), win_pct=85.71%, sum=287.0\n",
      "gen=12: tensor([967., 228.]), win_pct=80.92%, sum=1195.0\n",
      "gen=13: tensor([81., 15.]), win_pct=84.38%, sum=96.0\n",
      "gen=14: tensor([11.,  4.]), win_pct=73.33%, sum=15.0\n",
      "gen=15: tensor([1334.,  233.]), win_pct=85.13%, sum=1567.0\n",
      "gen=16: tensor([33.,  4.]), win_pct=89.19%, sum=37.0\n",
      "gen=17: tensor([56.,  7.]), win_pct=88.89%, sum=63.0\n",
      "gen=18: tensor([1115.,  245.]), win_pct=81.99%, sum=1360.0\n",
      "gen=19: tensor([143.,  37.]), win_pct=79.44%, sum=180.0\n",
      "gen=20: tensor([297., 129.]), win_pct=69.72%, sum=426.0\n",
      "player_probs=[0.5514836  0.44851646]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([41327.5000, 17642.5000]), win_pct=70.08%, sum=58970.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "gen=6: tensor([1297.,  349.]), win_pct=78.80%, sum=1646.0\n",
      "gen=7: tensor([3362.5000, 1705.5000]), win_pct=66.35%, sum=5068.0\n",
      "gen=8: tensor([680., 218.]), win_pct=75.72%, sum=898.0\n",
      "gen=9: tensor([2494.,  935.]), win_pct=72.73%, sum=3429.0\n",
      "gen=10: tensor([967., 317.]), win_pct=75.31%, sum=1284.0\n",
      "gen=11: tensor([6879.,  427.]), win_pct=94.16%, sum=7306.0\n",
      "gen=12: tensor([603.,  47.]), win_pct=92.77%, sum=650.0\n",
      "gen=13: tensor([4557., 1850.]), win_pct=71.13%, sum=6407.0\n",
      "gen=14: tensor([424., 146.]), win_pct=74.39%, sum=570.0\n",
      "gen=15: tensor([1828.,  290.]), win_pct=86.31%, sum=2118.0\n",
      "gen=16: tensor([489., 122.]), win_pct=80.03%, sum=611.0\n",
      "gen=17: tensor([324.,  61.]), win_pct=84.16%, sum=385.0\n",
      "gen=18: tensor([564., 197.]), win_pct=74.11%, sum=761.0\n",
      "gen=19: tensor([3401., 1391.]), win_pct=70.97%, sum=4792.0\n",
      "gen=20: tensor([3769., 2221.]), win_pct=62.92%, sum=5990.0\n",
      "player_probs=[0.5521419 0.4478581]\n",
      "\n",
      "prefix=(0, 4, 4, 3)\n",
      "gen=*: tensor([8937., 2897.]), win_pct=75.52%, sum=11834.0\n",
      "gen=1: tensor([36.,  3.]), win_pct=92.31%, sum=39.0\n",
      "gen=2: tensor([438., 159.]), win_pct=73.37%, sum=597.0\n",
      "gen=3: tensor([725., 351.]), win_pct=67.38%, sum=1076.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([1974.,  893.]), win_pct=68.85%, sum=2867.0\n",
      "gen=6: tensor([810.,  58.]), win_pct=93.32%, sum=868.0\n",
      "gen=7: tensor([690., 621.]), win_pct=52.63%, sum=1311.0\n",
      "gen=8: tensor([328.,  74.]), win_pct=81.59%, sum=402.0\n",
      "gen=9: tensor([236.,  31.]), win_pct=88.39%, sum=267.0\n",
      "gen=10: tensor([614.,  70.]), win_pct=89.77%, sum=684.0\n",
      "gen=11: tensor([387.,  13.]), win_pct=96.75%, sum=400.0\n",
      "gen=12: tensor([36.,  1.]), win_pct=97.30%, sum=37.0\n",
      "gen=13: tensor([229.,  22.]), win_pct=91.24%, sum=251.0\n",
      "gen=14: tensor([70., 35.]), win_pct=66.67%, sum=105.0\n",
      "gen=15: tensor([231.,  14.]), win_pct=94.29%, sum=245.0\n",
      "gen=16: tensor([67., 12.]), win_pct=84.81%, sum=79.0\n",
      "gen=17: tensor([51.,  7.]), win_pct=87.93%, sum=58.0\n",
      "gen=18: tensor([38., 14.]), win_pct=73.08%, sum=52.0\n",
      "gen=19: tensor([1776.,  410.]), win_pct=81.24%, sum=2186.0\n",
      "gen=20: tensor([199., 109.]), win_pct=64.61%, sum=308.0\n",
      "player_probs=[0.551904 0.448096]\n",
      "\n",
      "prefix=(0, 4, 4, 4)\n",
      "gen=*: tensor([14185.,  6380.]), win_pct=68.98%, sum=20565.0\n",
      "gen=1: tensor([30., 12.]), win_pct=71.43%, sum=42.0\n",
      "gen=2: tensor([728., 660.]), win_pct=52.45%, sum=1388.0\n",
      "gen=3: tensor([780., 636.]), win_pct=55.08%, sum=1416.0\n",
      "gen=4: tensor([29., 14.]), win_pct=67.44%, sum=43.0\n",
      "gen=5: tensor([442., 235.]), win_pct=65.29%, sum=677.0\n",
      "gen=6: tensor([123., 184.]), win_pct=40.07%, sum=307.0\n",
      "gen=7: tensor([792., 386.]), win_pct=67.23%, sum=1178.0\n",
      "gen=8: tensor([214.,  64.]), win_pct=76.98%, sum=278.0\n",
      "gen=9: tensor([630., 146.]), win_pct=81.19%, sum=776.0\n",
      "gen=10: tensor([55., 45.]), win_pct=55.00%, sum=100.0\n",
      "gen=11: tensor([956., 126.]), win_pct=88.35%, sum=1082.0\n",
      "gen=12: tensor([208.,  25.]), win_pct=89.27%, sum=233.0\n",
      "gen=13: tensor([3412., 1295.]), win_pct=72.49%, sum=4707.0\n",
      "gen=14: tensor([119.,  68.]), win_pct=63.64%, sum=187.0\n",
      "gen=15: tensor([1213.,  244.]), win_pct=83.25%, sum=1457.0\n",
      "gen=16: tensor([225.,  56.]), win_pct=80.07%, sum=281.0\n",
      "gen=17: tensor([169.,  31.]), win_pct=84.50%, sum=200.0\n",
      "gen=18: tensor([253.,  98.]), win_pct=72.08%, sum=351.0\n",
      "gen=19: tensor([1032.,  607.]), win_pct=62.97%, sum=1639.0\n",
      "gen=20: tensor([2775., 1448.]), win_pct=65.71%, sum=4223.0\n",
      "player_probs=[0.5523664 0.4476336]\n",
      "\n",
      "prefix=(0, 4, 4, 5)\n",
      "gen=*: tensor([4593., 3993.]), win_pct=53.49%, sum=8586.0\n",
      "gen=1: tensor([22.,  5.]), win_pct=81.48%, sum=27.0\n",
      "gen=2: tensor([255., 182.]), win_pct=58.35%, sum=437.0\n",
      "gen=3: tensor([1793., 2519.]), win_pct=41.58%, sum=4312.0\n",
      "gen=5: tensor([384., 367.]), win_pct=51.13%, sum=751.0\n",
      "gen=6: tensor([144.,  35.]), win_pct=80.45%, sum=179.0\n",
      "gen=7: tensor([472., 219.]), win_pct=68.31%, sum=691.0\n",
      "gen=8: tensor([51., 13.]), win_pct=79.69%, sum=64.0\n",
      "gen=9: tensor([28., 13.]), win_pct=68.29%, sum=41.0\n",
      "gen=10: tensor([132.,  49.]), win_pct=72.93%, sum=181.0\n",
      "gen=11: tensor([128.,  40.]), win_pct=76.19%, sum=168.0\n",
      "gen=12: tensor([111.,  14.]), win_pct=88.80%, sum=125.0\n",
      "gen=13: tensor([412., 232.]), win_pct=63.98%, sum=644.0\n",
      "gen=14: tensor([25.,  4.]), win_pct=86.21%, sum=29.0\n",
      "gen=15: tensor([50.,  9.]), win_pct=84.75%, sum=59.0\n",
      "gen=16: tensor([51., 14.]), win_pct=78.46%, sum=65.0\n",
      "gen=17: tensor([13.,  6.]), win_pct=68.42%, sum=19.0\n",
      "gen=18: tensor([54., 22.]), win_pct=71.05%, sum=76.0\n",
      "gen=19: tensor([281., 152.]), win_pct=64.90%, sum=433.0\n",
      "gen=20: tensor([187.,  98.]), win_pct=65.61%, sum=285.0\n",
      "player_probs=[0.5523007  0.44769928]\n",
      "\n",
      "prefix=(0, 4, 4, 6)\n",
      "gen=*: tensor([9904., 1662.]), win_pct=85.63%, sum=11566.0\n",
      "gen=1: tensor([24.,  7.]), win_pct=77.42%, sum=31.0\n",
      "gen=2: tensor([122.,  86.]), win_pct=58.65%, sum=208.0\n",
      "gen=3: tensor([149.,  58.]), win_pct=71.98%, sum=207.0\n",
      "gen=5: tensor([19.,  2.]), win_pct=90.48%, sum=21.0\n",
      "gen=6: tensor([43., 11.]), win_pct=79.63%, sum=54.0\n",
      "gen=7: tensor([1033.,  150.]), win_pct=87.32%, sum=1183.0\n",
      "gen=8: tensor([66., 24.]), win_pct=73.33%, sum=90.0\n",
      "gen=9: tensor([1419.,  643.]), win_pct=68.82%, sum=2062.0\n",
      "gen=10: tensor([70., 23.]), win_pct=75.27%, sum=93.0\n",
      "gen=11: tensor([5241.,  172.]), win_pct=96.82%, sum=5413.0\n",
      "gen=12: tensor([203.,   2.]), win_pct=99.02%, sum=205.0\n",
      "gen=13: tensor([397., 211.]), win_pct=65.30%, sum=608.0\n",
      "gen=14: tensor([190.,  16.]), win_pct=92.23%, sum=206.0\n",
      "gen=15: tensor([282.,   4.]), win_pct=98.60%, sum=286.0\n",
      "gen=16: tensor([93., 13.]), win_pct=87.74%, sum=106.0\n",
      "gen=17: tensor([58.,  4.]), win_pct=93.55%, sum=62.0\n",
      "gen=18: tensor([197.,  47.]), win_pct=80.74%, sum=244.0\n",
      "gen=19: tensor([144.,  71.]), win_pct=66.98%, sum=215.0\n",
      "gen=20: tensor([154., 118.]), win_pct=56.62%, sum=272.0\n",
      "player_probs=[0.5522456  0.44775435]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([13578.,  9890.]), win_pct=57.86%, sum=23468.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "gen=6: tensor([584., 527.]), win_pct=52.57%, sum=1111.0\n",
      "gen=7: tensor([385., 369.]), win_pct=51.06%, sum=754.0\n",
      "gen=8: tensor([939., 524.]), win_pct=64.18%, sum=1463.0\n",
      "gen=9: tensor([282., 492.]), win_pct=36.43%, sum=774.0\n",
      "gen=10: tensor([1079., 1630.]), win_pct=39.83%, sum=2709.0\n",
      "gen=11: tensor([105.,  86.]), win_pct=54.97%, sum=191.0\n",
      "gen=12: tensor([2459.,  540.]), win_pct=81.99%, sum=2999.0\n",
      "gen=13: tensor([ 91., 244.]), win_pct=27.16%, sum=335.0\n",
      "gen=14: tensor([1202.5000, 1005.5000]), win_pct=54.46%, sum=2208.0\n",
      "gen=15: tensor([563., 286.]), win_pct=66.31%, sum=849.0\n",
      "gen=16: tensor([588., 192.]), win_pct=75.38%, sum=780.0\n",
      "gen=17: tensor([567., 157.]), win_pct=78.31%, sum=724.0\n",
      "gen=18: tensor([1694.,  931.]), win_pct=64.53%, sum=2625.0\n",
      "gen=19: tensor([571., 367.]), win_pct=60.87%, sum=938.0\n",
      "gen=20: tensor([218., 442.]), win_pct=33.03%, sum=660.0\n",
      "player_probs=[0.55149066 0.44850934]\n",
      "\n",
      "prefix=(0, 5, 4)\n",
      "gen=*: tensor([6657., 7845.]), win_pct=45.90%, sum=14502.0\n",
      "gen=1: tensor([102., 113.]), win_pct=47.44%, sum=215.0\n",
      "gen=2: tensor([450., 802.]), win_pct=35.94%, sum=1252.0\n",
      "gen=3: tensor([62.5000, 64.5000]), win_pct=49.21%, sum=127.0\n",
      "gen=4: tensor([124., 116.]), win_pct=51.67%, sum=240.0\n",
      "gen=5: tensor([334., 486.]), win_pct=40.73%, sum=820.0\n",
      "gen=6: tensor([203., 396.]), win_pct=33.89%, sum=599.0\n",
      "gen=7: tensor([314., 357.]), win_pct=46.80%, sum=671.0\n",
      "gen=8: tensor([101., 119.]), win_pct=45.91%, sum=220.0\n",
      "gen=9: tensor([212., 455.]), win_pct=31.78%, sum=667.0\n",
      "gen=10: tensor([ 832., 1528.]), win_pct=35.25%, sum=2360.0\n",
      "gen=11: tensor([84., 85.]), win_pct=49.70%, sum=169.0\n",
      "gen=12: tensor([499., 148.]), win_pct=77.13%, sum=647.0\n",
      "gen=13: tensor([ 73., 240.]), win_pct=23.32%, sum=313.0\n",
      "gen=14: tensor([933.5000, 974.5000]), win_pct=48.93%, sum=1908.0\n",
      "gen=15: tensor([427., 263.]), win_pct=61.88%, sum=690.0\n",
      "gen=16: tensor([328., 154.]), win_pct=68.05%, sum=482.0\n",
      "gen=17: tensor([91., 39.]), win_pct=70.00%, sum=130.0\n",
      "gen=18: tensor([882., 760.]), win_pct=53.71%, sum=1642.0\n",
      "gen=19: tensor([428., 321.]), win_pct=57.14%, sum=749.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.5518846  0.44811547]\n",
      "\n",
      "prefix=(0, 5, 4, 4)\n",
      "gen=*: tensor([4227.5000, 3571.5000]), win_pct=54.21%, sum=7799.0\n",
      "gen=1: tensor([16., 12.]), win_pct=57.14%, sum=28.0\n",
      "gen=2: tensor([347., 560.]), win_pct=38.26%, sum=907.0\n",
      "gen=3: tensor([3., 4.]), win_pct=42.86%, sum=7.0\n",
      "gen=4: tensor([3., 6.]), win_pct=33.33%, sum=9.0\n",
      "gen=5: tensor([262., 347.]), win_pct=43.02%, sum=609.0\n",
      "gen=6: tensor([ 78., 153.]), win_pct=33.77%, sum=231.0\n",
      "gen=7: tensor([260., 188.]), win_pct=58.04%, sum=448.0\n",
      "gen=8: tensor([94., 95.]), win_pct=49.74%, sum=189.0\n",
      "gen=9: tensor([16.,  7.]), win_pct=69.57%, sum=23.0\n",
      "gen=10: tensor([221., 173.]), win_pct=56.09%, sum=394.0\n",
      "gen=11: tensor([73., 60.]), win_pct=54.89%, sum=133.0\n",
      "gen=12: tensor([361.,  95.]), win_pct=79.17%, sum=456.0\n",
      "gen=13: tensor([ 63., 207.]), win_pct=23.33%, sum=270.0\n",
      "gen=14: tensor([679.5000, 520.5000]), win_pct=56.62%, sum=1200.0\n",
      "gen=15: tensor([302., 142.]), win_pct=68.02%, sum=444.0\n",
      "gen=16: tensor([260.,  96.]), win_pct=73.03%, sum=356.0\n",
      "gen=17: tensor([69., 23.]), win_pct=75.00%, sum=92.0\n",
      "gen=18: tensor([766., 559.]), win_pct=57.81%, sum=1325.0\n",
      "gen=19: tensor([266., 161.]), win_pct=62.30%, sum=427.0\n",
      "gen=20: tensor([ 88., 163.]), win_pct=35.06%, sum=251.0\n",
      "player_probs=[0.55213076 0.4478692 ]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([25180.5000, 13503.5000]), win_pct=65.09%, sum=38684.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "gen=6: tensor([1994., 1294.]), win_pct=60.64%, sum=3288.0\n",
      "gen=7: tensor([131., 106.]), win_pct=55.27%, sum=237.0\n",
      "gen=8: tensor([238., 177.]), win_pct=57.35%, sum=415.0\n",
      "gen=9: tensor([731.5000, 529.5000]), win_pct=58.01%, sum=1261.0\n",
      "gen=10: tensor([ 863., 1507.]), win_pct=36.41%, sum=2370.0\n",
      "gen=11: tensor([284., 103.]), win_pct=73.39%, sum=387.0\n",
      "gen=12: tensor([1628.,  402.]), win_pct=80.20%, sum=2030.0\n",
      "gen=13: tensor([53., 98.]), win_pct=35.10%, sum=151.0\n",
      "gen=14: tensor([3508., 2068.]), win_pct=62.91%, sum=5576.0\n",
      "gen=15: tensor([370.,  69.]), win_pct=84.28%, sum=439.0\n",
      "gen=16: tensor([5000., 2320.]), win_pct=68.31%, sum=7320.0\n",
      "gen=17: tensor([6265., 1506.]), win_pct=80.62%, sum=7771.0\n",
      "gen=18: tensor([2154., 1374.]), win_pct=61.05%, sum=3528.0\n",
      "gen=19: tensor([612., 620.]), win_pct=49.68%, sum=1232.0\n",
      "gen=20: tensor([181., 334.]), win_pct=35.15%, sum=515.0\n",
      "player_probs=[0.551355 0.448645]\n",
      "\n",
      "prefix=(0, 6, 4)\n",
      "gen=*: tensor([9921.5000, 8800.5000]), win_pct=52.99%, sum=18722.0\n",
      "gen=1: tensor([ 94., 117.]), win_pct=44.55%, sum=211.0\n",
      "gen=2: tensor([ 69., 225.]), win_pct=23.47%, sum=294.0\n",
      "gen=3: tensor([45., 89.]), win_pct=33.58%, sum=134.0\n",
      "gen=4: tensor([24., 46.]), win_pct=34.29%, sum=70.0\n",
      "gen=5: tensor([89., 39.]), win_pct=69.53%, sum=128.0\n",
      "gen=6: tensor([74., 71.]), win_pct=51.03%, sum=145.0\n",
      "gen=7: tensor([109.,  98.]), win_pct=52.66%, sum=207.0\n",
      "gen=8: tensor([57., 29.]), win_pct=66.28%, sum=86.0\n",
      "gen=9: tensor([218.5000, 115.5000]), win_pct=65.42%, sum=334.0\n",
      "gen=10: tensor([ 621., 1373.]), win_pct=31.14%, sum=1994.0\n",
      "gen=11: tensor([218.,  96.]), win_pct=69.43%, sum=314.0\n",
      "gen=12: tensor([244., 136.]), win_pct=64.21%, sum=380.0\n",
      "gen=13: tensor([42., 95.]), win_pct=30.66%, sum=137.0\n",
      "gen=14: tensor([1581., 1585.]), win_pct=49.94%, sum=3166.0\n",
      "gen=15: tensor([68., 30.]), win_pct=69.39%, sum=98.0\n",
      "gen=16: tensor([3349., 2052.]), win_pct=62.01%, sum=5401.0\n",
      "gen=17: tensor([1336.,  635.]), win_pct=67.78%, sum=1971.0\n",
      "gen=18: tensor([1114., 1100.]), win_pct=50.32%, sum=2214.0\n",
      "gen=19: tensor([430., 561.]), win_pct=43.39%, sum=991.0\n",
      "gen=20: tensor([139., 308.]), win_pct=31.10%, sum=447.0\n",
      "player_probs=[0.55175406 0.448246  ]\n",
      "\n",
      "prefix=(0, 6, 4, 4)\n",
      "gen=*: tensor([4925., 3458.]), win_pct=58.75%, sum=8383.0\n",
      "gen=1: tensor([17., 19.]), win_pct=47.22%, sum=36.0\n",
      "gen=2: tensor([27., 88.]), win_pct=23.48%, sum=115.0\n",
      "gen=3: tensor([27., 40.]), win_pct=40.30%, sum=67.0\n",
      "gen=4: tensor([1., 1.]), win_pct=50.00%, sum=2.0\n",
      "gen=5: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=6: tensor([54., 46.]), win_pct=54.00%, sum=100.0\n",
      "gen=7: tensor([49., 73.]), win_pct=40.16%, sum=122.0\n",
      "gen=8: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=9: tensor([3., 2.]), win_pct=60.00%, sum=5.0\n",
      "gen=10: tensor([ 85., 124.]), win_pct=40.67%, sum=209.0\n",
      "gen=11: tensor([39., 15.]), win_pct=72.22%, sum=54.0\n",
      "gen=12: tensor([176.,  81.]), win_pct=68.48%, sum=257.0\n",
      "gen=13: tensor([36., 71.]), win_pct=33.64%, sum=107.0\n",
      "gen=14: tensor([97., 67.]), win_pct=59.15%, sum=164.0\n",
      "gen=15: tensor([55., 21.]), win_pct=72.37%, sum=76.0\n",
      "gen=16: tensor([2200., 1187.]), win_pct=64.95%, sum=3387.0\n",
      "gen=17: tensor([1015.,  472.]), win_pct=68.26%, sum=1487.0\n",
      "gen=18: tensor([739., 707.]), win_pct=51.11%, sum=1446.0\n",
      "gen=19: tensor([205., 312.]), win_pct=39.65%, sum=517.0\n",
      "gen=20: tensor([ 94., 126.]), win_pct=42.73%, sum=220.0\n",
      "player_probs=[0.5519967 0.4480033]\n",
      "\n",
      "prefix=(0, 6, 6)\n",
      "gen=*: tensor([5692., 2272.]), win_pct=71.47%, sum=7964.0\n",
      "gen=1: tensor([119.,  76.]), win_pct=61.03%, sum=195.0\n",
      "gen=2: tensor([1., 4.]), win_pct=20.00%, sum=5.0\n",
      "gen=4: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=5: tensor([21., 22.]), win_pct=48.84%, sum=43.0\n",
      "gen=6: tensor([1593., 1107.]), win_pct=59.00%, sum=2700.0\n",
      "gen=7: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=8: tensor([63., 58.]), win_pct=52.07%, sum=121.0\n",
      "gen=9: tensor([225., 323.]), win_pct=41.06%, sum=548.0\n",
      "gen=10: tensor([37., 32.]), win_pct=53.62%, sum=69.0\n",
      "gen=11: tensor([23.,  0.]), win_pct=100.00%, sum=23.0\n",
      "gen=12: tensor([301.,  30.]), win_pct=90.94%, sum=331.0\n",
      "gen=13: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=14: tensor([937., 314.]), win_pct=74.90%, sum=1251.0\n",
      "gen=15: tensor([53.,  6.]), win_pct=89.83%, sum=59.0\n",
      "gen=16: tensor([264.,  35.]), win_pct=88.29%, sum=299.0\n",
      "gen=17: tensor([1825.,  218.]), win_pct=89.33%, sum=2043.0\n",
      "gen=18: tensor([141.,  13.]), win_pct=91.56%, sum=154.0\n",
      "gen=19: tensor([69., 21.]), win_pct=76.67%, sum=90.0\n",
      "gen=20: tensor([11.,  7.]), win_pct=61.11%, sum=18.0\n",
      "player_probs=[0.55137026 0.44862977]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([3282.5000, 5879.5000]), win_pct=35.83%, sum=9162.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "gen=6: tensor([79., 65.]), win_pct=54.86%, sum=144.0\n",
      "gen=7: tensor([160.,  79.]), win_pct=66.95%, sum=239.0\n",
      "gen=8: tensor([ 710., 1756.]), win_pct=28.79%, sum=2466.0\n",
      "gen=9: tensor([118., 313.]), win_pct=27.38%, sum=431.0\n",
      "gen=10: tensor([140., 576.]), win_pct=19.55%, sum=716.0\n",
      "gen=11: tensor([28., 61.]), win_pct=31.46%, sum=89.0\n",
      "gen=12: tensor([45., 44.]), win_pct=50.56%, sum=89.0\n",
      "gen=13: tensor([10., 35.]), win_pct=22.22%, sum=45.0\n",
      "gen=14: tensor([ 79., 220.]), win_pct=26.42%, sum=299.0\n",
      "gen=15: tensor([84., 60.]), win_pct=58.33%, sum=144.0\n",
      "gen=16: tensor([55., 89.]), win_pct=38.19%, sum=144.0\n",
      "gen=17: tensor([53., 91.]), win_pct=36.81%, sum=144.0\n",
      "gen=18: tensor([ 57., 139.]), win_pct=29.08%, sum=196.0\n",
      "gen=19: tensor([ 42., 102.]), win_pct=29.17%, sum=144.0\n",
      "gen=20: tensor([ 66., 233.]), win_pct=22.07%, sum=299.0\n",
      "player_probs=[0.5521995  0.44780052]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Model 1\n",
      "Parameter containing:\n",
      "tensor([[ 0.1855, -0.1236,  0.0325, -0.2279, -0.1312,  0.2044, -0.0552,  0.0015,\n",
      "          0.0456, -0.0645, -0.2379,  0.0170,  0.0728,  0.1388, -0.0804,  0.0047,\n",
      "          0.2121, -0.1895,  0.0768,  0.0613,  0.1133, -0.3043,  0.0052, -0.1532,\n",
      "          0.2247,  0.2399,  0.0458,  0.2320,  0.2496, -0.0537, -0.2677, -0.1787],\n",
      "        [ 0.0808, -0.2869,  0.0688, -0.1646,  0.3144,  0.0958,  0.0340, -0.1392,\n",
      "         -0.2065,  0.4270,  0.2191, -0.1682,  0.0057, -0.0339, -0.0352,  0.3426,\n",
      "          0.0628,  0.1943, -0.1348, -0.0467, -0.1249, -0.0522,  0.3732,  0.0999,\n",
      "         -0.0559, -0.1346, -0.1585, -0.2288, -0.0584, -0.0580,  0.0284, -0.2018],\n",
      "        [ 0.3760, -0.0341, -0.0524, -0.1812, -0.2408,  0.1764, -0.0913, -0.1694,\n",
      "         -0.2916, -0.1422, -0.1412, -0.0781,  0.1165,  0.0633,  0.1347, -0.4096,\n",
      "          0.2124, -0.2153, -0.0055, -0.1609, -0.2353,  0.0733, -0.1883, -0.4946,\n",
      "          0.1866,  0.2968,  0.3776,  0.3554,  0.0164, -0.1493, -0.3089, -0.0679],\n",
      "        [ 0.0016, -0.0088, -0.1069,  0.1094, -0.0922,  0.3032,  0.2390,  0.1223,\n",
      "         -0.1586, -0.1466, -0.1003,  0.1502, -0.1688,  0.4107,  0.1677, -0.2314,\n",
      "          0.1650,  0.0888,  0.0102, -0.0915,  0.3839, -0.1758, -0.1703, -0.0917,\n",
      "          0.3358, -0.1317,  0.0204, -0.1294,  0.0792, -0.1334,  0.0147,  0.3009],\n",
      "        [-0.0675, -0.0137,  0.1669,  0.1069,  0.0721,  0.0373, -0.0502,  0.1817,\n",
      "          0.2545,  0.0166,  0.0391,  0.1686, -0.1655, -0.0686, -0.3643,  0.0639,\n",
      "         -0.0690,  0.0546,  0.1317,  0.0649,  0.0386,  0.0211,  0.1857,  0.0489,\n",
      "          0.0365, -0.0658, -0.2632, -0.1984, -0.0332,  0.0578,  0.0674,  0.0343],\n",
      "        [-0.1813, -0.1040,  0.0812, -0.1426,  0.0893,  0.0409, -0.1936,  0.0635,\n",
      "         -0.0032, -0.0580,  0.1889, -0.0017,  0.0574,  0.2297, -0.0126, -0.0326,\n",
      "          0.1255,  0.2326, -0.0110, -0.2326,  0.2620, -0.1879, -0.2292, -0.0136,\n",
      "         -0.0285, -0.1812, -0.0721, -0.2002,  0.0099, -0.0103, -0.1667,  0.1926],\n",
      "        [ 0.1855, -0.1004,  0.1512,  0.0503, -0.1972, -0.1217,  0.1700,  0.2461,\n",
      "         -0.1227,  0.4531, -0.0483,  0.2191, -0.1108,  0.1343, -0.1850,  0.1624,\n",
      "          0.1734,  0.2414, -0.1392,  0.2010,  0.0255, -0.0515,  0.0643,  0.0467,\n",
      "         -0.0635, -0.0821, -0.0450, -0.2129, -0.0438, -0.1449,  0.1395, -0.0261],\n",
      "        [-0.0468, -0.0246, -0.1006, -0.0404, -0.0264, -0.1215,  0.0276, -0.1783,\n",
      "         -0.0440,  0.0440, -0.0851, -0.2108,  0.0533, -0.1030,  0.3216,  0.0568,\n",
      "         -0.1618, -0.0998, -0.1731, -0.0034, -0.0830,  0.1363,  0.0584,  0.1357,\n",
      "         -0.1118,  0.0241,  0.0370,  0.0427, -0.0733, -0.0439,  0.0222, -0.1090]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([126100.5000,  73899.5000]), win_pct=63.05%, sum=200000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "gen=6: tensor([6709., 3291.]), win_pct=67.09%, sum=10000.0\n",
      "gen=7: tensor([6454.5000, 3545.5000]), win_pct=64.54%, sum=10000.0\n",
      "gen=8: tensor([5073., 4927.]), win_pct=50.73%, sum=10000.0\n",
      "gen=9: tensor([5754.5000, 4245.5000]), win_pct=57.54%, sum=10000.0\n",
      "gen=10: tensor([4074., 5926.]), win_pct=40.74%, sum=10000.0\n",
      "gen=11: tensor([8811., 1189.]), win_pct=88.11%, sum=10000.0\n",
      "gen=12: tensor([8075., 1925.]), win_pct=80.75%, sum=10000.0\n",
      "gen=13: tensor([7093.5000, 2906.5000]), win_pct=70.93%, sum=10000.0\n",
      "gen=14: tensor([5778.5000, 4221.5000]), win_pct=57.78%, sum=10000.0\n",
      "gen=15: tensor([8239., 1761.]), win_pct=82.39%, sum=10000.0\n",
      "gen=16: tensor([6879., 3121.]), win_pct=68.79%, sum=10000.0\n",
      "gen=17: tensor([7835., 2165.]), win_pct=78.35%, sum=10000.0\n",
      "gen=18: tensor([6722.5000, 3277.5000]), win_pct=67.22%, sum=10000.0\n",
      "gen=19: tensor([6481., 3519.]), win_pct=64.81%, sum=10000.0\n",
      "gen=20: tensor([5671., 4329.]), win_pct=56.71%, sum=10000.0\n",
      "player_probs=[0.62489843 0.3751015 ]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([ 8218., 10030.]), win_pct=45.04%, sum=18248.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "gen=6: tensor([ 90., 147.]), win_pct=37.97%, sum=237.0\n",
      "gen=7: tensor([324., 473.]), win_pct=40.65%, sum=797.0\n",
      "gen=8: tensor([1179., 1590.]), win_pct=42.58%, sum=2769.0\n",
      "gen=9: tensor([121., 334.]), win_pct=26.59%, sum=455.0\n",
      "gen=10: tensor([348., 421.]), win_pct=45.25%, sum=769.0\n",
      "gen=11: tensor([105., 107.]), win_pct=49.53%, sum=212.0\n",
      "gen=12: tensor([125.,  87.]), win_pct=58.96%, sum=212.0\n",
      "gen=13: tensor([ 60., 103.]), win_pct=36.81%, sum=163.0\n",
      "gen=14: tensor([144., 179.]), win_pct=44.58%, sum=323.0\n",
      "gen=15: tensor([219., 109.]), win_pct=66.77%, sum=328.0\n",
      "gen=16: tensor([143.,  97.]), win_pct=59.58%, sum=240.0\n",
      "gen=17: tensor([137., 103.]), win_pct=57.08%, sum=240.0\n",
      "gen=18: tensor([16., 29.]), win_pct=35.56%, sum=45.0\n",
      "gen=19: tensor([311., 419.]), win_pct=42.60%, sum=730.0\n",
      "gen=20: tensor([ 95., 161.]), win_pct=37.11%, sum=256.0\n",
      "player_probs=[0.6239609 0.3760391]\n",
      "\n",
      "prefix=(0, 2, 3)\n",
      "gen=*: tensor([3163., 4777.]), win_pct=39.84%, sum=7940.0\n",
      "gen=1: tensor([75., 90.]), win_pct=45.45%, sum=165.0\n",
      "gen=2: tensor([ 6., 30.]), win_pct=16.67%, sum=36.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([2022., 3652.]), win_pct=35.64%, sum=5674.0\n",
      "gen=5: tensor([469., 398.]), win_pct=54.09%, sum=867.0\n",
      "gen=6: tensor([7., 2.]), win_pct=77.78%, sum=9.0\n",
      "gen=7: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=8: tensor([ 91., 108.]), win_pct=45.73%, sum=199.0\n",
      "gen=9: tensor([25., 12.]), win_pct=67.57%, sum=37.0\n",
      "gen=10: tensor([284., 379.]), win_pct=42.84%, sum=663.0\n",
      "gen=11: tensor([4., 2.]), win_pct=66.67%, sum=6.0\n",
      "gen=12: tensor([19., 11.]), win_pct=63.33%, sum=30.0\n",
      "gen=14: tensor([29., 23.]), win_pct=55.77%, sum=52.0\n",
      "gen=15: tensor([45., 23.]), win_pct=66.18%, sum=68.0\n",
      "gen=16: tensor([13.,  2.]), win_pct=86.67%, sum=15.0\n",
      "gen=17: tensor([19., 13.]), win_pct=59.38%, sum=32.0\n",
      "gen=18: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=19: tensor([18.,  7.]), win_pct=72.00%, sum=25.0\n",
      "gen=20: tensor([14., 11.]), win_pct=56.00%, sum=25.0\n",
      "player_probs=[0.36212772 0.6378722 ]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([9949., 7572.]), win_pct=56.78%, sum=17521.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "gen=6: tensor([1722.,  460.]), win_pct=78.92%, sum=2182.0\n",
      "gen=7: tensor([143., 115.]), win_pct=55.43%, sum=258.0\n",
      "gen=8: tensor([192., 187.]), win_pct=50.66%, sum=379.0\n",
      "gen=9: tensor([ 744., 1086.]), win_pct=40.66%, sum=1830.0\n",
      "gen=10: tensor([325., 675.]), win_pct=32.50%, sum=1000.0\n",
      "gen=11: tensor([483., 248.]), win_pct=66.07%, sum=731.0\n",
      "gen=12: tensor([878., 335.]), win_pct=72.38%, sum=1213.0\n",
      "gen=13: tensor([ 49.5000, 126.5000]), win_pct=28.12%, sum=176.0\n",
      "gen=14: tensor([306., 493.]), win_pct=38.30%, sum=799.0\n",
      "gen=15: tensor([639., 200.]), win_pct=76.16%, sum=839.0\n",
      "gen=16: tensor([317., 181.]), win_pct=63.65%, sum=498.0\n",
      "gen=17: tensor([273., 115.]), win_pct=70.36%, sum=388.0\n",
      "gen=18: tensor([727.5000, 236.5000]), win_pct=75.47%, sum=964.0\n",
      "gen=19: tensor([511., 257.]), win_pct=66.54%, sum=768.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.6251088  0.37489122]\n",
      "\n",
      "prefix=(0, 3, 4)\n",
      "gen=*: tensor([4546., 6046.]), win_pct=42.92%, sum=10592.0\n",
      "gen=1: tensor([ 99., 103.]), win_pct=49.01%, sum=202.0\n",
      "gen=2: tensor([458., 821.]), win_pct=35.81%, sum=1279.0\n",
      "gen=3: tensor([19., 79.]), win_pct=19.39%, sum=98.0\n",
      "gen=4: tensor([116., 168.]), win_pct=40.85%, sum=284.0\n",
      "gen=5: tensor([656., 746.]), win_pct=46.79%, sum=1402.0\n",
      "gen=6: tensor([18., 14.]), win_pct=56.25%, sum=32.0\n",
      "gen=7: tensor([128., 107.]), win_pct=54.47%, sum=235.0\n",
      "gen=8: tensor([131., 160.]), win_pct=45.02%, sum=291.0\n",
      "gen=9: tensor([ 565., 1030.]), win_pct=35.42%, sum=1595.0\n",
      "gen=10: tensor([225., 637.]), win_pct=26.10%, sum=862.0\n",
      "gen=11: tensor([408., 230.]), win_pct=63.95%, sum=638.0\n",
      "gen=12: tensor([261., 206.]), win_pct=55.89%, sum=467.0\n",
      "gen=13: tensor([ 38.5000, 124.5000]), win_pct=23.62%, sum=163.0\n",
      "gen=14: tensor([228., 467.]), win_pct=32.81%, sum=695.0\n",
      "gen=15: tensor([365., 165.]), win_pct=68.87%, sum=530.0\n",
      "gen=16: tensor([216., 171.]), win_pct=55.81%, sum=387.0\n",
      "gen=17: tensor([135.,  92.]), win_pct=59.47%, sum=227.0\n",
      "gen=18: tensor([136.5000, 143.5000]), win_pct=48.75%, sum=280.0\n",
      "gen=19: tensor([205., 173.]), win_pct=54.23%, sum=378.0\n",
      "gen=20: tensor([138., 409.]), win_pct=25.23%, sum=547.0\n",
      "player_probs=[0.38535753 0.61464244]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([63413.5000, 22736.5000]), win_pct=73.61%, sum=86150.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "gen=6: tensor([2079.,  580.]), win_pct=78.19%, sum=2659.0\n",
      "gen=7: tensor([5220.5000, 2188.5000]), win_pct=70.46%, sum=7409.0\n",
      "gen=8: tensor([1753.,  616.]), win_pct=74.00%, sum=2369.0\n",
      "gen=9: tensor([3652., 1175.]), win_pct=75.66%, sum=4827.0\n",
      "gen=10: tensor([1090.,  359.]), win_pct=75.22%, sum=1449.0\n",
      "gen=11: tensor([7770.,  526.]), win_pct=93.66%, sum=8296.0\n",
      "gen=12: tensor([2906.,  457.]), win_pct=86.41%, sum=3363.0\n",
      "gen=13: tensor([6824., 2266.]), win_pct=75.07%, sum=9090.0\n",
      "gen=14: tensor([498., 158.]), win_pct=75.91%, sum=656.0\n",
      "gen=15: tensor([6314.,  993.]), win_pct=86.41%, sum=7307.0\n",
      "gen=16: tensor([724., 155.]), win_pct=82.37%, sum=879.0\n",
      "gen=17: tensor([508.,  86.]), win_pct=85.52%, sum=594.0\n",
      "gen=18: tensor([2039.,  509.]), win_pct=80.02%, sum=2548.0\n",
      "gen=19: tensor([4367., 1618.]), win_pct=72.97%, sum=5985.0\n",
      "gen=20: tensor([4857., 2506.]), win_pct=65.96%, sum=7363.0\n",
      "player_probs=[0.72290945 0.27709052]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([5812., 1448.]), win_pct=80.06%, sum=7260.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "gen=6: tensor([538., 166.]), win_pct=76.42%, sum=704.0\n",
      "gen=7: tensor([79., 24.]), win_pct=76.70%, sum=103.0\n",
      "gen=8: tensor([35., 11.]), win_pct=76.09%, sum=46.0\n",
      "gen=9: tensor([315.,  65.]), win_pct=82.89%, sum=380.0\n",
      "gen=10: tensor([33., 16.]), win_pct=67.35%, sum=49.0\n",
      "gen=11: tensor([246.,  41.]), win_pct=85.71%, sum=287.0\n",
      "gen=12: tensor([967., 228.]), win_pct=80.92%, sum=1195.0\n",
      "gen=13: tensor([81., 15.]), win_pct=84.38%, sum=96.0\n",
      "gen=14: tensor([11.,  4.]), win_pct=73.33%, sum=15.0\n",
      "gen=15: tensor([1334.,  233.]), win_pct=85.13%, sum=1567.0\n",
      "gen=16: tensor([33.,  4.]), win_pct=89.19%, sum=37.0\n",
      "gen=17: tensor([56.,  7.]), win_pct=88.89%, sum=63.0\n",
      "gen=18: tensor([1115.,  245.]), win_pct=81.99%, sum=1360.0\n",
      "gen=19: tensor([143.,  37.]), win_pct=79.44%, sum=180.0\n",
      "gen=20: tensor([297., 129.]), win_pct=69.72%, sum=426.0\n",
      "player_probs=[0.68034434 0.31965563]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([41327.5000, 17642.5000]), win_pct=70.08%, sum=58970.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "gen=6: tensor([1297.,  349.]), win_pct=78.80%, sum=1646.0\n",
      "gen=7: tensor([3362.5000, 1705.5000]), win_pct=66.35%, sum=5068.0\n",
      "gen=8: tensor([680., 218.]), win_pct=75.72%, sum=898.0\n",
      "gen=9: tensor([2494.,  935.]), win_pct=72.73%, sum=3429.0\n",
      "gen=10: tensor([967., 317.]), win_pct=75.31%, sum=1284.0\n",
      "gen=11: tensor([6879.,  427.]), win_pct=94.16%, sum=7306.0\n",
      "gen=12: tensor([603.,  47.]), win_pct=92.77%, sum=650.0\n",
      "gen=13: tensor([4557., 1850.]), win_pct=71.13%, sum=6407.0\n",
      "gen=14: tensor([424., 146.]), win_pct=74.39%, sum=570.0\n",
      "gen=15: tensor([1828.,  290.]), win_pct=86.31%, sum=2118.0\n",
      "gen=16: tensor([489., 122.]), win_pct=80.03%, sum=611.0\n",
      "gen=17: tensor([324.,  61.]), win_pct=84.16%, sum=385.0\n",
      "gen=18: tensor([564., 197.]), win_pct=74.11%, sum=761.0\n",
      "gen=19: tensor([3401., 1391.]), win_pct=70.97%, sum=4792.0\n",
      "gen=20: tensor([3769., 2221.]), win_pct=62.92%, sum=5990.0\n",
      "player_probs=[0.6434554 0.3565446]\n",
      "\n",
      "prefix=(0, 4, 4, 3)\n",
      "gen=*: tensor([8937., 2897.]), win_pct=75.52%, sum=11834.0\n",
      "gen=1: tensor([36.,  3.]), win_pct=92.31%, sum=39.0\n",
      "gen=2: tensor([438., 159.]), win_pct=73.37%, sum=597.0\n",
      "gen=3: tensor([725., 351.]), win_pct=67.38%, sum=1076.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([1974.,  893.]), win_pct=68.85%, sum=2867.0\n",
      "gen=6: tensor([810.,  58.]), win_pct=93.32%, sum=868.0\n",
      "gen=7: tensor([690., 621.]), win_pct=52.63%, sum=1311.0\n",
      "gen=8: tensor([328.,  74.]), win_pct=81.59%, sum=402.0\n",
      "gen=9: tensor([236.,  31.]), win_pct=88.39%, sum=267.0\n",
      "gen=10: tensor([614.,  70.]), win_pct=89.77%, sum=684.0\n",
      "gen=11: tensor([387.,  13.]), win_pct=96.75%, sum=400.0\n",
      "gen=12: tensor([36.,  1.]), win_pct=97.30%, sum=37.0\n",
      "gen=13: tensor([229.,  22.]), win_pct=91.24%, sum=251.0\n",
      "gen=14: tensor([70., 35.]), win_pct=66.67%, sum=105.0\n",
      "gen=15: tensor([231.,  14.]), win_pct=94.29%, sum=245.0\n",
      "gen=16: tensor([67., 12.]), win_pct=84.81%, sum=79.0\n",
      "gen=17: tensor([51.,  7.]), win_pct=87.93%, sum=58.0\n",
      "gen=18: tensor([38., 14.]), win_pct=73.08%, sum=52.0\n",
      "gen=19: tensor([1776.,  410.]), win_pct=81.24%, sum=2186.0\n",
      "gen=20: tensor([199., 109.]), win_pct=64.61%, sum=308.0\n",
      "player_probs=[0.3821403  0.61785966]\n",
      "\n",
      "prefix=(0, 4, 4, 4)\n",
      "gen=*: tensor([14185.,  6380.]), win_pct=68.98%, sum=20565.0\n",
      "gen=1: tensor([30., 12.]), win_pct=71.43%, sum=42.0\n",
      "gen=2: tensor([728., 660.]), win_pct=52.45%, sum=1388.0\n",
      "gen=3: tensor([780., 636.]), win_pct=55.08%, sum=1416.0\n",
      "gen=4: tensor([29., 14.]), win_pct=67.44%, sum=43.0\n",
      "gen=5: tensor([442., 235.]), win_pct=65.29%, sum=677.0\n",
      "gen=6: tensor([123., 184.]), win_pct=40.07%, sum=307.0\n",
      "gen=7: tensor([792., 386.]), win_pct=67.23%, sum=1178.0\n",
      "gen=8: tensor([214.,  64.]), win_pct=76.98%, sum=278.0\n",
      "gen=9: tensor([630., 146.]), win_pct=81.19%, sum=776.0\n",
      "gen=10: tensor([55., 45.]), win_pct=55.00%, sum=100.0\n",
      "gen=11: tensor([956., 126.]), win_pct=88.35%, sum=1082.0\n",
      "gen=12: tensor([208.,  25.]), win_pct=89.27%, sum=233.0\n",
      "gen=13: tensor([3412., 1295.]), win_pct=72.49%, sum=4707.0\n",
      "gen=14: tensor([119.,  68.]), win_pct=63.64%, sum=187.0\n",
      "gen=15: tensor([1213.,  244.]), win_pct=83.25%, sum=1457.0\n",
      "gen=16: tensor([225.,  56.]), win_pct=80.07%, sum=281.0\n",
      "gen=17: tensor([169.,  31.]), win_pct=84.50%, sum=200.0\n",
      "gen=18: tensor([253.,  98.]), win_pct=72.08%, sum=351.0\n",
      "gen=19: tensor([1032.,  607.]), win_pct=62.97%, sum=1639.0\n",
      "gen=20: tensor([2775., 1448.]), win_pct=65.71%, sum=4223.0\n",
      "player_probs=[0.56860834 0.4313917 ]\n",
      "\n",
      "prefix=(0, 4, 4, 5)\n",
      "gen=*: tensor([4593., 3993.]), win_pct=53.49%, sum=8586.0\n",
      "gen=1: tensor([22.,  5.]), win_pct=81.48%, sum=27.0\n",
      "gen=2: tensor([255., 182.]), win_pct=58.35%, sum=437.0\n",
      "gen=3: tensor([1793., 2519.]), win_pct=41.58%, sum=4312.0\n",
      "gen=5: tensor([384., 367.]), win_pct=51.13%, sum=751.0\n",
      "gen=6: tensor([144.,  35.]), win_pct=80.45%, sum=179.0\n",
      "gen=7: tensor([472., 219.]), win_pct=68.31%, sum=691.0\n",
      "gen=8: tensor([51., 13.]), win_pct=79.69%, sum=64.0\n",
      "gen=9: tensor([28., 13.]), win_pct=68.29%, sum=41.0\n",
      "gen=10: tensor([132.,  49.]), win_pct=72.93%, sum=181.0\n",
      "gen=11: tensor([128.,  40.]), win_pct=76.19%, sum=168.0\n",
      "gen=12: tensor([111.,  14.]), win_pct=88.80%, sum=125.0\n",
      "gen=13: tensor([412., 232.]), win_pct=63.98%, sum=644.0\n",
      "gen=14: tensor([25.,  4.]), win_pct=86.21%, sum=29.0\n",
      "gen=15: tensor([50.,  9.]), win_pct=84.75%, sum=59.0\n",
      "gen=16: tensor([51., 14.]), win_pct=78.46%, sum=65.0\n",
      "gen=17: tensor([13.,  6.]), win_pct=68.42%, sum=19.0\n",
      "gen=18: tensor([54., 22.]), win_pct=71.05%, sum=76.0\n",
      "gen=19: tensor([281., 152.]), win_pct=64.90%, sum=433.0\n",
      "gen=20: tensor([187.,  98.]), win_pct=65.61%, sum=285.0\n",
      "player_probs=[0.46351972 0.5364803 ]\n",
      "\n",
      "prefix=(0, 4, 4, 6)\n",
      "gen=*: tensor([9904., 1662.]), win_pct=85.63%, sum=11566.0\n",
      "gen=1: tensor([24.,  7.]), win_pct=77.42%, sum=31.0\n",
      "gen=2: tensor([122.,  86.]), win_pct=58.65%, sum=208.0\n",
      "gen=3: tensor([149.,  58.]), win_pct=71.98%, sum=207.0\n",
      "gen=5: tensor([19.,  2.]), win_pct=90.48%, sum=21.0\n",
      "gen=6: tensor([43., 11.]), win_pct=79.63%, sum=54.0\n",
      "gen=7: tensor([1033.,  150.]), win_pct=87.32%, sum=1183.0\n",
      "gen=8: tensor([66., 24.]), win_pct=73.33%, sum=90.0\n",
      "gen=9: tensor([1419.,  643.]), win_pct=68.82%, sum=2062.0\n",
      "gen=10: tensor([70., 23.]), win_pct=75.27%, sum=93.0\n",
      "gen=11: tensor([5241.,  172.]), win_pct=96.82%, sum=5413.0\n",
      "gen=12: tensor([203.,   2.]), win_pct=99.02%, sum=205.0\n",
      "gen=13: tensor([397., 211.]), win_pct=65.30%, sum=608.0\n",
      "gen=14: tensor([190.,  16.]), win_pct=92.23%, sum=206.0\n",
      "gen=15: tensor([282.,   4.]), win_pct=98.60%, sum=286.0\n",
      "gen=16: tensor([93., 13.]), win_pct=87.74%, sum=106.0\n",
      "gen=17: tensor([58.,  4.]), win_pct=93.55%, sum=62.0\n",
      "gen=18: tensor([197.,  47.]), win_pct=80.74%, sum=244.0\n",
      "gen=19: tensor([144.,  71.]), win_pct=66.98%, sum=215.0\n",
      "gen=20: tensor([154., 118.]), win_pct=56.62%, sum=272.0\n",
      "player_probs=[0.30707064 0.6929294 ]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([13578.,  9890.]), win_pct=57.86%, sum=23468.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "gen=6: tensor([584., 527.]), win_pct=52.57%, sum=1111.0\n",
      "gen=7: tensor([385., 369.]), win_pct=51.06%, sum=754.0\n",
      "gen=8: tensor([939., 524.]), win_pct=64.18%, sum=1463.0\n",
      "gen=9: tensor([282., 492.]), win_pct=36.43%, sum=774.0\n",
      "gen=10: tensor([1079., 1630.]), win_pct=39.83%, sum=2709.0\n",
      "gen=11: tensor([105.,  86.]), win_pct=54.97%, sum=191.0\n",
      "gen=12: tensor([2459.,  540.]), win_pct=81.99%, sum=2999.0\n",
      "gen=13: tensor([ 91., 244.]), win_pct=27.16%, sum=335.0\n",
      "gen=14: tensor([1202.5000, 1005.5000]), win_pct=54.46%, sum=2208.0\n",
      "gen=15: tensor([563., 286.]), win_pct=66.31%, sum=849.0\n",
      "gen=16: tensor([588., 192.]), win_pct=75.38%, sum=780.0\n",
      "gen=17: tensor([567., 157.]), win_pct=78.31%, sum=724.0\n",
      "gen=18: tensor([1694.,  931.]), win_pct=64.53%, sum=2625.0\n",
      "gen=19: tensor([571., 367.]), win_pct=60.87%, sum=938.0\n",
      "gen=20: tensor([218., 442.]), win_pct=33.03%, sum=660.0\n",
      "player_probs=[0.6570532 0.3429469]\n",
      "\n",
      "prefix=(0, 5, 4)\n",
      "gen=*: tensor([6657., 7845.]), win_pct=45.90%, sum=14502.0\n",
      "gen=1: tensor([102., 113.]), win_pct=47.44%, sum=215.0\n",
      "gen=2: tensor([450., 802.]), win_pct=35.94%, sum=1252.0\n",
      "gen=3: tensor([62.5000, 64.5000]), win_pct=49.21%, sum=127.0\n",
      "gen=4: tensor([124., 116.]), win_pct=51.67%, sum=240.0\n",
      "gen=5: tensor([334., 486.]), win_pct=40.73%, sum=820.0\n",
      "gen=6: tensor([203., 396.]), win_pct=33.89%, sum=599.0\n",
      "gen=7: tensor([314., 357.]), win_pct=46.80%, sum=671.0\n",
      "gen=8: tensor([101., 119.]), win_pct=45.91%, sum=220.0\n",
      "gen=9: tensor([212., 455.]), win_pct=31.78%, sum=667.0\n",
      "gen=10: tensor([ 832., 1528.]), win_pct=35.25%, sum=2360.0\n",
      "gen=11: tensor([84., 85.]), win_pct=49.70%, sum=169.0\n",
      "gen=12: tensor([499., 148.]), win_pct=77.13%, sum=647.0\n",
      "gen=13: tensor([ 73., 240.]), win_pct=23.32%, sum=313.0\n",
      "gen=14: tensor([933.5000, 974.5000]), win_pct=48.93%, sum=1908.0\n",
      "gen=15: tensor([427., 263.]), win_pct=61.88%, sum=690.0\n",
      "gen=16: tensor([328., 154.]), win_pct=68.05%, sum=482.0\n",
      "gen=17: tensor([91., 39.]), win_pct=70.00%, sum=130.0\n",
      "gen=18: tensor([882., 760.]), win_pct=53.71%, sum=1642.0\n",
      "gen=19: tensor([428., 321.]), win_pct=57.14%, sum=749.0\n",
      "gen=20: tensor([177., 424.]), win_pct=29.45%, sum=601.0\n",
      "player_probs=[0.28507188 0.7149281 ]\n",
      "\n",
      "prefix=(0, 5, 4, 4)\n",
      "gen=*: tensor([4227.5000, 3571.5000]), win_pct=54.21%, sum=7799.0\n",
      "gen=1: tensor([16., 12.]), win_pct=57.14%, sum=28.0\n",
      "gen=2: tensor([347., 560.]), win_pct=38.26%, sum=907.0\n",
      "gen=3: tensor([3., 4.]), win_pct=42.86%, sum=7.0\n",
      "gen=4: tensor([3., 6.]), win_pct=33.33%, sum=9.0\n",
      "gen=5: tensor([262., 347.]), win_pct=43.02%, sum=609.0\n",
      "gen=6: tensor([ 78., 153.]), win_pct=33.77%, sum=231.0\n",
      "gen=7: tensor([260., 188.]), win_pct=58.04%, sum=448.0\n",
      "gen=8: tensor([94., 95.]), win_pct=49.74%, sum=189.0\n",
      "gen=9: tensor([16.,  7.]), win_pct=69.57%, sum=23.0\n",
      "gen=10: tensor([221., 173.]), win_pct=56.09%, sum=394.0\n",
      "gen=11: tensor([73., 60.]), win_pct=54.89%, sum=133.0\n",
      "gen=12: tensor([361.,  95.]), win_pct=79.17%, sum=456.0\n",
      "gen=13: tensor([ 63., 207.]), win_pct=23.33%, sum=270.0\n",
      "gen=14: tensor([679.5000, 520.5000]), win_pct=56.62%, sum=1200.0\n",
      "gen=15: tensor([302., 142.]), win_pct=68.02%, sum=444.0\n",
      "gen=16: tensor([260.,  96.]), win_pct=73.03%, sum=356.0\n",
      "gen=17: tensor([69., 23.]), win_pct=75.00%, sum=92.0\n",
      "gen=18: tensor([766., 559.]), win_pct=57.81%, sum=1325.0\n",
      "gen=19: tensor([266., 161.]), win_pct=62.30%, sum=427.0\n",
      "gen=20: tensor([ 88., 163.]), win_pct=35.06%, sum=251.0\n",
      "player_probs=[0.51110727 0.48889273]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([25180.5000, 13503.5000]), win_pct=65.09%, sum=38684.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "gen=6: tensor([1994., 1294.]), win_pct=60.64%, sum=3288.0\n",
      "gen=7: tensor([131., 106.]), win_pct=55.27%, sum=237.0\n",
      "gen=8: tensor([238., 177.]), win_pct=57.35%, sum=415.0\n",
      "gen=9: tensor([731.5000, 529.5000]), win_pct=58.01%, sum=1261.0\n",
      "gen=10: tensor([ 863., 1507.]), win_pct=36.41%, sum=2370.0\n",
      "gen=11: tensor([284., 103.]), win_pct=73.39%, sum=387.0\n",
      "gen=12: tensor([1628.,  402.]), win_pct=80.20%, sum=2030.0\n",
      "gen=13: tensor([53., 98.]), win_pct=35.10%, sum=151.0\n",
      "gen=14: tensor([3508., 2068.]), win_pct=62.91%, sum=5576.0\n",
      "gen=15: tensor([370.,  69.]), win_pct=84.28%, sum=439.0\n",
      "gen=16: tensor([5000., 2320.]), win_pct=68.31%, sum=7320.0\n",
      "gen=17: tensor([6265., 1506.]), win_pct=80.62%, sum=7771.0\n",
      "gen=18: tensor([2154., 1374.]), win_pct=61.05%, sum=3528.0\n",
      "gen=19: tensor([612., 620.]), win_pct=49.68%, sum=1232.0\n",
      "gen=20: tensor([181., 334.]), win_pct=35.15%, sum=515.0\n",
      "player_probs=[0.60984075 0.39015922]\n",
      "\n",
      "prefix=(0, 6, 4)\n",
      "gen=*: tensor([9921.5000, 8800.5000]), win_pct=52.99%, sum=18722.0\n",
      "gen=1: tensor([ 94., 117.]), win_pct=44.55%, sum=211.0\n",
      "gen=2: tensor([ 69., 225.]), win_pct=23.47%, sum=294.0\n",
      "gen=3: tensor([45., 89.]), win_pct=33.58%, sum=134.0\n",
      "gen=4: tensor([24., 46.]), win_pct=34.29%, sum=70.0\n",
      "gen=5: tensor([89., 39.]), win_pct=69.53%, sum=128.0\n",
      "gen=6: tensor([74., 71.]), win_pct=51.03%, sum=145.0\n",
      "gen=7: tensor([109.,  98.]), win_pct=52.66%, sum=207.0\n",
      "gen=8: tensor([57., 29.]), win_pct=66.28%, sum=86.0\n",
      "gen=9: tensor([218.5000, 115.5000]), win_pct=65.42%, sum=334.0\n",
      "gen=10: tensor([ 621., 1373.]), win_pct=31.14%, sum=1994.0\n",
      "gen=11: tensor([218.,  96.]), win_pct=69.43%, sum=314.0\n",
      "gen=12: tensor([244., 136.]), win_pct=64.21%, sum=380.0\n",
      "gen=13: tensor([42., 95.]), win_pct=30.66%, sum=137.0\n",
      "gen=14: tensor([1581., 1585.]), win_pct=49.94%, sum=3166.0\n",
      "gen=15: tensor([68., 30.]), win_pct=69.39%, sum=98.0\n",
      "gen=16: tensor([3349., 2052.]), win_pct=62.01%, sum=5401.0\n",
      "gen=17: tensor([1336.,  635.]), win_pct=67.78%, sum=1971.0\n",
      "gen=18: tensor([1114., 1100.]), win_pct=50.32%, sum=2214.0\n",
      "gen=19: tensor([430., 561.]), win_pct=43.39%, sum=991.0\n",
      "gen=20: tensor([139., 308.]), win_pct=31.10%, sum=447.0\n",
      "player_probs=[0.31274298 0.687257  ]\n",
      "\n",
      "prefix=(0, 6, 4, 4)\n",
      "gen=*: tensor([4925., 3458.]), win_pct=58.75%, sum=8383.0\n",
      "gen=1: tensor([17., 19.]), win_pct=47.22%, sum=36.0\n",
      "gen=2: tensor([27., 88.]), win_pct=23.48%, sum=115.0\n",
      "gen=3: tensor([27., 40.]), win_pct=40.30%, sum=67.0\n",
      "gen=4: tensor([1., 1.]), win_pct=50.00%, sum=2.0\n",
      "gen=5: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=6: tensor([54., 46.]), win_pct=54.00%, sum=100.0\n",
      "gen=7: tensor([49., 73.]), win_pct=40.16%, sum=122.0\n",
      "gen=8: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=9: tensor([3., 2.]), win_pct=60.00%, sum=5.0\n",
      "gen=10: tensor([ 85., 124.]), win_pct=40.67%, sum=209.0\n",
      "gen=11: tensor([39., 15.]), win_pct=72.22%, sum=54.0\n",
      "gen=12: tensor([176.,  81.]), win_pct=68.48%, sum=257.0\n",
      "gen=13: tensor([36., 71.]), win_pct=33.64%, sum=107.0\n",
      "gen=14: tensor([97., 67.]), win_pct=59.15%, sum=164.0\n",
      "gen=15: tensor([55., 21.]), win_pct=72.37%, sum=76.0\n",
      "gen=16: tensor([2200., 1187.]), win_pct=64.95%, sum=3387.0\n",
      "gen=17: tensor([1015.,  472.]), win_pct=68.26%, sum=1487.0\n",
      "gen=18: tensor([739., 707.]), win_pct=51.11%, sum=1446.0\n",
      "gen=19: tensor([205., 312.]), win_pct=39.65%, sum=517.0\n",
      "gen=20: tensor([ 94., 126.]), win_pct=42.73%, sum=220.0\n",
      "player_probs=[0.41703367 0.5829663 ]\n",
      "\n",
      "prefix=(0, 6, 6)\n",
      "gen=*: tensor([5692., 2272.]), win_pct=71.47%, sum=7964.0\n",
      "gen=1: tensor([119.,  76.]), win_pct=61.03%, sum=195.0\n",
      "gen=2: tensor([1., 4.]), win_pct=20.00%, sum=5.0\n",
      "gen=4: tensor([3., 3.]), win_pct=50.00%, sum=6.0\n",
      "gen=5: tensor([21., 22.]), win_pct=48.84%, sum=43.0\n",
      "gen=6: tensor([1593., 1107.]), win_pct=59.00%, sum=2700.0\n",
      "gen=7: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=8: tensor([63., 58.]), win_pct=52.07%, sum=121.0\n",
      "gen=9: tensor([225., 323.]), win_pct=41.06%, sum=548.0\n",
      "gen=10: tensor([37., 32.]), win_pct=53.62%, sum=69.0\n",
      "gen=11: tensor([23.,  0.]), win_pct=100.00%, sum=23.0\n",
      "gen=12: tensor([301.,  30.]), win_pct=90.94%, sum=331.0\n",
      "gen=13: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=14: tensor([937., 314.]), win_pct=74.90%, sum=1251.0\n",
      "gen=15: tensor([53.,  6.]), win_pct=89.83%, sum=59.0\n",
      "gen=16: tensor([264.,  35.]), win_pct=88.29%, sum=299.0\n",
      "gen=17: tensor([1825.,  218.]), win_pct=89.33%, sum=2043.0\n",
      "gen=18: tensor([141.,  13.]), win_pct=91.56%, sum=154.0\n",
      "gen=19: tensor([69., 21.]), win_pct=76.67%, sum=90.0\n",
      "gen=20: tensor([11.,  7.]), win_pct=61.11%, sum=18.0\n",
      "player_probs=[0.61166614 0.38833386]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([3282.5000, 5879.5000]), win_pct=35.83%, sum=9162.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "gen=6: tensor([79., 65.]), win_pct=54.86%, sum=144.0\n",
      "gen=7: tensor([160.,  79.]), win_pct=66.95%, sum=239.0\n",
      "gen=8: tensor([ 710., 1756.]), win_pct=28.79%, sum=2466.0\n",
      "gen=9: tensor([118., 313.]), win_pct=27.38%, sum=431.0\n",
      "gen=10: tensor([140., 576.]), win_pct=19.55%, sum=716.0\n",
      "gen=11: tensor([28., 61.]), win_pct=31.46%, sum=89.0\n",
      "gen=12: tensor([45., 44.]), win_pct=50.56%, sum=89.0\n",
      "gen=13: tensor([10., 35.]), win_pct=22.22%, sum=45.0\n",
      "gen=14: tensor([ 79., 220.]), win_pct=26.42%, sum=299.0\n",
      "gen=15: tensor([84., 60.]), win_pct=58.33%, sum=144.0\n",
      "gen=16: tensor([55., 89.]), win_pct=38.19%, sum=144.0\n",
      "gen=17: tensor([53., 91.]), win_pct=36.81%, sum=144.0\n",
      "gen=18: tensor([ 57., 139.]), win_pct=29.08%, sum=196.0\n",
      "gen=19: tensor([ 42., 102.]), win_pct=29.17%, sum=144.0\n",
      "gen=20: tensor([ 66., 233.]), win_pct=22.07%, sum=299.0\n",
      "player_probs=[0.5766261 0.4233739]\n"
     ]
    }
   ],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tournament to calcualte ELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from contextlib import asynccontextmanager\n",
    "from rgi.rgizero.tournament import Tournament\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_player_factory(model, simulations, game, device, block_size, action_vocab, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a shared evaluator and returns a factory function that produces \n",
    "    new AlphazeroPlayer instances using that shared evaluator.\n",
    "    \"\"\"\n",
    "    # 1. Setup the shared evaluator\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(\n",
    "        model, \n",
    "        device=device, \n",
    "        block_size=block_size, \n",
    "        vocab=action_vocab\n",
    "    )\n",
    "    async_evaluator = AsyncNetworkEvaluator(\n",
    "        base_evaluator=serial_evaluator, \n",
    "        max_batch_size=max_batch_size, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 2. Start the evaluator background task\n",
    "    await async_evaluator.start()\n",
    "    \n",
    "    try:\n",
    "        # 3. Define the factory. This is called by Tournament for every game.\n",
    "        # It creates a NEW player instance but uses the SHARED async_evaluator.\n",
    "        def player_factory():\n",
    "            # Create a fresh RNG for each game/player instance\n",
    "            rng = np.random.default_rng(np.random.randint(0, 2**31))\n",
    "            return AlphazeroPlayer(\n",
    "                game, \n",
    "                async_evaluator, \n",
    "                rng=rng, \n",
    "                add_noise=True, \n",
    "                simulations=simulations\n",
    "            )\n",
    "            \n",
    "        yield player_factory\n",
    "        \n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        await async_evaluator.stop()\n",
    "\n",
    "async def run_tournament_async():\n",
    "    # Use async with to manage the lifecycle of the evaluators\n",
    "    async with (\n",
    "        # create_player_factory(model_dict[0], 100, game, device, block_size, action_vocab, 10) as factory_gen0_100,\n",
    "        # create_player_factory(model_dict[1], 100, game, device, block_size, action_vocab, 10) as factory_gen1_100,\n",
    "        # create_player_factory(model_dict[2], 100, game, device, block_size, action_vocab, 10) as factory_gen2_100,\n",
    "        # create_player_factory(model_dict[3], 100, game, device, block_size, action_vocab, 10) as factory_gen3_100,\n",
    "        # create_player_factory(model_dict[4], 100, game, device, block_size, action_vocab, 10) as factory_gen4_100,\n",
    "        # create_player_factory(model_dict[5], 100, game, device, block_size, action_vocab, 10) as factory_gen5_100,\n",
    "        # create_player_factory(model_dict[10], 100, game, device, block_size, action_vocab, 10) as factory_gen6_100,\n",
    "        # create_player_factory(model_dict[15], 100, game, device, block_size, action_vocab, 10) as factory_gen7_100,\n",
    "        # create_player_factory(model_dict[20], 100, game, device, block_size, action_vocab, 10) as factory_gen8_100,\n",
    "\n",
    "        create_player_factory(model_dict[0], 200, game, device, block_size, action_vocab, 10) as factory_gen0_200,\n",
    "        create_player_factory(model_dict[1], 200, game, device, block_size, action_vocab, 10) as factory_gen1_200,\n",
    "        create_player_factory(model_dict[2], 200, game, device, block_size, action_vocab, 10) as factory_gen2_200,\n",
    "        create_player_factory(model_dict[3], 200, game, device, block_size, action_vocab, 10) as factory_gen3_200,\n",
    "        create_player_factory(model_dict[4], 200, game, device, block_size, action_vocab, 10) as factory_gen4_200,\n",
    "        create_player_factory(model_dict[5], 200, game, device, block_size, action_vocab, 10) as factory_gen5_200,\n",
    "        create_player_factory(model_dict[10], 200, game, device, block_size, action_vocab, 10) as factory_gen10_200,\n",
    "        create_player_factory(model_dict[15], 200, game, device, block_size, action_vocab, 10) as factory_gen15_200,\n",
    "        create_player_factory(model_dict[20], 200, game, device, block_size, action_vocab, 10) as factory_gen20_200,\n",
    "        ):\n",
    "        \n",
    "        # The dictionary now maps names to FACTORIES (Callables), not Player instances\n",
    "        player_factories = {\n",
    "            # \"factory_gen0_100\": factory_gen0_100,\n",
    "            # \"factory_gen1_100\": factory_gen1_100,\n",
    "            # \"factory_gen2_100\": factory_gen2_100,\n",
    "            # \"factory_gen3_100\": factory_gen3_100,\n",
    "            # \"factory_gen4_100\": factory_gen4_100,\n",
    "            # \"factory_gen5_100\": factory_gen5_100,\n",
    "            # \"factory_gen6_100\": factory_gen6_100,\n",
    "            # \"factory_gen7_100\": factory_gen7_100,\n",
    "\n",
    "            \"factory_gen0_200\": factory_gen0_200,\n",
    "            \"factory_gen1_200\": factory_gen1_200,\n",
    "            \"factory_gen2_200\": factory_gen2_200,\n",
    "            #\"factory_gen3_200\": factory_gen3_200,\n",
    "            #\"factory_gen4_200\": factory_gen4_200,\n",
    "            \"factory_gen5_200\": factory_gen5_200,\n",
    "            \"factory_gen10_200\": factory_gen10_200,\n",
    "            #\"factory_gen15_200\": factory_gen15_200,\n",
    "            \"factory_gen20_200\": factory_gen20_200,\n",
    "        }\n",
    "        \n",
    "        tournament = Tournament(game, player_factories, initial_elo=1000)\n",
    "        \n",
    "        print(\"Running tournament...\")\n",
    "        await tournament.run(num_games=1_000, concurrent_games=2000)\n",
    "        tournament.print_standings()\n",
    "\n",
    "if RUN_TOURNAMENT:\n",
    "    await run_tournament_async()\n",
    "\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 10000/10000 [1:25:59<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen6_200     1140.5     1247     827-419-1      \n",
    "# 2     factory_gen2_200     1100.1     1251     693-554-4      \n",
    "# 3     factory_gen5_100     1074.4     1251     598-652-1      \n",
    "# 4     factory_gen3_200     1029.1     1252     674-573-5      \n",
    "# 5     factory_gen4_200     1027.0     1248     711-536-1      \n",
    "# 6     factory_gen0_200     1020.0     1254     444-810-0      \n",
    "# 7     factory_gen5_200     990.2      1248     742-502-4      \n",
    "# 8     factory_gen7_100     987.5      1250     650-597-3      \n",
    "# 9     factory_gen7_200     979.2      1248     768-476-4      \n",
    "# 10    factory_gen2_100     974.0      1249     522-723-4      \n",
    "# 11    factory_gen6_100     966.6      1248     684-564-0      \n",
    "# 12    factory_gen4_100     964.2      1251     557-693-1      \n",
    "# 13    factory_gen1_100     962.5      1252     547-705-0      \n",
    "# 14    factory_gen3_100     947.0      1251     528-723-0      \n",
    "# 15    factory_gen1_200     941.1      1252     630-620-2      \n",
    "# 16    factory_gen0_100     896.5      1248     410-838-0     \n",
    "\n",
    "\n",
    "## 20 generations.\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 1000/1000 [08:35<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen10_200    1114.2     333      212-120-1      \n",
    "# 2     factory_gen2_200     1032.6     333      190-141-2      \n",
    "# 3     factory_gen1_200     1003.9     334      159-175-0      \n",
    "# 4     factory_gen20_200    1000.9     335      171-164-0      \n",
    "# 5     factory_gen5_200     974.6      331      183-146-2      \n",
    "# 6     factory_gen0_200     873.8      334      82-251-1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_config_fields: {'dropout', 'n_embd', 'n_max_context', 'bias', 'n_head', 'n_layer'}\n",
      "train_config_fields: {'model_name', 'max_epochs', 'decay_lr', 'dtype', 'eval_iters', 'eval_only', 'beta1', 'batch_size', 'weight_decay', 'always_save_checkpoint', 'device', 'compile', 'eval_interval', 'grad_clip', 'min_lr', 'wandb_log', 'lr_decay_iters', 'model_version', 'learning_rate', 'gradient_accumulation_steps', 'max_iters', 'log_interval', 'beta2', 'warmup_iters'}\n"
     ]
    }
   ],
   "source": [
    "# reload_local_modules()\n",
    "\n",
    "from rgi.rgizero.models.tuner import Tuner\n",
    "\n",
    "transform_config_fields = {f.name for f in dataclasses.fields(TransformerConfig)}\n",
    "train_config_fields = {f.name for f in dataclasses.fields(TrainConfig)}\n",
    "\n",
    "print(f'transform_config_fields: {transform_config_fields}')\n",
    "print(f'train_config_fields: {train_config_fields}')\n",
    "\n",
    "\n",
    "def train_with(**overrides):\n",
    "    \"\"\"Wrapper fn to train a model using the latest train.py code and the given overrides.\"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    for override in overrides:\n",
    "        if override not in transform_config_fields and override not in train_config_fields:\n",
    "            raise ValueError(f\"Invalid override: {override}\")\n",
    "\n",
    "\n",
    "    model_config_overrides = {k:v for k,v in overrides.items() if k in transform_config_fields}\n",
    "    train_config_overrides = {k:v for k,v in overrides.items() if k in train_config_fields}\n",
    "\n",
    "    model_config = TransformerConfig(**model_config_overrides)\n",
    "    train_config = TrainConfig(**train_config_overrides)\n",
    "\n",
    "    print(f\"model_config={model_config}\")\n",
    "    print(f\"train_config={train_config}\")\n",
    "    model = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "\n",
    "    training_splits = [f'gen-{generation_id}' for generation_id in range(1, NUM_GENERATIONS+1)]\n",
    "\n",
    "    model, trainer = train_model(model, training_splits, train_config)\n",
    "    loss_dict = trainer.estimate_loss()\n",
    "    loss_dict = {k: float(v) for k, v in loss_dict.items()}\n",
    "\n",
    "    # def train_model(model, training_splits, train_config):\n",
    "    # loss_dict = train.train_and_evaluate(**overrides)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"## train_loss: {loss_dict['train']:.4f}, val_loss: {loss_dict['val']:.4f}, Time taken: {elapsed}s, overrides={overrides}\")\n",
    "    return loss_dict, elapsed\n",
    "\n",
    "# train_with(model_name='c4-tuning', model_version='0.1',\n",
    "#             n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8,  # tiny model\n",
    "#             max_iters=100, max_epochs=10,\n",
    "#             eval_iters = 20, log_interval = 100, \n",
    "#             gradient_accumulation_steps = 1,\n",
    "#             batch_size = 32, learning_rate = LEARNING_RATE,    \n",
    "#             lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "#             min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "#             beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "#             warmup_iters = 0,  # not super necessary potentially\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reloaded rgi\n",
      "reloaded rgi.rgizero\n",
      "reloaded rgi.rgizero.games\n",
      "reloaded rgi.rgizero.games.base\n",
      "reloaded rgi.rgizero.games.connect4\n",
      "reloaded rgi.rgizero.players\n",
      "reloaded rgi.rgizero.players.base\n",
      "reloaded rgi.rgizero.players.alphazero\n",
      "reloaded rgi.rgizero.common\n",
      "reloaded rgi.rgizero.games.history_wrapper\n",
      "reloaded rgi.rgizero.data\n",
      "reloaded rgi.rgizero.data.trajectory_dataset\n",
      "reloaded rgi.rgizero.models\n",
      "reloaded rgi.rgizero.models.transformer\n",
      "reloaded rgi.rgizero.models.token_transformer\n",
      "reloaded rgi.rgizero.models.action_history_transformer\n",
      "reloaded rgi.rgizero.train\n",
      "reloaded rgi.rgizero.tournament\n",
      "reloaded rgi.rgizero.models.tuner\n",
      "  -> Updated 'Connect4Game' in globals() from 'rgi.rgizero.games.connect4'\n",
      "  -> Updated 'AlphazeroPlayer' in globals() from 'rgi.rgizero.players.alphazero'\n",
      "  -> Updated 'play_game' in globals() from 'rgi.rgizero.players.alphazero'\n",
      "  -> Updated 'HistoryTrackingGame' in globals() from 'rgi.rgizero.games.history_wrapper'\n",
      "  -> Updated 'Vocab' in globals() from 'rgi.rgizero.data.trajectory_dataset'\n",
      "  -> Updated 'TOKENS' in globals() from 'rgi.rgizero.common'\n",
      "  -> Updated 'ActionHistoryTransformer' in globals() from 'rgi.rgizero.models.action_history_transformer'\n",
      "  -> Updated 'TransformerConfig' in globals() from 'rgi.rgizero.models.transformer'\n",
      "  -> Updated 'AsyncNetworkEvaluator' in globals() from 'rgi.rgizero.models.action_history_transformer'\n",
      "  -> Updated 'ActionHistoryTransformerEvaluator' in globals() from 'rgi.rgizero.models.action_history_transformer'\n",
      "  -> Updated 'play_game_async' in globals() from 'rgi.rgizero.players.alphazero'\n",
      "  -> Updated 'TrajectoryDatasetBuilder' in globals() from 'rgi.rgizero.data.trajectory_dataset'\n",
      "  -> Updated 'TrajectoryDataset' in globals() from 'rgi.rgizero.data.trajectory_dataset'\n",
      "  -> Updated 'build_trajectory_loader' in globals() from 'rgi.rgizero.data.trajectory_dataset'\n",
      "  -> Updated 'Trainer' in globals() from 'rgi.rgizero.train'\n",
      "  -> Updated 'TrainConfig' in globals() from 'rgi.rgizero.train'\n",
      "  -> Updated 'Tournament' in globals() from 'rgi.rgizero.tournament'\n",
      "  -> Updated 'Tuner' in globals() from 'rgi.rgizero.models.tuner'\n"
     ]
    }
   ],
   "source": [
    "reload_local_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: None\n",
      "## Initial Model, loss=2.4566147327423096 elapsed=13.30814504623413s\n",
      "## Tuning generation 1: model_name\n",
      "## Tuning generation 1: model_version\n",
      "## Tuning generation 1: n_max_context\n",
      "## Tuning generation 1: n_layer\n",
      "## Tuning generation 1: n_layer improved, val=4, best=2.4735591411590576, delta=-0.016944408416748047 elapsed=8.71258020401001s delta=-4.595564842224121s\n",
      "## Tuning generation 1: n_head\n",
      "## Tuning generation 1: n_head improved, val=4, best=2.4671905040740967, delta=0.0063686370849609375 elapsed=7.430401086807251s delta=-1.2821791172027588s\n",
      "## Tuning generation 1: n_embd\n",
      "## Tuning generation 1: max_iters\n",
      "## Tuning generation 1: max_epochs\n",
      "## Tuning generation 1: eval_iters\n",
      "## Tuning generation 1: log_interval\n",
      "## Tuning generation 1: eval_interval\n",
      "## Tuning generation 1: gradient_accumulation_steps\n",
      "## Tuning generation 1: batch_size\n",
      "## Tuning generation 1: learning_rate\n",
      "## Tuning generation 1: decay_lr\n",
      "## Tuning generation 1: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 1: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 1: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 1: weight_decay\n",
      "## Tuning generation 1: beta1\n",
      "## Tuning generation 1: beta2\n",
      "## Tuning generation 1: grad_clip\n",
      "## Tuning generation 1: dtype\n",
      "## Tuning generation 1: dropout\n",
      "## Tuning generation 1: bias\n",
      "## Tuning generation 2: model_name\n",
      "## Tuning generation 2: model_version\n",
      "## Tuning generation 2: n_max_context\n",
      "## Tuning generation 2: n_layer\n",
      "## Tuning generation 2: n_layer improved, val=2, best=2.4568657875061035, delta=0.010324716567993164 elapsed=7.078122138977051s delta=-0.3522789478302002s\n",
      "## Tuning generation 2: n_head\n",
      "## Tuning generation 2: n_embd\n",
      "## Tuning generation 2: max_iters\n",
      "## Tuning generation 2: max_epochs\n",
      "## Tuning generation 2: eval_iters\n",
      "## Tuning generation 2: log_interval\n",
      "## Tuning generation 2: eval_interval\n",
      "## Tuning generation 2: gradient_accumulation_steps\n",
      "## Tuning generation 2: batch_size\n",
      "## Tuning generation 2: learning_rate\n",
      "## Tuning generation 2: decay_lr\n",
      "## Tuning generation 2: decay_lr improved, val=False, best=2.4701898097991943, delta=-0.01332402229309082 elapsed=7.0309741497039795s delta=-0.04714798927307129s\n",
      "## Tuning generation 2: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 2: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 2: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0]\n",
      "## Tuning generation 2: weight_decay\n",
      "## Tuning generation 2: beta1\n",
      "## Tuning generation 2: beta2\n",
      "## Tuning generation 2: grad_clip\n",
      "## Tuning generation 2: dtype\n",
      "## Tuning generation 2: dropout\n",
      "## Tuning generation 2: bias\n",
      "## Tuning generation 3: model_name\n",
      "## Tuning generation 3: model_version\n",
      "## Tuning generation 3: n_max_context\n",
      "## Tuning generation 3: n_layer\n",
      "## Tuning generation 3: n_head\n",
      "## Tuning generation 3: n_head improved, val=2, best=2.4767191410064697, delta=-0.006529331207275391 elapsed=6.984344959259033s delta=-0.04662919044494629s\n",
      "## Tuning generation 3: n_embd\n",
      "## Tuning generation 3: max_iters\n",
      "## Tuning generation 3: max_epochs\n",
      "## Tuning generation 3: eval_iters\n",
      "## Tuning generation 3: log_interval\n",
      "## Tuning generation 3: eval_interval\n",
      "## Tuning generation 3: gradient_accumulation_steps\n",
      "## Tuning generation 3: batch_size\n",
      "## Tuning generation 3: batch_size improved, val=16, best=2.4693422317504883, delta=0.007376909255981445 elapsed=6.748286962509155s delta=-0.23605799674987793s\n",
      "## Tuning generation 3: learning_rate\n",
      "## Tuning generation 3: learning_rate improved, val=0.1, best=2.4723215103149414, delta=-0.002979278564453125 elapsed=6.459797143936157s delta=-0.28848981857299805s\n",
      "## Tuning generation 3: decay_lr\n",
      "## Tuning generation 3: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 3: min_lr\n",
      "## Computed tune options: min_lr = [0.01]\n",
      "## Tuning generation 3: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0]\n",
      "## Tuning generation 3: weight_decay\n",
      "## Tuning generation 3: beta1\n",
      "## Tuning generation 3: beta2\n",
      "## Tuning generation 3: grad_clip\n",
      "## Tuning generation 3: dtype\n",
      "## Tuning generation 3: dropout\n",
      "## Tuning generation 3: bias\n",
      "## Tuning generation 4: model_name\n",
      "## Tuning generation 4: model_version\n",
      "## Tuning generation 4: n_max_context\n",
      "## Tuning generation 4: n_layer\n",
      "## Tuning generation 4: n_head\n",
      "## Tuning generation 4: n_embd\n",
      "## Tuning generation 4: max_iters\n",
      "## Tuning generation 4: max_epochs\n",
      "## Tuning generation 4: eval_iters\n",
      "## Tuning generation 4: log_interval\n",
      "## Tuning generation 4: eval_interval\n",
      "## Tuning generation 4: gradient_accumulation_steps\n",
      "## Tuning generation 4: batch_size\n",
      "## Tuning generation 4: learning_rate\n",
      "## Tuning generation 4: decay_lr\n",
      "## Tuning generation 4: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 4: min_lr\n",
      "## Computed tune options: min_lr = [0.01]\n",
      "## Tuning generation 4: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0]\n",
      "## Tuning generation 4: weight_decay\n",
      "## Tuning generation 4: beta1\n",
      "## Tuning generation 4: beta2\n",
      "## Tuning generation 4: grad_clip\n",
      "## Tuning generation 4: dtype\n",
      "## Tuning generation 4: dropout\n",
      "## Tuning generation 4: bias\n",
      "No updates for generation 4, stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Pass as arg to Tuner instead of monkey patching...\n",
    "import rgi.rgizero.models.tuner\n",
    "rgi.rgizero.models.tuner.train_with = train_with\n",
    "\n",
    "initial_params = dict(\n",
    "    model_name='c4-tuning', model_version='0.1',\n",
    "    n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8,  # tiny model\n",
    "    max_iters=100, max_epochs=1_000_000, # Make max_epoch high, rely on max_iters to stop.\n",
    "    eval_iters = 200, log_interval = 200, eval_interval = 10_000,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 32, learning_rate = LEARNING_RATE,    \n",
    "    decay_lr = True,  # whether to decay the learning rate\n",
    "    lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    "\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    grad_clip = 1.0,  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = \"bfloat16\",\n",
    "\n",
    "    # block_size = block_size,\n",
    "    # vocab_size = action_vocab.vocab_size,  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    dropout = 0.0,\n",
    "    bias = False,  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    ")\n",
    "\n",
    "tune_options = {k: [v] for k, v in initial_params.items()}\n",
    "\n",
    "tune_options.update(dict(\n",
    "    batch_size = [16, 32, 64, 128, 256],\n",
    "    gradient_accumulation_steps = [1, 4, 16],\n",
    "    learning_rate = [0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "    lr_decay_iters = [100],\n",
    "    beta1 = [0.9],\n",
    "    beta2 = [0.95, 0.99],\n",
    "    warmup_iters = [0],\n",
    "    n_embd = [8, 16, 32, 64, 128],\n",
    "    n_layer = [2, 4, 8, 16, 32],\n",
    "    n_head = [2, 4, 8, 16, 32],\n",
    "    max_iters = [100, 300, 1_000, 3_000, 10_000, 30_000, 100_000],\n",
    "\n",
    "    dtype = [\"bfloat16\", \"float16\", \"float32\"],\n",
    "    # vocab_size = [action_vocab.vocab_size],\n",
    "    dropout = [0.0, 0.01, 0.05, 0.1],\n",
    "    bias = [True, False],\n",
    "    \n",
    "    decay_lr = [True, False],\n",
    "))\n",
    "\n",
    "computed_tune_options = dict(\n",
    "    min_lr = lambda opt: [opt['learning_rate'] / 10],\n",
    "    lr_decay_iters = lambda opt: [opt['max_iters']],\n",
    "    warmup_iters = lambda opt: [0, 100, 1000] if opt['decay_lr'] else [0],\n",
    ")\n",
    "\n",
    "TUNER_VERSION = \"0.0.2\"\n",
    "\n",
    "\n",
    "# TODO: We need to recalculate the 'calculated' options every time any hparam is changed...\n",
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=100.00)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: None\n",
      "## Initial Model, loss=2.4566147327423096 elapsed=13.30814504623413s\n",
      "## Tuning generation 1: model_name\n",
      "## Tuning generation 1: model_version\n",
      "## Tuning generation 1: n_max_context\n",
      "## Tuning generation 1: n_layer\n",
      "## Tuning generation 1: n_layer improved, val=4, best=2.4735591411590576, delta=-0.016944408416748047 elapsed=8.71258020401001s delta=-4.595564842224121s\n",
      "## Tuning generation 1: n_head\n",
      "## Tuning generation 1: n_head improved, val=4, best=2.4671905040740967, delta=0.0063686370849609375 elapsed=7.430401086807251s delta=-1.2821791172027588s\n",
      "## Tuning generation 1: n_embd\n",
      "## Tuning generation 1: max_iters\n",
      "## Tuning generation 1: max_epochs\n",
      "## Tuning generation 1: eval_iters\n",
      "## Tuning generation 1: log_interval\n",
      "## Tuning generation 1: eval_interval\n",
      "## Tuning generation 1: gradient_accumulation_steps\n",
      "## Tuning generation 1: batch_size\n",
      "## Tuning generation 1: learning_rate\n",
      "## Tuning generation 1: decay_lr\n",
      "## Tuning generation 1: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 1: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 1: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 1: weight_decay\n",
      "## Tuning generation 1: beta1\n",
      "## Tuning generation 1: beta2\n",
      "## Tuning generation 1: grad_clip\n",
      "## Tuning generation 1: dtype\n",
      "## Tuning generation 1: dropout\n",
      "## Tuning generation 1: bias\n",
      "## Tuning generation 2: model_name\n",
      "## Tuning generation 2: model_version\n",
      "## Tuning generation 2: n_max_context\n",
      "## Tuning generation 2: n_layer\n",
      "## Tuning generation 2: n_layer improved, val=2, best=2.4568657875061035, delta=0.010324716567993164 elapsed=7.078122138977051s delta=-0.3522789478302002s\n",
      "## Tuning generation 2: n_head\n",
      "## Tuning generation 2: n_embd\n",
      "## Tuning generation 2: max_iters\n",
      "## Tuning generation 2: max_epochs\n",
      "## Tuning generation 2: eval_iters\n",
      "## Tuning generation 2: log_interval\n",
      "## Tuning generation 2: eval_interval\n",
      "## Tuning generation 2: gradient_accumulation_steps\n",
      "## Tuning generation 2: batch_size\n",
      "## Tuning generation 2: learning_rate\n",
      "## Tuning generation 2: decay_lr\n",
      "## Tuning generation 2: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 2: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 2: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=100, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/100/100: loss 2.7801, time 4126.20ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 100, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: division by zero\n",
      "## Tuning generation 2: weight_decay\n",
      "## Tuning generation 2: beta1\n",
      "## Tuning generation 2: beta2\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/100/100: loss 2.7801, time 3241.02ms\n",
      "## train_loss: 2.4543, val_loss: 2.4579, Time taken: 8.06332015991211s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: grad_clip\n",
      "## Tuning generation 2: dtype\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/100/100: loss 2.7801, time 3219.84ms\n",
      "## train_loss: 2.4532, val_loss: 2.4569, Time taken: 8.28602910041809s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: dropout\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.01, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/100/100: loss 2.7802, time 2688.36ms\n",
      "## train_loss: 2.4599, val_loss: 2.4635, Time taken: 7.544525146484375s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.01, 'bias': False}\n",
      "## Tuning generation 2: bias\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=True)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 20, with 234 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7675, val loss 2.7675\n",
      "iter 0/100/100: loss 2.7674, time 2673.95ms\n",
      "## train_loss: 2.4639, val_loss: 2.4673, Time taken: 7.678985834121704s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': True}\n",
      "## Tuning generation 3: model_name\n",
      "## Tuning generation 3: model_version\n",
      "## Tuning generation 3: n_max_context\n",
      "## Tuning generation 3: n_layer\n",
      "## Tuning generation 3: n_head\n",
      "## Tuning generation 3: n_embd\n",
      "## Tuning generation 3: max_iters\n",
      "## Tuning generation 3: max_epochs\n",
      "## Tuning generation 3: eval_iters\n",
      "## Tuning generation 3: log_interval\n",
      "## Tuning generation 3: eval_interval\n",
      "## Tuning generation 3: gradient_accumulation_steps\n",
      "## Tuning generation 3: batch_size\n",
      "## Tuning generation 3: learning_rate\n",
      "## Tuning generation 3: decay_lr\n",
      "## Tuning generation 3: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 3: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 3: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 3: weight_decay\n",
      "## Tuning generation 3: beta1\n",
      "## Tuning generation 3: beta2\n",
      "## Tuning generation 3: grad_clip\n",
      "## Tuning generation 3: dtype\n",
      "## Tuning generation 3: dropout\n",
      "## Tuning generation 3: bias\n",
      "No updates for generation 3, stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=10.00)\n",
    "tuner.autotune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: None\n",
      "## Initial Model, loss=2.4566147327423096 elapsed=13.30814504623413s\n",
      "## Tuning generation 1: model_name\n",
      "## Tuning generation 1: model_version\n",
      "## Tuning generation 1: n_max_context\n",
      "## Tuning generation 1: n_layer\n",
      "## Tuning generation 1: n_layer improved, val=4, best=2.4735591411590576, delta=-0.016944408416748047 elapsed=8.71258020401001s delta=-4.595564842224121s\n",
      "## Tuning generation 1: n_head\n",
      "## Tuning generation 1: n_head improved, val=4, best=2.4671905040740967, delta=0.0063686370849609375 elapsed=7.430401086807251s delta=-1.2821791172027588s\n",
      "## Tuning generation 1: n_embd\n",
      "## Tuning generation 1: max_iters\n",
      "## Tuning generation 1: max_epochs\n",
      "## Tuning generation 1: eval_iters\n",
      "## Tuning generation 1: log_interval\n",
      "## Tuning generation 1: eval_interval\n",
      "## Tuning generation 1: gradient_accumulation_steps\n",
      "## Tuning generation 1: batch_size\n",
      "## Tuning generation 1: learning_rate\n",
      "## Tuning generation 1: decay_lr\n",
      "## Tuning generation 1: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 1: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 1: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 1: weight_decay\n",
      "## Tuning generation 1: beta1\n",
      "## Tuning generation 1: beta2\n",
      "## Tuning generation 1: grad_clip\n",
      "## Tuning generation 1: dtype\n",
      "## Tuning generation 1: dropout\n",
      "## Tuning generation 1: bias\n",
      "## Tuning generation 2: model_name\n",
      "## Tuning generation 2: model_version\n",
      "## Tuning generation 2: n_max_context\n",
      "## Tuning generation 2: n_layer\n",
      "## Tuning generation 2: n_layer improved, val=2, best=2.4568657875061035, delta=0.010324716567993164 elapsed=7.078122138977051s delta=-0.3522789478302002s\n",
      "## Tuning generation 2: n_head\n",
      "## Tuning generation 2: n_embd\n",
      "## Tuning generation 2: max_iters\n",
      "## Tuning generation 2: max_epochs\n",
      "## Tuning generation 2: eval_iters\n",
      "## Tuning generation 2: log_interval\n",
      "## Tuning generation 2: eval_interval\n",
      "## Tuning generation 2: gradient_accumulation_steps\n",
      "## Tuning generation 2: batch_size\n",
      "## Tuning generation 2: learning_rate\n",
      "## Tuning generation 2: decay_lr\n",
      "## Tuning generation 2: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 2: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 2: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 2: weight_decay\n",
      "## Tuning generation 2: beta1\n",
      "## Tuning generation 2: beta2\n",
      "## Tuning generation 2: grad_clip\n",
      "## Tuning generation 2: dtype\n",
      "## Tuning generation 2: dropout\n",
      "## Tuning generation 2: bias\n",
      "## Tuning generation 3: model_name\n",
      "## Tuning generation 3: model_version\n",
      "## Tuning generation 3: n_max_context\n",
      "## Tuning generation 3: n_layer\n",
      "## Tuning generation 3: n_head\n",
      "## Tuning generation 3: n_embd\n",
      "## Tuning generation 3: max_iters\n",
      "## Tuning generation 3: max_epochs\n",
      "## Tuning generation 3: eval_iters\n",
      "## Tuning generation 3: log_interval\n",
      "## Tuning generation 3: eval_interval\n",
      "## Tuning generation 3: gradient_accumulation_steps\n",
      "## Tuning generation 3: batch_size\n",
      "## Tuning generation 3: learning_rate\n",
      "## Tuning generation 3: decay_lr\n",
      "## Tuning generation 3: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [100]\n",
      "## Tuning generation 3: min_lr\n",
      "## Computed tune options: min_lr = [0.005]\n",
      "## Tuning generation 3: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "## Tuning generation 3: weight_decay\n",
      "## Tuning generation 3: beta1\n",
      "## Tuning generation 3: beta2\n",
      "## Tuning generation 3: grad_clip\n",
      "## Tuning generation 3: dtype\n",
      "## Tuning generation 3: dropout\n",
      "## Tuning generation 3: bias\n",
      "No updates for generation 3, stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=1.00)\n",
    "tuner.autotune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: None\n",
      "## Initial Model, loss=2.4566147327423096 elapsed=13.30814504623413s\n",
      "## Tuning generation 1: model_name\n",
      "## Tuning generation 1: model_version\n",
      "## Tuning generation 1: n_max_context\n",
      "## Tuning generation 1: n_layer\n",
      "## Tuning generation 1: n_head\n",
      "## Tuning generation 1: n_head improved, val=4, best=2.4568657875061035, delta=-0.0002510547637939453 elapsed=7.078122138977051s delta=-6.23002290725708s\n",
      "## Tuning generation 1: n_embd\n",
      "## Tuning generation 1: max_iters\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/1000/1000: loss 2.7801, time 2632.85ms\n",
      "iter 200/1000/1000: loss 2.4566, time 19.44ms\n",
      "iter 400/1000/1000: loss 2.4669, time 18.39ms\n",
      "iter 600/1000/1000: loss 2.2991, time 15.69ms\n",
      "iter 800/1000/1000: loss 2.4373, time 16.97ms\n",
      "iter 1000/1000/1000: loss 2.3962, time 19.59ms\n",
      "## train_loss: 2.4000, val_loss: 2.4017, Time taken: 23.149885892868042s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/3000/3000: loss 2.7801, time 2779.52ms\n",
      "iter 200/3000/3000: loss 2.4566, time 15.57ms\n",
      "iter 400/3000/3000: loss 2.4669, time 14.66ms\n",
      "iter 600/3000/3000: loss 2.2991, time 18.22ms\n",
      "iter 800/3000/3000: loss 2.4373, time 17.82ms\n",
      "iter 1000/3000/3000: loss 2.4024, time 15.21ms\n",
      "iter 1200/3000/3000: loss 2.3679, time 16.46ms\n",
      "iter 1400/3000/3000: loss 2.3685, time 19.81ms\n",
      "iter 1600/3000/3000: loss 2.3233, time 16.27ms\n",
      "iter 1800/3000/3000: loss 2.3673, time 18.56ms\n",
      "iter 2000/3000/3000: loss 2.4984, time 17.10ms\n",
      "iter 2200/3000/3000: loss 2.3550, time 18.47ms\n",
      "iter 2400/3000/3000: loss 2.3984, time 13.65ms\n",
      "iter 2600/3000/3000: loss 2.3717, time 16.84ms\n",
      "iter 2800/3000/3000: loss 2.3985, time 19.57ms\n",
      "iter 3000/3000/3000: loss 2.3248, time 27.78ms\n",
      "## train_loss: 2.3658, val_loss: 2.3659, Time taken: 57.403000831604004s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: max_iters improved, val=1000, best=2.4016551971435547, delta=0.05521059036254883 elapsed=23.149885892868042s delta=16.07176375389099s\n",
      "## Tuning generation 1: max_epochs\n",
      "## Tuning generation 1: eval_iters\n",
      "## Tuning generation 1: log_interval\n",
      "## Tuning generation 1: eval_interval\n",
      "## Tuning generation 1: gradient_accumulation_steps\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=4, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/1000/1000: loss 2.7793, time 2715.87ms\n",
      "iter 200/1000/1000: loss 2.4502, time 73.32ms\n",
      "iter 400/1000/1000: loss 2.3762, time 55.90ms\n",
      "iter 600/1000/1000: loss 2.4998, time 54.10ms\n",
      "iter 800/1000/1000: loss 2.3518, time 57.01ms\n",
      "iter 1000/1000/1000: loss 2.3952, time 56.36ms\n",
      "## train_loss: 2.3927, val_loss: 2.3946, Time taken: 68.4585509300232s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 4, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: batch_size\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3726.27ms\n",
      "iter 200/1000/1000: loss 2.4809, time 18.42ms\n",
      "iter 400/1000/1000: loss 2.3989, time 18.41ms\n",
      "iter 600/1000/1000: loss 2.4035, time 18.82ms\n",
      "iter 800/1000/1000: loss 2.3418, time 20.44ms\n",
      "iter 1000/1000/1000: loss 2.3379, time 24.06ms\n",
      "## train_loss: 2.3752, val_loss: 2.3764, Time taken: 26.967992067337036s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7818, time 35946.12ms\n",
      "iter 200/1000/1000: loss 2.4475, time 22.62ms\n",
      "iter 400/1000/1000: loss 2.3956, time 22.54ms\n",
      "iter 600/1000/1000: loss 2.4304, time 22.80ms\n",
      "iter 800/1000/1000: loss 2.3874, time 55.34ms\n",
      "iter 1000/1000/1000: loss 2.3239, time 24.65ms\n",
      "## train_loss: 2.3853, val_loss: 2.3830, Time taken: 79.7925820350647s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: batch_size improved, val=64, best=2.3764355182647705, delta=0.02521967887878418 elapsed=26.967992067337036s delta=3.818106174468994s\n",
      "## Tuning generation 1: learning_rate\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.1, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3864.26ms\n",
      "iter 200/1000/1000: loss 2.4681, time 20.27ms\n",
      "iter 400/1000/1000: loss 2.4748, time 17.89ms\n",
      "iter 600/1000/1000: loss 2.4271, time 29.19ms\n",
      "iter 800/1000/1000: loss 2.3815, time 19.07ms\n",
      "iter 1000/1000/1000: loss 2.3627, time 18.93ms\n",
      "## train_loss: 2.4037, val_loss: 2.4021, Time taken: 26.999999046325684s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.1, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3235.99ms\n",
      "iter 200/1000/1000: loss 2.4552, time 19.09ms\n",
      "iter 400/1000/1000: loss 2.3875, time 18.03ms\n",
      "iter 600/1000/1000: loss 2.4082, time 18.42ms\n",
      "iter 800/1000/1000: loss 2.3451, time 22.47ms\n",
      "iter 1000/1000/1000: loss 2.3412, time 20.71ms\n",
      "## train_loss: 2.3759, val_loss: 2.3764, Time taken: 25.873490810394287s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: learning_rate improved, val=0.01, best=2.376422882080078, delta=1.2636184692382812e-05 elapsed=25.873490810394287s delta=-1.094501256942749s\n",
      "## Tuning generation 1: decay_lr\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 2897.22ms\n",
      "iter 200/1000/1000: loss 2.4576, time 18.32ms\n",
      "iter 400/1000/1000: loss 2.3951, time 18.60ms\n",
      "iter 600/1000/1000: loss 2.4008, time 18.47ms\n",
      "iter 800/1000/1000: loss 2.3548, time 17.72ms\n",
      "iter 1000/1000/1000: loss 2.3282, time 21.35ms\n",
      "## train_loss: 2.3724, val_loss: 2.3729, Time taken: 25.25483798980713s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: decay_lr improved, val=False, best=2.372875928878784, delta=0.0035469532012939453 elapsed=25.25483798980713s delta=-0.6186528205871582s\n",
      "## Tuning generation 1: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [1000]\n",
      "## Tuning generation 1: min_lr\n",
      "## Computed tune options: min_lr = [0.001]\n",
      "## Tuning generation 1: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0]\n",
      "## Tuning generation 1: weight_decay\n",
      "## Tuning generation 1: beta1\n",
      "## Tuning generation 1: beta2\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 2919.03ms\n",
      "iter 200/1000/1000: loss 2.4493, time 18.15ms\n",
      "iter 400/1000/1000: loss 2.3988, time 18.55ms\n",
      "iter 600/1000/1000: loss 2.3986, time 22.31ms\n",
      "iter 800/1000/1000: loss 2.3616, time 19.26ms\n",
      "iter 1000/1000/1000: loss 2.3279, time 22.13ms\n",
      "## train_loss: 2.3710, val_loss: 2.3716, Time taken: 25.82448410987854s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: beta2 improved, val=0.99, best=2.371572256088257, delta=0.0013036727905273438 elapsed=25.82448410987854s delta=0.5696461200714111s\n",
      "## Tuning generation 1: grad_clip\n",
      "## Tuning generation 1: dtype\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 2923.78ms\n",
      "iter 200/1000/1000: loss 2.4493, time 19.50ms\n",
      "iter 400/1000/1000: loss 2.3988, time 18.03ms\n",
      "iter 600/1000/1000: loss 2.3986, time 17.88ms\n",
      "iter 800/1000/1000: loss 2.3616, time 25.72ms\n",
      "iter 1000/1000/1000: loss 2.3279, time 25.71ms\n",
      "## train_loss: 2.3711, val_loss: 2.3716, Time taken: 25.409729957580566s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3135.95ms\n",
      "iter 200/1000/1000: loss 2.4493, time 30.91ms\n",
      "iter 400/1000/1000: loss 2.3988, time 19.25ms\n",
      "iter 600/1000/1000: loss 2.3986, time 20.44ms\n",
      "iter 800/1000/1000: loss 2.3616, time 18.44ms\n",
      "iter 1000/1000/1000: loss 2.3279, time 17.83ms\n",
      "## train_loss: 2.3710, val_loss: 2.3715, Time taken: 25.919113874435425s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float32', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: dtype improved, val=float16, best=2.3715856075286865, delta=-1.33514404296875e-05 elapsed=25.409729957580566s delta=-0.41475415229797363s\n",
      "## Tuning generation 1: dropout\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.01, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7837, time 3433.44ms\n",
      "iter 200/1000/1000: loss 2.4688, time 17.43ms\n",
      "iter 400/1000/1000: loss 2.4141, time 20.00ms\n",
      "iter 600/1000/1000: loss 2.4158, time 21.45ms\n",
      "iter 800/1000/1000: loss 2.3627, time 17.76ms\n",
      "iter 1000/1000/1000: loss 2.3323, time 22.13ms\n",
      "## train_loss: 2.3760, val_loss: 2.3770, Time taken: 27.230495929718018s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.01, 'bias': False}\n",
      "## Tuning generation 1: bias\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=True)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 20, with 234 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7676, val loss 2.7675\n",
      "iter 0/1000/1000: loss 2.7681, time 4239.14ms\n",
      "iter 200/1000/1000: loss 2.4486, time 20.48ms\n",
      "iter 400/1000/1000: loss 2.3450, time 20.52ms\n",
      "iter 600/1000/1000: loss 2.3907, time 19.78ms\n",
      "iter 800/1000/1000: loss 2.3661, time 30.74ms\n",
      "iter 1000/1000/1000: loss 2.3474, time 22.04ms\n",
      "## train_loss: 2.3729, val_loss: 2.3749, Time taken: 31.279248237609863s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': True}\n",
      "## Tuning generation 2: model_name\n",
      "## Tuning generation 2: model_version\n",
      "## Tuning generation 2: n_max_context\n",
      "## Tuning generation 2: n_layer\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 19, with 3,504 parameters\n",
      "num non-decayed parameter tensors: 11, with 82 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7768, val loss 2.7764\n",
      "iter 0/1000/1000: loss 2.7772, time 3177.17ms\n",
      "iter 200/1000/1000: loss 2.4871, time 28.68ms\n",
      "iter 400/1000/1000: loss 2.3755, time 23.11ms\n",
      "iter 600/1000/1000: loss 2.3521, time 24.62ms\n",
      "iter 800/1000/1000: loss 2.2952, time 23.29ms\n",
      "iter 1000/1000/1000: loss 2.4099, time 21.47ms\n",
      "## train_loss: 2.3758, val_loss: 2.3813, Time taken: 32.83165097236633s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 4, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: n_head\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3085.37ms\n",
      "iter 200/1000/1000: loss 2.4541, time 16.04ms\n",
      "iter 400/1000/1000: loss 2.3865, time 19.90ms\n",
      "iter 600/1000/1000: loss 2.4080, time 14.84ms\n",
      "iter 800/1000/1000: loss 2.3454, time 24.23ms\n",
      "iter 1000/1000/1000: loss 2.3271, time 21.83ms\n",
      "## train_loss: 2.3751, val_loss: 2.3760, Time taken: 25.865479946136475s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=2, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3045.48ms\n",
      "iter 200/1000/1000: loss 2.4378, time 17.32ms\n",
      "iter 400/1000/1000: loss 2.4045, time 20.66ms\n",
      "iter 600/1000/1000: loss 2.4071, time 19.14ms\n",
      "iter 800/1000/1000: loss 2.3666, time 18.06ms\n",
      "iter 1000/1000/1000: loss 2.3347, time 20.36ms\n",
      "## train_loss: 2.3768, val_loss: 2.3766, Time taken: 24.91559910774231s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 2, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: n_embd\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=16, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 7,008 parameters\n",
      "num non-decayed parameter tensors: 7, with 90 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7644, val loss 2.7644\n",
      "iter 0/1000/1000: loss 2.7590, time 4010.52ms\n",
      "iter 200/1000/1000: loss 2.3775, time 19.04ms\n",
      "iter 400/1000/1000: loss 2.4314, time 17.66ms\n",
      "iter 600/1000/1000: loss 2.4014, time 19.13ms\n",
      "iter 800/1000/1000: loss 2.4277, time 15.24ms\n",
      "iter 1000/1000/1000: loss 2.3556, time 21.69ms\n",
      "## train_loss: 2.3703, val_loss: 2.3696, Time taken: 28.59390425682068s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 16, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: max_iters\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/3000/3000: loss 2.7836, time 3262.02ms\n",
      "iter 200/3000/3000: loss 2.4493, time 18.50ms\n",
      "iter 400/3000/3000: loss 2.3988, time 18.71ms\n",
      "iter 600/3000/3000: loss 2.3986, time 19.37ms\n",
      "iter 800/3000/3000: loss 2.3616, time 15.78ms\n",
      "iter 1000/3000/3000: loss 2.4471, time 20.27ms\n",
      "iter 1200/3000/3000: loss 2.3795, time 16.73ms\n",
      "iter 1400/3000/3000: loss 2.3627, time 19.42ms\n",
      "iter 1600/3000/3000: loss 2.4115, time 19.02ms\n",
      "iter 1800/3000/3000: loss 2.3424, time 19.26ms\n",
      "iter 2000/3000/3000: loss 2.3847, time 18.70ms\n",
      "iter 2200/3000/3000: loss 2.3049, time 20.37ms\n",
      "iter 2400/3000/3000: loss 2.3629, time 18.88ms\n",
      "iter 2600/3000/3000: loss 2.3648, time 18.88ms\n",
      "iter 2800/3000/3000: loss 2.3916, time 18.55ms\n",
      "iter 3000/3000/3000: loss 2.3067, time 21.78ms\n",
      "## train_loss: 2.3648, val_loss: 2.3615, Time taken: 66.48829102516174s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=300, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/300/300: loss 2.7836, time 2845.79ms\n",
      "iter 200/300/300: loss 2.4493, time 17.93ms\n",
      "## train_loss: 2.4036, val_loss: 2.4031, Time taken: 11.850450992584229s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 300, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: max_epochs\n",
      "## Tuning generation 2: eval_iters\n",
      "## Tuning generation 2: log_interval\n",
      "## Tuning generation 2: eval_interval\n",
      "## Tuning generation 2: gradient_accumulation_steps\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=4, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7803, time 2843.42ms\n",
      "iter 200/1000/1000: loss 2.4123, time 83.19ms\n",
      "iter 400/1000/1000: loss 2.3730, time 95.09ms\n",
      "iter 600/1000/1000: loss 2.3389, time 84.77ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 4, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}: \n",
      "## Tuning generation 2: batch_size\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7818, time 4479.19ms\n",
      "iter 200/1000/1000: loss 2.4416, time 17.70ms\n",
      "iter 400/1000/1000: loss 2.3770, time 22.37ms\n",
      "iter 600/1000/1000: loss 2.4244, time 21.72ms\n",
      "iter 800/1000/1000: loss 2.3849, time 22.44ms\n",
      "iter 1000/1000/1000: loss 2.3201, time 25.14ms\n",
      "## train_loss: 2.3791, val_loss: 2.3766, Time taken: 31.829303979873657s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/1000/1000: loss 2.7801, time 3245.32ms\n",
      "iter 200/1000/1000: loss 2.4546, time 16.80ms\n",
      "iter 400/1000/1000: loss 2.4551, time 17.38ms\n",
      "iter 600/1000/1000: loss 2.3294, time 16.90ms\n",
      "iter 800/1000/1000: loss 2.4448, time 15.82ms\n",
      "iter 1000/1000/1000: loss 2.3821, time 19.32ms\n",
      "## train_loss: 2.3960, val_loss: 2.3962, Time taken: 23.44884490966797s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: learning_rate\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3133.48ms\n",
      "iter 200/1000/1000: loss 2.4428, time 19.39ms\n",
      "iter 400/1000/1000: loss 2.4820, time 18.98ms\n",
      "iter 600/1000/1000: loss 2.4192, time 17.50ms\n",
      "iter 800/1000/1000: loss 2.3852, time 21.46ms\n",
      "iter 1000/1000/1000: loss 2.3502, time 18.44ms\n",
      "## train_loss: 2.4089, val_loss: 2.4056, Time taken: 26.329823970794678s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.05, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: decay_lr\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 2890.08ms\n",
      "iter 200/1000/1000: loss 2.4577, time 20.34ms\n",
      "iter 400/1000/1000: loss 2.3996, time 18.37ms\n",
      "iter 600/1000/1000: loss 2.4106, time 19.00ms\n",
      "iter 800/1000/1000: loss 2.3507, time 19.41ms\n",
      "iter 1000/1000/1000: loss 2.3400, time 20.46ms\n",
      "## train_loss: 2.3794, val_loss: 2.3803, Time taken: 25.58227276802063s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [1000]\n",
      "## Tuning generation 2: min_lr\n",
      "## Computed tune options: min_lr = [0.001]\n",
      "## Tuning generation 2: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0]\n",
      "## Tuning generation 2: weight_decay\n",
      "## Tuning generation 2: beta1\n",
      "## Tuning generation 2: beta2\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.01, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/1000/1000: loss 2.7836, time 3257.83ms\n",
      "iter 200/1000/1000: loss 2.4576, time 17.91ms\n",
      "iter 400/1000/1000: loss 2.3951, time 17.90ms\n",
      "iter 600/1000/1000: loss 2.4008, time 28.93ms\n",
      "iter 800/1000/1000: loss 2.3548, time 18.49ms\n",
      "iter 1000/1000/1000: loss 2.3282, time 20.84ms\n",
      "## train_loss: 2.3724, val_loss: 2.3729, Time taken: 25.701690912246704s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: grad_clip\n",
      "## Tuning generation 2: dtype\n",
      "## Tuning generation 2: dropout\n",
      "## Tuning generation 2: bias\n",
      "No updates for generation 2, stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.10)\n",
    "tuner.autotune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: None\n",
      "## Initial Model, loss=2.4566147327423096 elapsed=13.30814504623413s\n",
      "## Tuning generation 1: model_name\n",
      "## Tuning generation 1: model_version\n",
      "## Tuning generation 1: n_max_context\n",
      "## Tuning generation 1: n_layer\n",
      "## Tuning generation 1: n_head\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=16, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 16, 'n_embd': 8, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: \n",
      "## Tuning generation 1: n_head improved, val=8, best=2.4566547870635986, delta=-4.00543212890625e-05 elapsed=7.352856874465942s delta=-5.9552881717681885s\n",
      "## Tuning generation 1: n_embd\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=16, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=100, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 7,008 parameters\n",
      "num non-decayed parameter tensors: 7, with 90 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7642, val loss 2.7649\n",
      "iter 0/100/100: loss 2.7630, time 3372.23ms\n",
      "## train_loss: 2.4722, val_loss: 2.4672, Time taken: 7.936033010482788s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 16, 'max_iters': 100, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: max_iters\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=300, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/300/300: loss 2.7801, time 3054.33ms\n",
      "iter 200/300/300: loss 2.4666, time 18.46ms\n",
      "## train_loss: 2.4374, val_loss: 2.4388, Time taken: 11.571418046951294s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 300, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=1000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/1000/1000: loss 2.7801, time 2645.99ms\n",
      "iter 200/1000/1000: loss 2.4666, time 15.62ms\n",
      "iter 400/1000/1000: loss 2.4612, time 15.50ms\n",
      "iter 600/1000/1000: loss 2.2952, time 14.29ms\n",
      "iter 800/1000/1000: loss 2.4495, time 17.84ms\n",
      "iter 1000/1000/1000: loss 2.4116, time 19.78ms\n",
      "## train_loss: 2.4102, val_loss: 2.4134, Time taken: 22.883904933929443s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 1000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/3000/3000: loss 2.7801, time 2842.93ms\n",
      "iter 200/3000/3000: loss 2.4666, time 15.34ms\n",
      "iter 400/3000/3000: loss 2.4612, time 18.30ms\n",
      "iter 600/3000/3000: loss 2.2952, time 14.78ms\n",
      "iter 800/3000/3000: loss 2.4495, time 15.07ms\n",
      "iter 1000/3000/3000: loss 2.4098, time 18.20ms\n",
      "iter 1200/3000/3000: loss 2.4021, time 16.89ms\n",
      "iter 1400/3000/3000: loss 2.3917, time 17.38ms\n",
      "iter 1600/3000/3000: loss 2.3353, time 31.65ms\n",
      "iter 1800/3000/3000: loss 2.3453, time 34.24ms\n",
      "iter 2000/3000/3000: loss 2.4888, time 21.19ms\n",
      "iter 2200/3000/3000: loss 2.3569, time 16.89ms\n",
      "iter 2400/3000/3000: loss 2.3962, time 11.45ms\n",
      "iter 2600/3000/3000: loss 2.3729, time 17.64ms\n",
      "iter 2800/3000/3000: loss 2.3980, time 17.31ms\n",
      "iter 3000/3000/3000: loss 2.3417, time 19.95ms\n",
      "## train_loss: 2.3653, val_loss: 2.3671, Time taken: 58.82357120513916s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/6250/10000: loss 2.7801, time 2773.26ms\n",
      "iter 200/6250/10000: loss 2.4666, time 17.97ms\n",
      "iter 400/6250/10000: loss 2.4612, time 17.56ms\n",
      "iter 600/6250/10000: loss 2.2952, time 17.64ms\n",
      "iter 800/6250/10000: loss 2.4495, time 17.83ms\n",
      "iter 1000/6250/10000: loss 2.4098, time 15.66ms\n",
      "iter 1200/6250/10000: loss 2.4021, time 18.08ms\n",
      "iter 1400/6250/10000: loss 2.3917, time 15.26ms\n",
      "iter 1600/6250/10000: loss 2.3353, time 16.81ms\n",
      "iter 1800/6250/10000: loss 2.3453, time 17.54ms\n",
      "iter 2000/6250/10000: loss 2.4886, time 15.76ms\n",
      "iter 2200/6250/10000: loss 2.3569, time 18.04ms\n",
      "iter 2400/6250/10000: loss 2.3962, time 13.96ms\n",
      "iter 2600/6250/10000: loss 2.3730, time 18.53ms\n",
      "iter 2800/6250/10000: loss 2.3981, time 20.09ms\n",
      "iter 3000/6250/10000: loss 2.4240, time 15.78ms\n",
      "iter 3200/6250/10000: loss 2.4479, time 20.28ms\n",
      "iter 3400/6250/10000: loss 2.3181, time 19.22ms\n",
      "iter 3600/6250/10000: loss 2.3789, time 15.46ms\n",
      "iter 3800/6250/10000: loss 2.3889, time 17.68ms\n",
      "iter 4000/6250/10000: loss 2.3807, time 17.75ms\n",
      "iter 4200/6250/10000: loss 2.3492, time 18.11ms\n",
      "iter 4400/6250/10000: loss 2.3677, time 13.78ms\n",
      "iter 4600/6250/10000: loss 2.5219, time 15.74ms\n",
      "iter 4800/6250/10000: loss 2.3085, time 18.47ms\n",
      "iter 5000/6250/10000: loss 2.3762, time 14.44ms\n",
      "iter 5200/6250/10000: loss 2.2391, time 16.85ms\n",
      "iter 5400/6250/10000: loss 2.3776, time 14.70ms\n",
      "iter 5600/6250/10000: loss 2.3500, time 17.22ms\n",
      "iter 5800/6250/10000: loss 2.3707, time 14.37ms\n",
      "iter 6000/6250/10000: loss 2.3736, time 15.82ms\n",
      "iter 6200/6250/10000: loss 2.3856, time 13.08ms\n",
      "iter 6400/10000/10000: loss 2.3604, time 14.87ms\n",
      "iter 6600/10000/10000: loss 2.2858, time 20.38ms\n",
      "iter 6800/10000/10000: loss 2.2598, time 19.29ms\n",
      "iter 7000/10000/10000: loss 2.4682, time 15.30ms\n",
      "iter 7200/10000/10000: loss 2.3211, time 17.95ms\n",
      "iter 7400/10000/10000: loss 2.2643, time 17.44ms\n",
      "iter 7600/10000/10000: loss 2.2457, time 14.19ms\n",
      "iter 7800/10000/10000: loss 2.3518, time 16.22ms\n",
      "iter 8000/10000/10000: loss 2.4037, time 17.06ms\n",
      "iter 8200/10000/10000: loss 2.3877, time 14.71ms\n",
      "iter 8400/10000/10000: loss 2.3961, time 16.58ms\n",
      "iter 8600/10000/10000: loss 2.4002, time 18.43ms\n",
      "iter 8800/10000/10000: loss 2.2940, time 13.96ms\n",
      "iter 9000/10000/10000: loss 2.3465, time 18.53ms\n",
      "iter 9200/10000/10000: loss 2.4387, time 14.72ms\n",
      "iter 9400/10000/10000: loss 2.3874, time 17.72ms\n",
      "iter 9600/10000/10000: loss 2.2454, time 21.34ms\n",
      "iter 9800/10000/10000: loss 2.2845, time 30.13ms\n",
      "step 10000: train loss 2.3712, val loss 2.3717\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.4189, time 2495.98ms\n",
      "## train_loss: 2.3639, val_loss: 2.3603, Time taken: 185.90822005271912s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: max_iters improved, val=3000, best=2.367056369781494, delta=0.08959841728210449 elapsed=58.82357120513916s delta=51.47071433067322s\n",
      "## Tuning generation 1: max_epochs\n",
      "## Tuning generation 1: eval_iters\n",
      "## Tuning generation 1: log_interval\n",
      "## Tuning generation 1: eval_interval\n",
      "## Tuning generation 1: gradient_accumulation_steps\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=4, batch_size=32, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7803\n",
      "iter 0/3000/3000: loss 2.7793, time 2641.58ms\n",
      "iter 200/3000/3000: loss 2.3763, time 58.57ms\n",
      "iter 400/3000/3000: loss 2.3677, time 51.19ms\n",
      "iter 600/3000/3000: loss 2.4775, time 52.98ms\n",
      "iter 800/3000/3000: loss 2.3463, time 55.99ms\n",
      "iter 1000/3000/3000: loss 2.3882, time 105.76ms\n",
      "iter 1200/3000/3000: loss 2.4182, time 58.17ms\n",
      "iter 1400/3000/3000: loss 2.3247, time 60.63ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 4, 'batch_size': 32, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: \n",
      "## Tuning generation 1: batch_size\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=64, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7805, val loss 2.7804\n",
      "iter 0/3000/3000: loss 2.7836, time 3196.78ms\n",
      "iter 200/3000/3000: loss 2.4592, time 20.09ms\n",
      "iter 400/3000/3000: loss 2.4043, time 16.38ms\n",
      "iter 600/3000/3000: loss 2.4047, time 19.20ms\n",
      "iter 800/3000/3000: loss 2.3612, time 18.56ms\n",
      "iter 1000/3000/3000: loss 2.4401, time 23.21ms\n",
      "iter 1200/3000/3000: loss 2.3801, time 22.24ms\n",
      "iter 1400/3000/3000: loss 2.3677, time 19.98ms\n",
      "iter 1600/3000/3000: loss 2.4117, time 19.68ms\n",
      "iter 1800/3000/3000: loss 2.3374, time 19.51ms\n",
      "iter 2000/3000/3000: loss 2.3821, time 15.06ms\n",
      "iter 2200/3000/3000: loss 2.3099, time 21.26ms\n",
      "iter 2400/3000/3000: loss 2.3997, time 19.48ms\n",
      "iter 2600/3000/3000: loss 2.3609, time 20.01ms\n",
      "iter 2800/3000/3000: loss 2.3793, time 15.63ms\n",
      "iter 3000/3000/3000: loss 2.3006, time 26.06ms\n",
      "## train_loss: 2.3540, val_loss: 2.3539, Time taken: 66.0193178653717s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 64, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 4629.90ms\n",
      "iter 200/1563/3000: loss 2.4501, time 24.15ms\n",
      "iter 400/1563/3000: loss 2.3892, time 26.86ms\n",
      "iter 600/1563/3000: loss 2.4389, time 23.92ms\n",
      "iter 800/1563/3000: loss 2.3914, time 24.04ms\n",
      "iter 1000/1563/3000: loss 2.4045, time 26.46ms\n",
      "iter 1200/1563/3000: loss 2.3975, time 24.18ms\n",
      "iter 1400/1563/3000: loss 2.3521, time 27.42ms\n",
      "iter 1600/3000/3000: loss 2.3490, time 17.56ms\n",
      "iter 1800/3000/3000: loss 2.3945, time 25.68ms\n",
      "iter 2000/3000/3000: loss 2.3436, time 18.39ms\n",
      "iter 2200/3000/3000: loss 2.3736, time 26.87ms\n",
      "iter 2400/3000/3000: loss 2.3708, time 16.86ms\n",
      "iter 2600/3000/3000: loss 2.3491, time 24.33ms\n",
      "iter 2800/3000/3000: loss 2.3537, time 23.14ms\n",
      "iter 3000/3000/3000: loss 2.3563, time 25.30ms\n",
      "## train_loss: 2.3487, val_loss: 2.3502, Time taken: 87.53910708427429s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.05, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/782/3000: loss 2.7813, time 58618.99ms\n",
      "iter 200/782/3000: loss 2.4440, time 30.97ms\n",
      "iter 400/782/3000: loss 2.3951, time 28.59ms\n",
      "iter 600/782/3000: loss 2.3911, time 20.84ms\n",
      "iter 800/1564/3000: loss 2.3749, time 21.70ms\n",
      "iter 1000/1564/3000: loss 2.3751, time 23.02ms\n",
      "iter 1200/1564/3000: loss 2.3604, time 27.33ms\n",
      "iter 1400/1564/3000: loss 2.3678, time 25.17ms\n",
      "iter 1600/2346/3000: loss 2.3912, time 21.91ms\n",
      "iter 1800/2346/3000: loss 2.3506, time 21.54ms\n",
      "iter 2000/2346/3000: loss 2.3165, time 25.82ms\n",
      "iter 2200/2346/3000: loss 2.3481, time 22.84ms\n",
      "iter 2400/3000/3000: loss 2.3768, time 29.06ms\n",
      "iter 2600/3000/3000: loss 2.3432, time 19.82ms\n",
      "iter 2800/3000/3000: loss 2.3556, time 27.95ms\n",
      "iter 3000/3000/3000: loss 2.4210, time 30.82ms\n",
      "## train_loss: 2.3550, val_loss: 2.3512, Time taken: 210.2355601787567s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: batch_size improved, val=128, best=2.3502447605133057, delta=0.016811609268188477 elapsed=87.53910708427429s delta=28.715535879135132s\n",
      "## Tuning generation 1: learning_rate\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.1, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3718.42ms\n",
      "iter 200/1563/3000: loss 2.4509, time 17.50ms\n",
      "iter 400/1563/3000: loss 2.3870, time 22.46ms\n",
      "iter 600/1563/3000: loss 2.4484, time 23.65ms\n",
      "iter 800/1563/3000: loss 2.3798, time 21.01ms\n",
      "iter 1000/1563/3000: loss 2.4067, time 16.30ms\n",
      "iter 1200/1563/3000: loss 2.4162, time 24.60ms\n",
      "iter 1400/1563/3000: loss 2.3726, time 17.27ms\n",
      "iter 1600/3000/3000: loss 2.3732, time 18.48ms\n",
      "iter 1800/3000/3000: loss 2.4114, time 17.36ms\n",
      "iter 2000/3000/3000: loss 2.3509, time 17.80ms\n",
      "iter 2200/3000/3000: loss 2.3761, time 26.38ms\n",
      "iter 2400/3000/3000: loss 2.3770, time 23.02ms\n",
      "iter 2600/3000/3000: loss 2.3832, time 20.27ms\n",
      "iter 2800/3000/3000: loss 2.3805, time 20.91ms\n",
      "iter 3000/3000/3000: loss 2.3756, time 26.31ms\n",
      "## train_loss: 2.3595, val_loss: 2.3604, Time taken: 70.36316800117493s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.1, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3658.46ms\n",
      "iter 200/1563/3000: loss 2.4361, time 17.43ms\n",
      "iter 400/1563/3000: loss 2.3780, time 16.94ms\n",
      "iter 600/1563/3000: loss 2.4267, time 20.34ms\n",
      "iter 800/1563/3000: loss 2.3889, time 21.52ms\n",
      "iter 1000/1563/3000: loss 2.3962, time 17.10ms\n",
      "iter 1200/1563/3000: loss 2.4097, time 23.00ms\n",
      "iter 1400/1563/3000: loss 2.3560, time 17.40ms\n",
      "iter 1600/3000/3000: loss 2.3461, time 32.85ms\n",
      "iter 1800/3000/3000: loss 2.4006, time 26.89ms\n",
      "iter 2000/3000/3000: loss 2.3325, time 20.38ms\n",
      "iter 2200/3000/3000: loss 2.3680, time 17.68ms\n",
      "iter 2400/3000/3000: loss 2.3497, time 23.67ms\n",
      "iter 2600/3000/3000: loss 2.3457, time 28.40ms\n",
      "iter 2800/3000/3000: loss 2.3661, time 23.48ms\n",
      "iter 3000/3000/3000: loss 2.3534, time 27.18ms\n",
      "## train_loss: 2.3459, val_loss: 2.3480, Time taken: 70.31881427764893s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: learning_rate improved, val=0.01, best=2.347978353500366, delta=0.002266407012939453 elapsed=70.31881427764893s delta=-17.220292806625366s\n",
      "## Tuning generation 1: decay_lr\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3681.97ms\n",
      "iter 200/1563/3000: loss 2.4092, time 18.84ms\n",
      "iter 400/1563/3000: loss 2.3724, time 16.33ms\n",
      "iter 600/1563/3000: loss 2.4194, time 23.12ms\n",
      "iter 800/1563/3000: loss 2.3840, time 26.87ms\n",
      "iter 1000/1563/3000: loss 2.3931, time 21.78ms\n",
      "iter 1200/1563/3000: loss 2.3976, time 17.43ms\n",
      "iter 1400/1563/3000: loss 2.3413, time 18.70ms\n",
      "iter 1600/3000/3000: loss 2.3393, time 16.73ms\n",
      "iter 1800/3000/3000: loss 2.4079, time 17.19ms\n",
      "iter 2000/3000/3000: loss 2.3359, time 19.37ms\n",
      "iter 2200/3000/3000: loss 2.3621, time 17.46ms\n",
      "iter 2400/3000/3000: loss 2.3466, time 18.63ms\n",
      "iter 2600/3000/3000: loss 2.3553, time 18.52ms\n",
      "iter 2800/3000/3000: loss 2.3522, time 17.37ms\n",
      "iter 3000/3000/3000: loss 2.3437, time 25.64ms\n",
      "## train_loss: 2.3546, val_loss: 2.3562, Time taken: 70.7153742313385s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [3000]\n",
      "## Tuning generation 1: min_lr\n",
      "## Computed tune options: min_lr = [0.001]\n",
      "## Tuning generation 1: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=100, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3840.64ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 100, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: division by zero\n",
      "## Tuning generation 1: weight_decay\n",
      "## Tuning generation 1: beta1\n",
      "## Tuning generation 1: beta2\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3490.60ms\n",
      "iter 200/1563/3000: loss 2.4432, time 16.93ms\n",
      "iter 400/1563/3000: loss 2.3811, time 20.22ms\n",
      "iter 600/1563/3000: loss 2.4246, time 17.52ms\n",
      "iter 800/1563/3000: loss 2.3904, time 17.87ms\n",
      "iter 1000/1563/3000: loss 2.4069, time 19.19ms\n",
      "iter 1200/1563/3000: loss 2.4030, time 16.94ms\n",
      "iter 1400/1563/3000: loss 2.3447, time 25.30ms\n",
      "iter 1600/3000/3000: loss 2.3458, time 23.21ms\n",
      "iter 1800/3000/3000: loss 2.4100, time 18.74ms\n",
      "iter 2000/3000/3000: loss 2.3320, time 16.95ms\n",
      "iter 2200/3000/3000: loss 2.3602, time 22.82ms\n",
      "iter 2400/3000/3000: loss 2.3498, time 25.42ms\n",
      "iter 2600/3000/3000: loss 2.3581, time 27.90ms\n",
      "iter 2800/3000/3000: loss 2.3669, time 25.02ms\n",
      "iter 3000/3000/3000: loss 2.3509, time 19.92ms\n",
      "## train_loss: 2.3503, val_loss: 2.3518, Time taken: 70.13734102249146s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: grad_clip\n",
      "## Tuning generation 1: dtype\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3510.72ms\n",
      "iter 200/1563/3000: loss 2.4361, time 17.74ms\n",
      "iter 400/1563/3000: loss 2.3780, time 17.67ms\n",
      "iter 600/1563/3000: loss 2.4267, time 16.21ms\n",
      "iter 800/1563/3000: loss 2.3889, time 25.30ms\n",
      "iter 1000/1563/3000: loss 2.3962, time 17.80ms\n",
      "iter 1200/1563/3000: loss 2.4097, time 18.11ms\n",
      "iter 1400/1563/3000: loss 2.3560, time 18.62ms\n",
      "iter 1600/3000/3000: loss 2.3461, time 23.58ms\n",
      "iter 1800/3000/3000: loss 2.4006, time 21.81ms\n",
      "iter 2000/3000/3000: loss 2.3325, time 20.67ms\n",
      "iter 2200/3000/3000: loss 2.3680, time 25.19ms\n",
      "iter 2400/3000/3000: loss 2.3497, time 17.43ms\n",
      "iter 2600/3000/3000: loss 2.3457, time 17.94ms\n",
      "iter 2800/3000/3000: loss 2.3661, time 24.15ms\n",
      "iter 3000/3000/3000: loss 2.3534, time 20.17ms\n",
      "## train_loss: 2.3459, val_loss: 2.3480, Time taken: 70.67614197731018s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 1: dropout\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.01, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7820, time 3906.89ms\n",
      "iter 200/1563/3000: loss 2.4409, time 29.07ms\n",
      "iter 400/1563/3000: loss 2.3746, time 26.49ms\n",
      "iter 600/1563/3000: loss 2.4284, time 20.55ms\n",
      "iter 800/1563/3000: loss 2.3868, time 18.85ms\n",
      "iter 1000/1563/3000: loss 2.4000, time 17.51ms\n",
      "iter 1200/1563/3000: loss 2.4035, time 19.99ms\n",
      "iter 1400/1563/3000: loss 2.3552, time 19.49ms\n",
      "iter 1600/3000/3000: loss 2.3517, time 24.59ms\n",
      "iter 1800/3000/3000: loss 2.3992, time 26.63ms\n",
      "iter 2000/3000/3000: loss 2.3394, time 18.02ms\n",
      "iter 2200/3000/3000: loss 2.3659, time 22.93ms\n",
      "iter 2400/3000/3000: loss 2.3626, time 28.30ms\n",
      "iter 2600/3000/3000: loss 2.3668, time 25.28ms\n",
      "iter 2800/3000/3000: loss 2.3571, time 18.18ms\n",
      "iter 3000/3000/3000: loss 2.3817, time 21.08ms\n",
      "## train_loss: 2.3571, val_loss: 2.3583, Time taken: 73.88284111022949s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.01, 'bias': False}\n",
      "## Tuning generation 1: bias\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=8, dropout=0.0, bias=True)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 20, with 234 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7675, val loss 2.7675\n",
      "iter 0/1563/3000: loss 2.7673, time 5750.17ms\n",
      "iter 200/1563/3000: loss 2.4043, time 26.58ms\n",
      "iter 400/1563/3000: loss 2.4059, time 26.57ms\n",
      "iter 600/1563/3000: loss 2.3942, time 20.51ms\n",
      "iter 800/1563/3000: loss 2.3408, time 29.70ms\n",
      "iter 1000/1563/3000: loss 2.3696, time 23.47ms\n",
      "iter 1200/1563/3000: loss 2.3774, time 27.03ms\n",
      "iter 1400/1563/3000: loss 2.3348, time 26.09ms\n",
      "iter 1600/3000/3000: loss 2.4271, time 20.94ms\n",
      "iter 1800/3000/3000: loss 2.3788, time 30.26ms\n",
      "iter 2000/3000/3000: loss 2.3425, time 21.45ms\n",
      "iter 2200/3000/3000: loss 2.3180, time 24.42ms\n",
      "iter 2400/3000/3000: loss 2.3165, time 27.87ms\n",
      "iter 2600/3000/3000: loss 2.3234, time 24.42ms\n",
      "iter 2800/3000/3000: loss 2.4077, time 26.68ms\n",
      "iter 3000/3000/3000: loss 2.3275, time 27.69ms\n",
      "## train_loss: 2.3519, val_loss: 2.3538, Time taken: 93.17727017402649s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': True}\n",
      "## Tuning generation 2: model_name\n",
      "## Tuning generation 2: model_version\n",
      "## Tuning generation 2: n_max_context\n",
      "## Tuning generation 2: n_layer\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=4, n_head=8, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 3,504 parameters\n",
      "num non-decayed parameter tensors: 11, with 82 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7767, val loss 2.7764\n",
      "iter 0/1563/3000: loss 2.7764, time 4152.40ms\n",
      "iter 200/1563/3000: loss 2.3920, time 25.77ms\n",
      "iter 400/1563/3000: loss 2.3509, time 28.26ms\n",
      "iter 600/1563/3000: loss 2.3408, time 27.12ms\n",
      "iter 800/1563/3000: loss 2.3355, time 22.32ms\n",
      "iter 1000/1563/3000: loss 2.4000, time 21.70ms\n",
      "iter 1200/1563/3000: loss 2.4674, time 22.00ms\n",
      "iter 1400/1563/3000: loss 2.2843, time 36.30ms\n",
      "iter 1600/3000/3000: loss 2.3860, time 30.06ms\n",
      "iter 1800/3000/3000: loss 2.3296, time 30.23ms\n",
      "iter 2000/3000/3000: loss 2.3810, time 17.74ms\n",
      "iter 2200/3000/3000: loss 2.4050, time 33.27ms\n",
      "iter 2400/3000/3000: loss 2.3870, time 25.91ms\n",
      "iter 2600/3000/3000: loss 2.3532, time 27.82ms\n",
      "iter 2800/3000/3000: loss 2.3478, time 24.47ms\n",
      "iter 3000/3000/3000: loss 2.3027, time 26.91ms\n",
      "## train_loss: 2.3546, val_loss: 2.3567, Time taken: 94.34753704071045s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 4, 'n_head': 8, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: n_head\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=16, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 16, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: \n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=4, n_embd=8, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 1,968 parameters\n",
      "num non-decayed parameter tensors: 7, with 50 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7803, val loss 2.7804\n",
      "iter 0/1563/3000: loss 2.7818, time 3953.21ms\n",
      "iter 200/1563/3000: loss 2.4427, time 21.17ms\n",
      "iter 400/1563/3000: loss 2.3834, time 20.86ms\n",
      "iter 600/1563/3000: loss 2.4229, time 24.00ms\n",
      "iter 800/1563/3000: loss 2.3816, time 20.50ms\n",
      "iter 1000/1563/3000: loss 2.3996, time 22.10ms\n",
      "iter 1200/1563/3000: loss 2.3895, time 24.31ms\n",
      "iter 1400/1563/3000: loss 2.3479, time 22.55ms\n",
      "iter 1600/3000/3000: loss 2.3328, time 17.43ms\n",
      "iter 1800/3000/3000: loss 2.4000, time 21.61ms\n",
      "iter 2000/3000/3000: loss 2.3339, time 35.33ms\n",
      "iter 2200/3000/3000: loss 2.3710, time 20.60ms\n",
      "iter 2400/3000/3000: loss 2.3583, time 23.84ms\n",
      "iter 2600/3000/3000: loss 2.3457, time 18.43ms\n",
      "iter 2800/3000/3000: loss 2.3627, time 17.72ms\n",
      "iter 3000/3000/3000: loss 2.3485, time 23.56ms\n",
      "## train_loss: 2.3487, val_loss: 2.3495, Time taken: 71.54539608955383s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: n_embd\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=16, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 7,008 parameters\n",
      "num non-decayed parameter tensors: 7, with 90 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7643, val loss 2.7641\n",
      "iter 0/1563/3000: loss 2.7623, time 5216.12ms\n",
      "iter 200/1563/3000: loss 2.3973, time 28.03ms\n",
      "iter 400/1563/3000: loss 2.3824, time 19.03ms\n",
      "iter 600/1563/3000: loss 2.3603, time 17.71ms\n",
      "iter 800/1563/3000: loss 2.3265, time 18.01ms\n",
      "iter 1000/1563/3000: loss 2.3704, time 24.03ms\n",
      "iter 1200/1563/3000: loss 2.3356, time 23.19ms\n",
      "iter 1400/1563/3000: loss 2.3470, time 19.06ms\n",
      "iter 1600/3000/3000: loss 2.3448, time 35.34ms\n",
      "iter 1800/3000/3000: loss 2.2887, time 24.09ms\n",
      "iter 2000/3000/3000: loss 2.3511, time 24.01ms\n",
      "iter 2200/3000/3000: loss 2.2905, time 14.60ms\n",
      "iter 2400/3000/3000: loss 2.2975, time 17.80ms\n",
      "iter 2600/3000/3000: loss 2.2820, time 26.97ms\n",
      "iter 2800/3000/3000: loss 2.2679, time 17.23ms\n",
      "iter 3000/3000/3000: loss 2.3169, time 19.20ms\n",
      "## train_loss: 2.3074, val_loss: 2.3029, Time taken: 79.03981709480286s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 16, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=32, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 26,304 parameters\n",
      "num non-decayed parameter tensors: 7, with 170 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7673, val loss 2.7662\n",
      "iter 0/1563/3000: loss 2.7661, time 4825.71ms\n",
      "iter 200/1563/3000: loss 2.3703, time 22.90ms\n",
      "iter 400/1563/3000: loss 2.3769, time 21.45ms\n",
      "iter 600/1563/3000: loss 2.3812, time 17.74ms\n",
      "iter 800/1563/3000: loss 2.4293, time 26.87ms\n",
      "iter 1000/1563/3000: loss 2.2882, time 25.23ms\n",
      "iter 1200/1563/3000: loss 2.3202, time 20.03ms\n",
      "iter 1400/1563/3000: loss 2.2821, time 23.98ms\n",
      "iter 1600/3000/3000: loss 2.2367, time 26.29ms\n",
      "iter 1800/3000/3000: loss 2.2855, time 21.51ms\n",
      "iter 2000/3000/3000: loss 2.2770, time 22.14ms\n",
      "iter 2200/3000/3000: loss 2.2779, time 20.72ms\n",
      "iter 2400/3000/3000: loss 2.3000, time 22.40ms\n",
      "iter 2600/3000/3000: loss 2.2485, time 17.79ms\n",
      "iter 2800/3000/3000: loss 2.2146, time 16.89ms\n",
      "iter 3000/3000/3000: loss 2.3318, time 25.80ms\n",
      "## train_loss: 2.2762, val_loss: 2.2759, Time taken: 80.02912735939026s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 32, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7810, val loss 2.7807\n",
      "iter 0/1563/3000: loss 2.7783, time 5001.98ms\n",
      "iter 200/1563/3000: loss 2.4490, time 22.71ms\n",
      "iter 400/1563/3000: loss 2.3708, time 26.18ms\n",
      "iter 600/1563/3000: loss 2.3538, time 23.68ms\n",
      "iter 800/1563/3000: loss 2.3953, time 20.29ms\n",
      "iter 1000/1563/3000: loss 2.3142, time 20.37ms\n",
      "iter 1200/1563/3000: loss 2.3358, time 22.06ms\n",
      "iter 1400/1563/3000: loss 2.2565, time 25.30ms\n",
      "iter 1600/3000/3000: loss 2.3245, time 20.71ms\n",
      "iter 1800/3000/3000: loss 2.3569, time 19.53ms\n",
      "iter 2000/3000/3000: loss 2.4090, time 19.26ms\n",
      "iter 2200/3000/3000: loss 2.3299, time 22.90ms\n",
      "iter 2400/3000/3000: loss 2.2850, time 20.17ms\n",
      "iter 2600/3000/3000: loss 2.2910, time 20.84ms\n",
      "iter 2800/3000/3000: loss 2.2324, time 19.43ms\n",
      "iter 3000/3000/3000: loss 2.2823, time 32.54ms\n",
      "## train_loss: 2.2598, val_loss: 2.2671, Time taken: 79.03133606910706s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=128, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=3000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 400,128 parameters\n",
      "num non-decayed parameter tensors: 7, with 650 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.8670, val loss 2.8664\n",
      "iter 0/1563/3000: loss 2.8673, time 5594.49ms\n",
      "iter 200/1563/3000: loss 2.4558, time 21.34ms\n",
      "iter 400/1563/3000: loss 2.4161, time 26.11ms\n",
      "iter 600/1563/3000: loss 2.4070, time 24.64ms\n",
      "iter 800/1563/3000: loss 2.3553, time 23.22ms\n",
      "iter 1000/1563/3000: loss 2.3635, time 20.97ms\n",
      "iter 1200/1563/3000: loss 2.3489, time 15.94ms\n",
      "iter 1400/1563/3000: loss 2.3485, time 23.07ms\n",
      "iter 1600/3000/3000: loss 2.2710, time 25.72ms\n",
      "iter 1800/3000/3000: loss 2.3708, time 23.61ms\n",
      "iter 2000/3000/3000: loss 2.3494, time 19.71ms\n",
      "iter 2200/3000/3000: loss 2.3324, time 19.25ms\n",
      "iter 2400/3000/3000: loss 2.3215, time 22.31ms\n",
      "iter 2600/3000/3000: loss 2.3721, time 27.38ms\n",
      "iter 2800/3000/3000: loss 2.2665, time 24.37ms\n",
      "iter 3000/3000/3000: loss 2.2624, time 22.11ms\n",
      "## train_loss: 2.3166, val_loss: 2.3121, Time taken: 79.82923007011414s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 128, 'max_iters': 3000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: n_embd improved, val=64, best=2.2670912742614746, delta=0.0808870792388916 elapsed=79.03133606910706s delta=8.71252179145813s\n",
      "## Tuning generation 2: max_iters\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7810, val loss 2.7807\n",
      "iter 0/1563/10000: loss 2.7783, time 3847.36ms\n",
      "iter 200/1563/10000: loss 2.4491, time 20.65ms\n",
      "iter 400/1563/10000: loss 2.3705, time 23.40ms\n",
      "iter 600/1563/10000: loss 2.3533, time 22.79ms\n",
      "iter 800/1563/10000: loss 2.3921, time 35.04ms\n",
      "iter 1000/1563/10000: loss 2.3168, time 20.95ms\n",
      "iter 1200/1563/10000: loss 2.3462, time 19.38ms\n",
      "iter 1400/1563/10000: loss 2.2591, time 23.57ms\n",
      "iter 1600/3126/10000: loss 2.3255, time 20.34ms\n",
      "iter 1800/3126/10000: loss 2.3510, time 20.83ms\n",
      "iter 2000/3126/10000: loss 2.4015, time 22.95ms\n",
      "iter 2200/3126/10000: loss 2.3322, time 20.14ms\n",
      "iter 2400/3126/10000: loss 2.2862, time 28.58ms\n",
      "iter 2600/3126/10000: loss 2.2867, time 19.91ms\n",
      "iter 2800/3126/10000: loss 2.2268, time 17.61ms\n",
      "iter 3000/3126/10000: loss 2.2838, time 29.71ms\n",
      "iter 3200/4689/10000: loss 2.1936, time 25.17ms\n",
      "iter 3400/4689/10000: loss 2.2987, time 25.58ms\n",
      "iter 3600/4689/10000: loss 2.2533, time 20.20ms\n",
      "iter 3800/4689/10000: loss 2.2099, time 20.25ms\n",
      "iter 4000/4689/10000: loss 2.2258, time 23.44ms\n",
      "iter 4200/4689/10000: loss 2.3077, time 19.60ms\n",
      "iter 4400/4689/10000: loss 2.2271, time 20.56ms\n",
      "iter 4600/4689/10000: loss 2.2503, time 21.16ms\n",
      "iter 4800/6252/10000: loss 2.2098, time 16.05ms\n",
      "iter 5000/6252/10000: loss 2.2454, time 31.91ms\n",
      "iter 5200/6252/10000: loss 2.2500, time 21.67ms\n",
      "iter 5400/6252/10000: loss 2.2711, time 21.50ms\n",
      "iter 5600/6252/10000: loss 2.1744, time 25.52ms\n",
      "iter 5800/6252/10000: loss 2.2131, time 20.23ms\n",
      "iter 6000/6252/10000: loss 2.2071, time 22.98ms\n",
      "iter 6200/6252/10000: loss 2.2328, time 19.47ms\n",
      "iter 6400/7815/10000: loss 2.2225, time 20.75ms\n",
      "iter 6600/7815/10000: loss 2.2092, time 26.69ms\n",
      "iter 6800/7815/10000: loss 2.2490, time 25.52ms\n",
      "iter 7000/7815/10000: loss 2.2414, time 23.40ms\n",
      "iter 7200/7815/10000: loss 2.2298, time 19.30ms\n",
      "iter 7400/7815/10000: loss 2.1887, time 20.39ms\n",
      "iter 7600/7815/10000: loss 2.2240, time 18.16ms\n",
      "iter 7800/7815/10000: loss 2.1963, time 19.58ms\n",
      "iter 8000/9378/10000: loss 2.2378, time 21.93ms\n",
      "iter 8200/9378/10000: loss 2.1556, time 21.01ms\n",
      "iter 8400/9378/10000: loss 2.2741, time 20.84ms\n",
      "iter 8600/9378/10000: loss 2.2256, time 23.16ms\n",
      "iter 8800/9378/10000: loss 2.2929, time 26.41ms\n",
      "iter 9000/9378/10000: loss 2.2136, time 23.15ms\n",
      "iter 9200/9378/10000: loss 2.2299, time 89.54ms\n",
      "iter 9400/10000/10000: loss 2.1587, time 19.27ms\n",
      "iter 9600/10000/10000: loss 2.2035, time 25.29ms\n",
      "iter 9800/10000/10000: loss 2.1861, time 21.30ms\n",
      "step 10000: train loss 2.1953, val loss 2.1977\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.2104, time 4028.75ms\n",
      "## train_loss: 2.1896, val_loss: 2.1937, Time taken: 243.14845395088196s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=30000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7810, val loss 2.7807\n",
      "iter 0/1563/30000: loss 2.7783, time 4006.21ms\n",
      "iter 200/1563/30000: loss 2.4492, time 26.15ms\n",
      "iter 400/1563/30000: loss 2.3691, time 29.89ms\n",
      "iter 600/1563/30000: loss 2.3588, time 19.14ms\n",
      "iter 800/1563/30000: loss 2.3943, time 20.71ms\n",
      "iter 1000/1563/30000: loss 2.3148, time 24.39ms\n",
      "iter 1200/1563/30000: loss 2.3376, time 23.86ms\n",
      "iter 1400/1563/30000: loss 2.2572, time 23.71ms\n",
      "iter 1600/3126/30000: loss 2.3187, time 20.42ms\n",
      "iter 1800/3126/30000: loss 2.3514, time 20.42ms\n",
      "iter 2000/3126/30000: loss 2.4085, time 23.57ms\n",
      "iter 2200/3126/30000: loss 2.3253, time 21.28ms\n",
      "iter 2400/3126/30000: loss 2.2960, time 25.95ms\n",
      "iter 2600/3126/30000: loss 2.2938, time 19.98ms\n",
      "iter 2800/3126/30000: loss 2.2276, time 18.22ms\n",
      "iter 3000/3126/30000: loss 2.2761, time 24.11ms\n",
      "iter 3200/4689/30000: loss 2.2042, time 25.13ms\n",
      "iter 3400/4689/30000: loss 2.3002, time 22.39ms\n",
      "iter 3600/4689/30000: loss 2.2482, time 20.15ms\n",
      "iter 3800/4689/30000: loss 2.2059, time 20.67ms\n",
      "iter 4000/4689/30000: loss 2.2217, time 22.82ms\n",
      "iter 4200/4689/30000: loss 2.3147, time 18.53ms\n",
      "iter 4400/4689/30000: loss 2.2286, time 31.48ms\n",
      "iter 4600/4689/30000: loss 2.2461, time 24.00ms\n",
      "iter 4800/6252/30000: loss 2.2107, time 18.38ms\n",
      "iter 5000/6252/30000: loss 2.2497, time 20.39ms\n",
      "iter 5200/6252/30000: loss 2.2556, time 20.22ms\n",
      "iter 5400/6252/30000: loss 2.2601, time 21.80ms\n",
      "iter 5600/6252/30000: loss 2.1743, time 23.58ms\n",
      "iter 5800/6252/30000: loss 2.2270, time 24.73ms\n",
      "iter 6000/6252/30000: loss 2.2133, time 20.09ms\n",
      "iter 6200/6252/30000: loss 2.2386, time 19.99ms\n",
      "iter 6400/7815/30000: loss 2.2321, time 23.64ms\n",
      "iter 6600/7815/30000: loss 2.2128, time 23.06ms\n",
      "iter 6800/7815/30000: loss 2.2513, time 23.90ms\n",
      "iter 7000/7815/30000: loss 2.2368, time 19.23ms\n",
      "iter 7200/7815/30000: loss 2.2322, time 20.57ms\n",
      "iter 7400/7815/30000: loss 2.1884, time 30.76ms\n",
      "iter 7600/7815/30000: loss 2.2270, time 16.67ms\n",
      "iter 7800/7815/30000: loss 2.2065, time 23.70ms\n",
      "iter 8000/9378/30000: loss 2.2261, time 20.30ms\n",
      "iter 8200/9378/30000: loss 2.1551, time 20.74ms\n",
      "iter 8400/9378/30000: loss 2.2649, time 20.20ms\n",
      "iter 8600/9378/30000: loss 2.2339, time 23.05ms\n",
      "iter 8800/9378/30000: loss 2.2891, time 21.99ms\n",
      "iter 9000/9378/30000: loss 2.1974, time 20.11ms\n",
      "iter 9200/9378/30000: loss 2.2390, time 22.24ms\n",
      "iter 9400/10941/30000: loss 2.1488, time 25.07ms\n",
      "iter 9600/10941/30000: loss 2.2307, time 24.34ms\n",
      "iter 9800/10941/30000: loss 2.1966, time 24.60ms\n",
      "step 10000: train loss 2.1987, val loss 2.1979\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10941/30000: loss 2.2525, time 5190.22ms\n",
      "iter 10200/10941/30000: loss 2.2684, time 24.78ms\n",
      "iter 10400/10941/30000: loss 2.1960, time 16.33ms\n",
      "iter 10600/10941/30000: loss 2.1603, time 23.04ms\n",
      "iter 10800/10941/30000: loss 2.2114, time 20.28ms\n",
      "iter 11000/12504/30000: loss 2.2251, time 22.54ms\n",
      "iter 11200/12504/30000: loss 2.1710, time 21.81ms\n",
      "iter 11400/12504/30000: loss 2.1676, time 19.23ms\n",
      "iter 11600/12504/30000: loss 2.2095, time 24.64ms\n",
      "iter 11800/12504/30000: loss 2.1469, time 20.05ms\n",
      "iter 12000/12504/30000: loss 2.1984, time 20.72ms\n",
      "iter 12200/12504/30000: loss 2.2413, time 19.61ms\n",
      "iter 12400/12504/30000: loss 2.2706, time 19.65ms\n",
      "iter 12600/14067/30000: loss 2.2105, time 23.22ms\n",
      "iter 12800/14067/30000: loss 2.2096, time 24.71ms\n",
      "iter 13000/14067/30000: loss 2.2025, time 21.62ms\n",
      "iter 13200/14067/30000: loss 2.2185, time 20.07ms\n",
      "iter 13400/14067/30000: loss 2.1729, time 20.95ms\n",
      "iter 13600/14067/30000: loss 2.1776, time 23.38ms\n",
      "iter 13800/14067/30000: loss 2.2091, time 26.29ms\n",
      "iter 14000/14067/30000: loss 2.2124, time 24.51ms\n",
      "iter 14200/15630/30000: loss 2.2449, time 24.27ms\n",
      "iter 14400/15630/30000: loss 2.1996, time 22.55ms\n",
      "iter 14600/15630/30000: loss 2.1508, time 20.59ms\n",
      "iter 14800/15630/30000: loss 2.1909, time 19.73ms\n",
      "iter 15000/15630/30000: loss 2.1314, time 20.51ms\n",
      "iter 15200/15630/30000: loss 2.1804, time 20.34ms\n",
      "iter 15400/15630/30000: loss 2.2147, time 19.10ms\n",
      "iter 15600/15630/30000: loss 2.1716, time 23.37ms\n",
      "iter 15800/17193/30000: loss 2.2101, time 24.22ms\n",
      "iter 16000/17193/30000: loss 2.2244, time 20.45ms\n",
      "iter 16200/17193/30000: loss 2.1386, time 21.26ms\n",
      "iter 16400/17193/30000: loss 2.1992, time 18.59ms\n",
      "iter 16600/17193/30000: loss 2.1436, time 23.12ms\n",
      "iter 16800/17193/30000: loss 2.2379, time 21.32ms\n",
      "iter 17000/17193/30000: loss 2.2744, time 20.64ms\n",
      "iter 17200/18756/30000: loss 2.1250, time 19.97ms\n",
      "iter 17400/18756/30000: loss 2.2402, time 19.74ms\n",
      "iter 17600/18756/30000: loss 2.1324, time 24.65ms\n",
      "iter 17800/18756/30000: loss 2.2221, time 19.75ms\n",
      "iter 18000/18756/30000: loss 2.1195, time 20.69ms\n",
      "iter 18200/18756/30000: loss 2.1534, time 22.33ms\n",
      "iter 18400/18756/30000: loss 2.1896, time 19.86ms\n",
      "iter 18600/18756/30000: loss 2.1889, time 21.17ms\n",
      "iter 18800/20319/30000: loss 2.1724, time 25.00ms\n",
      "iter 19000/20319/30000: loss 2.1995, time 21.00ms\n",
      "iter 19200/20319/30000: loss 2.2352, time 20.06ms\n",
      "iter 19400/20319/30000: loss 2.2376, time 23.45ms\n",
      "iter 19600/20319/30000: loss 2.1609, time 25.61ms\n",
      "iter 19800/20319/30000: loss 2.1597, time 26.24ms\n",
      "step 20000: train loss 2.1848, val loss 2.1825\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 20000/20319/30000: loss 2.2194, time 3864.43ms\n",
      "iter 20200/20319/30000: loss 2.1216, time 20.60ms\n",
      "iter 20400/21882/30000: loss 2.2015, time 16.38ms\n",
      "iter 20600/21882/30000: loss 2.2463, time 19.32ms\n",
      "iter 20800/21882/30000: loss 2.2085, time 23.90ms\n",
      "iter 21000/21882/30000: loss 2.2245, time 30.23ms\n",
      "iter 21200/21882/30000: loss 2.2273, time 20.89ms\n",
      "iter 21400/21882/30000: loss 2.1713, time 23.82ms\n",
      "iter 21600/21882/30000: loss 2.2217, time 20.78ms\n",
      "iter 21800/21882/30000: loss 2.1360, time 25.68ms\n",
      "iter 22000/23445/30000: loss 2.1368, time 19.79ms\n",
      "iter 22200/23445/30000: loss 2.1873, time 20.14ms\n",
      "iter 22400/23445/30000: loss 2.1556, time 20.45ms\n",
      "iter 22600/23445/30000: loss 2.2146, time 22.89ms\n",
      "iter 22800/23445/30000: loss 2.1836, time 27.02ms\n",
      "iter 23000/23445/30000: loss 2.1848, time 17.04ms\n",
      "iter 23200/23445/30000: loss 2.2251, time 20.98ms\n",
      "iter 23400/23445/30000: loss 2.2145, time 24.26ms\n",
      "iter 23600/25008/30000: loss 2.1889, time 21.33ms\n",
      "iter 23800/25008/30000: loss 2.1694, time 21.99ms\n",
      "iter 24000/25008/30000: loss 2.1607, time 19.96ms\n",
      "iter 24200/25008/30000: loss 2.1711, time 20.21ms\n",
      "iter 24400/25008/30000: loss 2.2630, time 23.28ms\n",
      "iter 24600/25008/30000: loss 2.2148, time 25.37ms\n",
      "iter 24800/25008/30000: loss 2.1944, time 19.21ms\n",
      "iter 25000/25008/30000: loss 2.2743, time 22.02ms\n",
      "iter 25200/26571/30000: loss 2.2160, time 20.32ms\n",
      "iter 25400/26571/30000: loss 2.1719, time 24.45ms\n",
      "iter 25600/26571/30000: loss 2.1817, time 26.52ms\n",
      "iter 25800/26571/30000: loss 2.1847, time 21.80ms\n",
      "iter 26000/26571/30000: loss 2.1424, time 19.99ms\n",
      "iter 26200/26571/30000: loss 2.1191, time 18.83ms\n",
      "iter 26400/26571/30000: loss 2.2192, time 24.52ms\n",
      "iter 26600/28134/30000: loss 2.1931, time 24.98ms\n",
      "iter 26800/28134/30000: loss 2.2110, time 20.15ms\n",
      "iter 27000/28134/30000: loss 2.1631, time 24.22ms\n",
      "iter 27200/28134/30000: loss 2.1397, time 19.05ms\n",
      "iter 27400/28134/30000: loss 2.1904, time 21.84ms\n",
      "iter 27600/28134/30000: loss 2.1774, time 26.35ms\n",
      "iter 27800/28134/30000: loss 2.1987, time 21.39ms\n",
      "iter 28000/28134/30000: loss 2.1592, time 28.39ms\n",
      "iter 28200/29697/30000: loss 2.1580, time 21.37ms\n",
      "iter 28400/29697/30000: loss 2.1796, time 24.92ms\n",
      "iter 28600/29697/30000: loss 2.1418, time 27.02ms\n",
      "iter 28800/29697/30000: loss 2.1735, time 21.94ms\n",
      "iter 29000/29697/30000: loss 2.1557, time 20.42ms\n",
      "iter 29200/29697/30000: loss 2.2290, time 20.35ms\n",
      "iter 29400/29697/30000: loss 2.1658, time 31.16ms\n",
      "iter 29600/29697/30000: loss 2.1622, time 20.15ms\n",
      "iter 29800/30000/30000: loss 2.1234, time 20.56ms\n",
      "step 30000: train loss 2.1789, val loss 2.1835\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 30000/30000/30000: loss 2.1821, time 4112.20ms\n",
      "## train_loss: 2.1807, val_loss: 2.1793, Time taken: 699.0081942081451s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 30000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: max_iters improved, val=10000, best=2.193652868270874, delta=0.07343840599060059 elapsed=243.14845395088196s delta=164.1171178817749s\n",
      "## Tuning generation 2: max_epochs\n",
      "## Tuning generation 2: eval_iters\n",
      "## Tuning generation 2: log_interval\n",
      "## Tuning generation 2: eval_interval\n",
      "## Tuning generation 2: gradient_accumulation_steps\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=4, batch_size=128, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7810, val loss 2.7807\n",
      "iter 0/1563/10000: loss 2.7908, time 4260.09ms\n",
      "iter 200/1563/10000: loss 2.2680, time 78.64ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 4, 'batch_size': 128, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: \n",
      "## Tuning generation 2: batch_size\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 8593.45ms\n",
      "iter 200/782/10000: loss 2.4018, time 26.60ms\n",
      "iter 400/782/10000: loss 2.3631, time 26.04ms\n",
      "iter 600/782/10000: loss 2.3424, time 26.98ms\n",
      "iter 800/1564/10000: loss 2.3270, time 28.99ms\n",
      "iter 1000/1564/10000: loss 2.3746, time 23.77ms\n",
      "iter 1200/1564/10000: loss 2.2829, time 25.86ms\n",
      "iter 1400/1564/10000: loss 2.2776, time 30.48ms\n",
      "iter 1600/2346/10000: loss 2.2520, time 24.12ms\n",
      "iter 1800/2346/10000: loss 2.2151, time 23.04ms\n",
      "iter 2000/2346/10000: loss 2.3241, time 27.46ms\n",
      "iter 2200/2346/10000: loss 2.2416, time 25.79ms\n",
      "iter 2400/3128/10000: loss 2.2374, time 23.83ms\n",
      "iter 2600/3128/10000: loss 2.1913, time 28.58ms\n",
      "iter 2800/3128/10000: loss 2.2460, time 29.25ms\n",
      "iter 3000/3128/10000: loss 2.2267, time 30.27ms\n",
      "iter 3200/3910/10000: loss 2.1949, time 29.11ms\n",
      "iter 3400/3910/10000: loss 2.2208, time 26.52ms\n",
      "iter 3600/3910/10000: loss 2.2113, time 26.55ms\n",
      "iter 3800/3910/10000: loss 2.2379, time 24.88ms\n",
      "iter 4000/4692/10000: loss 2.2186, time 28.36ms\n",
      "iter 4200/4692/10000: loss 2.2146, time 23.68ms\n",
      "iter 4400/4692/10000: loss 2.2727, time 23.61ms\n",
      "iter 4600/4692/10000: loss 2.1636, time 34.55ms\n",
      "iter 4800/5474/10000: loss 2.1803, time 26.73ms\n",
      "iter 5000/5474/10000: loss 2.2352, time 30.03ms\n",
      "iter 5200/5474/10000: loss 2.2000, time 24.30ms\n",
      "iter 5400/5474/10000: loss 2.2159, time 26.26ms\n",
      "iter 5600/6256/10000: loss 2.1812, time 23.62ms\n",
      "iter 5800/6256/10000: loss 2.1735, time 30.03ms\n",
      "iter 6000/6256/10000: loss 2.1744, time 27.84ms\n",
      "iter 6200/6256/10000: loss 2.1466, time 26.49ms\n",
      "iter 6400/7038/10000: loss 2.2071, time 28.08ms\n",
      "iter 6600/7038/10000: loss 2.1795, time 24.60ms\n",
      "iter 6800/7038/10000: loss 2.1490, time 26.91ms\n",
      "iter 7000/7038/10000: loss 2.2060, time 24.70ms\n",
      "iter 7200/7820/10000: loss 2.1738, time 24.30ms\n",
      "iter 7400/7820/10000: loss 2.1723, time 25.24ms\n",
      "iter 7600/7820/10000: loss 2.1679, time 28.11ms\n",
      "iter 7800/7820/10000: loss 2.1763, time 33.89ms\n",
      "iter 8000/8602/10000: loss 2.1814, time 25.53ms\n",
      "iter 8200/8602/10000: loss 2.2000, time 30.98ms\n",
      "iter 8400/8602/10000: loss 2.1787, time 24.08ms\n",
      "iter 8600/8602/10000: loss 2.2148, time 25.19ms\n",
      "iter 8800/9384/10000: loss 2.1262, time 23.88ms\n",
      "iter 9000/9384/10000: loss 2.2159, time 56.05ms\n",
      "iter 9200/9384/10000: loss 2.2307, time 33.91ms\n",
      "iter 9400/10000/10000: loss 2.2206, time 24.19ms\n",
      "iter 9600/10000/10000: loss 2.1284, time 26.50ms\n",
      "iter 9800/10000/10000: loss 2.1825, time 27.39ms\n",
      "step 10000: train loss 2.1779, val loss 2.1758\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1708, time 7177.44ms\n",
      "## train_loss: 2.1792, val_loss: 2.1801, Time taken: 315.0719769001007s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: batch_size improved, val=256, best=2.180067539215088, delta=0.013585329055786133 elapsed=315.0719769001007s delta=71.92352294921875s\n",
      "## Tuning generation 2: learning_rate\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.05, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 5354.15ms\n",
      "iter 200/782/10000: loss 2.4562, time 32.25ms\n",
      "iter 400/782/10000: loss 2.4233, time 27.11ms\n",
      "iter 600/782/10000: loss 2.4407, time 26.41ms\n",
      "iter 800/1564/10000: loss 2.4016, time 28.74ms\n",
      "iter 1000/1564/10000: loss 2.4467, time 24.51ms\n",
      "iter 1200/1564/10000: loss 2.3277, time 28.53ms\n",
      "iter 1400/1564/10000: loss 2.3548, time 27.53ms\n",
      "iter 1600/2346/10000: loss 2.3152, time 26.09ms\n",
      "iter 1800/2346/10000: loss 2.3010, time 26.94ms\n",
      "iter 2000/2346/10000: loss 2.3880, time 27.00ms\n",
      "iter 2200/2346/10000: loss 2.3117, time 25.77ms\n",
      "iter 2400/3128/10000: loss 2.3230, time 27.71ms\n",
      "iter 2600/3128/10000: loss 2.2610, time 25.37ms\n",
      "iter 2800/3128/10000: loss 2.3023, time 24.69ms\n",
      "iter 3000/3128/10000: loss 2.3093, time 25.12ms\n",
      "iter 3200/3910/10000: loss 2.2688, time 25.35ms\n",
      "iter 3400/3910/10000: loss 2.2606, time 24.30ms\n",
      "iter 3600/3910/10000: loss 2.2812, time 29.14ms\n",
      "iter 3800/3910/10000: loss 2.2842, time 25.74ms\n",
      "iter 4000/4692/10000: loss 2.2729, time 23.78ms\n",
      "iter 4200/4692/10000: loss 2.2564, time 26.40ms\n",
      "iter 4400/4692/10000: loss 2.3066, time 23.76ms\n",
      "iter 4600/4692/10000: loss 2.2104, time 32.42ms\n",
      "iter 4800/5474/10000: loss 2.2243, time 25.22ms\n",
      "iter 5000/5474/10000: loss 2.2819, time 32.82ms\n",
      "iter 5200/5474/10000: loss 2.2598, time 33.54ms\n",
      "iter 5400/5474/10000: loss 2.2355, time 26.13ms\n",
      "iter 5600/6256/10000: loss 2.2305, time 25.69ms\n",
      "iter 5800/6256/10000: loss 2.2098, time 31.18ms\n",
      "iter 6000/6256/10000: loss 2.2061, time 27.28ms\n",
      "iter 6200/6256/10000: loss 2.1797, time 26.43ms\n",
      "iter 6400/7038/10000: loss 2.2296, time 27.41ms\n",
      "iter 6600/7038/10000: loss 2.2085, time 24.42ms\n",
      "iter 6800/7038/10000: loss 2.1820, time 27.77ms\n",
      "iter 7000/7038/10000: loss 2.2189, time 28.02ms\n",
      "iter 7200/7820/10000: loss 2.2093, time 25.40ms\n",
      "iter 7400/7820/10000: loss 2.1930, time 24.96ms\n",
      "iter 7600/7820/10000: loss 2.1898, time 28.66ms\n",
      "iter 7800/7820/10000: loss 2.2001, time 25.06ms\n",
      "iter 8000/8602/10000: loss 2.1904, time 27.67ms\n",
      "iter 8200/8602/10000: loss 2.2084, time 23.91ms\n",
      "iter 8400/8602/10000: loss 2.1999, time 24.06ms\n",
      "iter 8600/8602/10000: loss 2.2384, time 29.22ms\n",
      "iter 8800/9384/10000: loss 2.1501, time 24.37ms\n",
      "iter 9000/9384/10000: loss 2.2346, time 23.68ms\n",
      "iter 9200/9384/10000: loss 2.2412, time 30.19ms\n",
      "iter 9400/10000/10000: loss 2.2365, time 26.05ms\n",
      "iter 9600/10000/10000: loss 2.1440, time 24.25ms\n",
      "iter 9800/10000/10000: loss 2.1896, time 28.29ms\n",
      "step 10000: train loss 2.1887, val loss 2.1875\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1864, time 5861.86ms\n",
      "## train_loss: 2.1877, val_loss: 2.1882, Time taken: 279.48123812675476s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.05, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: decay_lr\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=False, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 5630.22ms\n",
      "iter 200/782/10000: loss 2.3918, time 25.69ms\n",
      "iter 400/782/10000: loss 2.3741, time 23.87ms\n",
      "iter 600/782/10000: loss 2.3574, time 26.79ms\n",
      "iter 800/1564/10000: loss 2.3269, time 22.74ms\n",
      "iter 1000/1564/10000: loss 2.3851, time 23.14ms\n",
      "iter 1200/1564/10000: loss 2.2817, time 27.46ms\n",
      "iter 1400/1564/10000: loss 2.2686, time 23.24ms\n",
      "iter 1600/2346/10000: loss 2.2501, time 23.61ms\n",
      "iter 1800/2346/10000: loss 2.2201, time 26.97ms\n",
      "iter 2000/2346/10000: loss 2.3218, time 23.86ms\n",
      "iter 2200/2346/10000: loss 2.2417, time 24.28ms\n",
      "iter 2400/3128/10000: loss 2.2269, time 25.14ms\n",
      "iter 2600/3128/10000: loss 2.2076, time 36.84ms\n",
      "iter 2800/3128/10000: loss 2.2413, time 28.69ms\n",
      "iter 3000/3128/10000: loss 2.2469, time 25.07ms\n",
      "iter 3200/3910/10000: loss 2.1970, time 27.90ms\n",
      "iter 3400/3910/10000: loss 2.2264, time 24.13ms\n",
      "iter 3600/3910/10000: loss 2.2175, time 25.38ms\n",
      "iter 3800/3910/10000: loss 2.2356, time 26.60ms\n",
      "iter 4000/4692/10000: loss 2.2271, time 23.70ms\n",
      "iter 4200/4692/10000: loss 2.2218, time 24.51ms\n",
      "iter 4400/4692/10000: loss 2.2808, time 26.65ms\n",
      "iter 4600/4692/10000: loss 2.1741, time 23.11ms\n",
      "iter 4800/5474/10000: loss 2.2000, time 28.27ms\n",
      "iter 5000/5474/10000: loss 2.2434, time 25.64ms\n",
      "iter 5200/5474/10000: loss 2.2114, time 23.98ms\n",
      "iter 5400/5474/10000: loss 2.2257, time 23.20ms\n",
      "iter 5600/6256/10000: loss 2.1960, time 28.60ms\n",
      "iter 5800/6256/10000: loss 2.1866, time 25.08ms\n",
      "iter 6000/6256/10000: loss 2.2088, time 23.59ms\n",
      "iter 6200/6256/10000: loss 2.1692, time 27.68ms\n",
      "iter 6400/7038/10000: loss 2.2270, time 23.42ms\n",
      "iter 6600/7038/10000: loss 2.1914, time 23.85ms\n",
      "iter 6800/7038/10000: loss 2.1697, time 25.43ms\n",
      "iter 7000/7038/10000: loss 2.2144, time 27.39ms\n",
      "iter 7200/7820/10000: loss 2.1892, time 29.45ms\n",
      "iter 7400/7820/10000: loss 2.1967, time 23.84ms\n",
      "iter 7600/7820/10000: loss 2.1850, time 27.85ms\n",
      "iter 7800/7820/10000: loss 2.1911, time 24.25ms\n",
      "iter 8000/8602/10000: loss 2.1860, time 25.16ms\n",
      "iter 8200/8602/10000: loss 2.2166, time 26.85ms\n",
      "iter 8400/8602/10000: loss 2.2048, time 24.52ms\n",
      "iter 8600/8602/10000: loss 2.2321, time 24.80ms\n",
      "iter 8800/9384/10000: loss 2.1516, time 26.60ms\n",
      "iter 9000/9384/10000: loss 2.2282, time 24.32ms\n",
      "iter 9200/9384/10000: loss 2.2374, time 26.57ms\n",
      "iter 9400/10000/10000: loss 2.2542, time 29.03ms\n",
      "iter 9600/10000/10000: loss 2.1522, time 24.13ms\n",
      "iter 9800/10000/10000: loss 2.2024, time 23.52ms\n",
      "step 10000: train loss 2.1987, val loss 2.1970\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.2014, time 5952.91ms\n",
      "## train_loss: 2.1974, val_loss: 2.1969, Time taken: 278.9522669315338s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': False, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: lr_decay_iters\n",
      "## Computed tune options: lr_decay_iters = [10000]\n",
      "## Tuning generation 2: min_lr\n",
      "## Computed tune options: min_lr = [0.001]\n",
      "## Tuning generation 2: warmup_iters\n",
      "## Computed tune options: warmup_iters = [0, 100, 1000]\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, decay_lr=True, warmup_iters=100, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 6250.59ms\n",
      "Error training with params {'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 100, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}: division by zero\n",
      "## Tuning generation 2: weight_decay\n",
      "## Tuning generation 2: beta1\n",
      "## Tuning generation 2: beta2\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 5657.37ms\n",
      "iter 200/782/10000: loss 2.4058, time 25.43ms\n",
      "iter 400/782/10000: loss 2.3695, time 28.92ms\n",
      "iter 600/782/10000: loss 2.3509, time 28.04ms\n",
      "iter 800/1564/10000: loss 2.3331, time 25.47ms\n",
      "iter 1000/1564/10000: loss 2.3916, time 27.60ms\n",
      "iter 1200/1564/10000: loss 2.2897, time 23.85ms\n",
      "iter 1400/1564/10000: loss 2.2835, time 25.73ms\n",
      "iter 1600/2346/10000: loss 2.2527, time 25.01ms\n",
      "iter 1800/2346/10000: loss 2.2198, time 24.46ms\n",
      "iter 2000/2346/10000: loss 2.3234, time 24.69ms\n",
      "iter 2200/2346/10000: loss 2.2545, time 27.46ms\n",
      "iter 2400/3128/10000: loss 2.2409, time 23.44ms\n",
      "iter 2600/3128/10000: loss 2.2027, time 24.63ms\n",
      "iter 2800/3128/10000: loss 2.2488, time 26.20ms\n",
      "iter 3000/3128/10000: loss 2.2408, time 23.73ms\n",
      "iter 3200/3910/10000: loss 2.1937, time 24.17ms\n",
      "iter 3400/3910/10000: loss 2.2136, time 26.57ms\n",
      "iter 3600/3910/10000: loss 2.2044, time 26.14ms\n",
      "iter 3800/3910/10000: loss 2.2358, time 30.65ms\n",
      "iter 4000/4692/10000: loss 2.2232, time 24.67ms\n",
      "iter 4200/4692/10000: loss 2.2108, time 26.36ms\n",
      "iter 4400/4692/10000: loss 2.2718, time 29.22ms\n",
      "iter 4600/4692/10000: loss 2.1542, time 24.59ms\n",
      "iter 4800/5474/10000: loss 2.1832, time 27.29ms\n",
      "iter 5000/5474/10000: loss 2.2354, time 27.24ms\n",
      "iter 5200/5474/10000: loss 2.1987, time 26.19ms\n",
      "iter 5400/5474/10000: loss 2.2108, time 27.27ms\n",
      "iter 5600/6256/10000: loss 2.1829, time 24.69ms\n",
      "iter 5800/6256/10000: loss 2.1620, time 22.90ms\n",
      "iter 6000/6256/10000: loss 2.1820, time 27.92ms\n",
      "iter 6200/6256/10000: loss 2.1499, time 24.49ms\n",
      "iter 6400/7038/10000: loss 2.1965, time 23.44ms\n",
      "iter 6600/7038/10000: loss 2.1759, time 27.22ms\n",
      "iter 6800/7038/10000: loss 2.1676, time 24.36ms\n",
      "iter 7000/7038/10000: loss 2.2084, time 24.66ms\n",
      "iter 7200/7820/10000: loss 2.1858, time 26.23ms\n",
      "iter 7400/7820/10000: loss 2.1736, time 24.71ms\n",
      "iter 7600/7820/10000: loss 2.1755, time 26.70ms\n",
      "iter 7800/7820/10000: loss 2.1755, time 40.38ms\n",
      "iter 8000/8602/10000: loss 2.1751, time 27.77ms\n",
      "iter 8200/8602/10000: loss 2.1965, time 31.73ms\n",
      "iter 8400/8602/10000: loss 2.1829, time 37.15ms\n",
      "iter 8600/8602/10000: loss 2.2194, time 31.48ms\n",
      "iter 8800/9384/10000: loss 2.1437, time 23.16ms\n",
      "iter 9000/9384/10000: loss 2.2291, time 25.47ms\n",
      "iter 9200/9384/10000: loss 2.2228, time 23.69ms\n",
      "iter 9400/10000/10000: loss 2.2389, time 31.30ms\n",
      "iter 9600/10000/10000: loss 2.1277, time 25.50ms\n",
      "iter 9800/10000/10000: loss 2.1622, time 26.20ms\n",
      "step 10000: train loss 2.1789, val loss 2.1766\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1695, time 5773.51ms\n",
      "## train_loss: 2.1759, val_loss: 2.1765, Time taken: 279.39044094085693s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'bfloat16', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: beta2 improved, val=0.99, best=2.1764602661132812, delta=0.0036072731018066406 elapsed=279.39044094085693s delta=-35.681535959243774s\n",
      "## Tuning generation 2: grad_clip\n",
      "## Tuning generation 2: dtype\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float16', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodo/src/rgi3/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 5636.12ms\n",
      "iter 200/782/10000: loss 2.4060, time 23.32ms\n",
      "iter 400/782/10000: loss 2.3678, time 28.25ms\n",
      "iter 600/782/10000: loss 2.3605, time 24.35ms\n",
      "iter 800/1564/10000: loss 2.3318, time 26.97ms\n",
      "iter 1000/1564/10000: loss 2.3856, time 25.39ms\n",
      "iter 1200/1564/10000: loss 2.2859, time 29.07ms\n",
      "iter 1400/1564/10000: loss 2.2919, time 36.24ms\n",
      "iter 1600/2346/10000: loss 2.2510, time 28.34ms\n",
      "iter 1800/2346/10000: loss 2.2256, time 27.03ms\n",
      "iter 2000/2346/10000: loss 2.3328, time 23.89ms\n",
      "iter 2200/2346/10000: loss 2.2515, time 25.78ms\n",
      "iter 2400/3128/10000: loss 2.2343, time 29.15ms\n",
      "iter 2600/3128/10000: loss 2.1876, time 26.00ms\n",
      "iter 2800/3128/10000: loss 2.2513, time 25.38ms\n",
      "iter 3000/3128/10000: loss 2.2433, time 26.28ms\n",
      "iter 3200/3910/10000: loss 2.1981, time 24.76ms\n",
      "iter 3400/3910/10000: loss 2.2150, time 32.11ms\n",
      "iter 3600/3910/10000: loss 2.2024, time 25.74ms\n",
      "iter 3800/3910/10000: loss 2.2345, time 28.34ms\n",
      "iter 4000/4692/10000: loss 2.2237, time 24.66ms\n",
      "iter 4200/4692/10000: loss 2.2241, time 24.21ms\n",
      "iter 4400/4692/10000: loss 2.2815, time 26.30ms\n",
      "iter 4600/4692/10000: loss 2.1671, time 24.67ms\n",
      "iter 4800/5474/10000: loss 2.1706, time 23.96ms\n",
      "iter 5000/5474/10000: loss 2.2382, time 27.34ms\n",
      "iter 5200/5474/10000: loss 2.2013, time 31.71ms\n",
      "iter 5400/5474/10000: loss 2.2163, time 29.00ms\n",
      "iter 5600/6256/10000: loss 2.1837, time 24.02ms\n",
      "iter 5800/6256/10000: loss 2.1664, time 29.57ms\n",
      "iter 6000/6256/10000: loss 2.1797, time 24.80ms\n",
      "iter 6200/6256/10000: loss 2.1401, time 24.13ms\n",
      "iter 6400/7038/10000: loss 2.1920, time 26.62ms\n",
      "iter 6600/7038/10000: loss 2.1720, time 24.65ms\n",
      "iter 6800/7038/10000: loss 2.1616, time 24.21ms\n",
      "iter 7000/7038/10000: loss 2.1962, time 27.30ms\n",
      "iter 7200/7820/10000: loss 2.1801, time 24.27ms\n",
      "iter 7400/7820/10000: loss 2.1689, time 23.20ms\n",
      "iter 7600/7820/10000: loss 2.1781, time 26.14ms\n",
      "iter 7800/7820/10000: loss 2.1785, time 24.08ms\n",
      "iter 8000/8602/10000: loss 2.1700, time 28.93ms\n",
      "iter 8200/8602/10000: loss 2.1957, time 24.31ms\n",
      "iter 8400/8602/10000: loss 2.1784, time 26.30ms\n",
      "iter 8600/8602/10000: loss 2.2252, time 24.75ms\n",
      "iter 8800/9384/10000: loss 2.1302, time 25.43ms\n",
      "iter 9000/9384/10000: loss 2.2178, time 26.23ms\n",
      "iter 9200/9384/10000: loss 2.2227, time 24.80ms\n",
      "iter 9400/10000/10000: loss 2.2367, time 26.59ms\n",
      "iter 9600/10000/10000: loss 2.1228, time 30.69ms\n",
      "iter 9800/10000/10000: loss 2.1629, time 29.08ms\n",
      "step 10000: train loss 2.1761, val loss 2.1747\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1712, time 5683.31ms\n",
      "## train_loss: 2.1743, val_loss: 2.1750, Time taken: 282.5201082229614s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float16', 'dropout': 0.0, 'bias': False}\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7791, time 5777.09ms\n",
      "iter 200/782/10000: loss 2.4074, time 26.23ms\n",
      "iter 400/782/10000: loss 2.3717, time 27.42ms\n",
      "iter 600/782/10000: loss 2.3608, time 32.35ms\n",
      "iter 800/1564/10000: loss 2.3274, time 25.15ms\n",
      "iter 1000/1564/10000: loss 2.3727, time 24.60ms\n",
      "iter 1200/1564/10000: loss 2.2802, time 26.77ms\n",
      "iter 1400/1564/10000: loss 2.2902, time 25.24ms\n",
      "iter 1600/2346/10000: loss 2.2489, time 25.55ms\n",
      "iter 1800/2346/10000: loss 2.2248, time 24.92ms\n",
      "iter 2000/2346/10000: loss 2.3235, time 24.49ms\n",
      "iter 2200/2346/10000: loss 2.2435, time 25.55ms\n",
      "iter 2400/3128/10000: loss 2.2373, time 24.41ms\n",
      "iter 2600/3128/10000: loss 2.1982, time 22.78ms\n",
      "iter 2800/3128/10000: loss 2.2402, time 25.46ms\n",
      "iter 3000/3128/10000: loss 2.2384, time 23.44ms\n",
      "iter 3200/3910/10000: loss 2.1894, time 23.91ms\n",
      "iter 3400/3910/10000: loss 2.2079, time 25.27ms\n",
      "iter 3600/3910/10000: loss 2.2079, time 25.76ms\n",
      "iter 3800/3910/10000: loss 2.2286, time 23.87ms\n",
      "iter 4000/4692/10000: loss 2.2225, time 25.31ms\n",
      "iter 4200/4692/10000: loss 2.2193, time 24.62ms\n",
      "iter 4400/4692/10000: loss 2.2706, time 28.72ms\n",
      "iter 4600/4692/10000: loss 2.1669, time 26.61ms\n",
      "iter 4800/5474/10000: loss 2.1880, time 27.76ms\n",
      "iter 5000/5474/10000: loss 2.2308, time 27.77ms\n",
      "iter 5200/5474/10000: loss 2.1941, time 25.38ms\n",
      "iter 5400/5474/10000: loss 2.2103, time 26.66ms\n",
      "iter 5600/6256/10000: loss 2.1842, time 24.97ms\n",
      "iter 5800/6256/10000: loss 2.1690, time 26.23ms\n",
      "iter 6000/6256/10000: loss 2.1791, time 27.01ms\n",
      "iter 6200/6256/10000: loss 2.1451, time 24.22ms\n",
      "iter 6400/7038/10000: loss 2.1950, time 23.18ms\n",
      "iter 6600/7038/10000: loss 2.1694, time 27.29ms\n",
      "iter 6800/7038/10000: loss 2.1560, time 24.68ms\n",
      "iter 7000/7038/10000: loss 2.2061, time 23.00ms\n",
      "iter 7200/7820/10000: loss 2.1776, time 27.30ms\n",
      "iter 7400/7820/10000: loss 2.1645, time 25.29ms\n",
      "iter 7600/7820/10000: loss 2.1765, time 24.24ms\n",
      "iter 7800/7820/10000: loss 2.1719, time 28.84ms\n",
      "iter 8000/8602/10000: loss 2.1675, time 23.23ms\n",
      "iter 8200/8602/10000: loss 2.1920, time 23.63ms\n",
      "iter 8400/8602/10000: loss 2.1776, time 27.61ms\n",
      "iter 8600/8602/10000: loss 2.2165, time 23.85ms\n",
      "iter 8800/9384/10000: loss 2.1368, time 27.54ms\n",
      "iter 9000/9384/10000: loss 2.2164, time 25.35ms\n",
      "iter 9200/9384/10000: loss 2.2164, time 84.91ms\n",
      "iter 9400/10000/10000: loss 2.2265, time 29.53ms\n",
      "iter 9600/10000/10000: loss 2.1178, time 26.09ms\n",
      "iter 9800/10000/10000: loss 2.1604, time 26.47ms\n",
      "step 10000: train loss 2.1734, val loss 2.1719\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1642, time 5807.84ms\n",
      "## train_loss: 2.1704, val_loss: 2.1714, Time taken: 279.63446497917175s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float32', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 2: dtype improved, val=float32, best=2.1713898181915283, delta=0.00507044792175293 elapsed=279.63446497917175s delta=0.24402403831481934s\n",
      "## Tuning generation 2: dropout\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.01, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7809, val loss 2.7809\n",
      "iter 0/782/10000: loss 2.7803, time 6309.13ms\n",
      "iter 200/782/10000: loss 2.4051, time 31.27ms\n",
      "iter 400/782/10000: loss 2.3714, time 26.31ms\n",
      "iter 600/782/10000: loss 2.3570, time 28.71ms\n",
      "iter 800/1564/10000: loss 2.3259, time 25.96ms\n",
      "iter 1000/1564/10000: loss 2.3797, time 26.82ms\n",
      "iter 1200/1564/10000: loss 2.2848, time 27.74ms\n",
      "iter 1400/1564/10000: loss 2.2805, time 24.90ms\n",
      "iter 1600/2346/10000: loss 2.2416, time 25.64ms\n",
      "iter 1800/2346/10000: loss 2.2141, time 32.19ms\n",
      "iter 2000/2346/10000: loss 2.3248, time 24.98ms\n",
      "iter 2200/2346/10000: loss 2.2440, time 24.62ms\n",
      "iter 2400/3128/10000: loss 2.2436, time 27.42ms\n",
      "iter 2600/3128/10000: loss 2.1951, time 32.05ms\n",
      "iter 2800/3128/10000: loss 2.2398, time 28.69ms\n",
      "iter 3000/3128/10000: loss 2.2444, time 26.08ms\n",
      "iter 3200/3910/10000: loss 2.2013, time 28.82ms\n",
      "iter 3400/3910/10000: loss 2.2193, time 24.97ms\n",
      "iter 3600/3910/10000: loss 2.1987, time 26.36ms\n",
      "iter 3800/3910/10000: loss 2.2375, time 28.05ms\n",
      "iter 4000/4692/10000: loss 2.2254, time 24.67ms\n",
      "iter 4200/4692/10000: loss 2.2317, time 33.17ms\n",
      "iter 4400/4692/10000: loss 2.2935, time 26.34ms\n",
      "iter 4600/4692/10000: loss 2.1659, time 28.70ms\n",
      "iter 4800/5474/10000: loss 2.1885, time 26.01ms\n",
      "iter 5000/5474/10000: loss 2.2546, time 24.72ms\n",
      "iter 5200/5474/10000: loss 2.2043, time 29.92ms\n",
      "iter 5400/5474/10000: loss 2.2223, time 33.96ms\n",
      "iter 5600/6256/10000: loss 2.1872, time 30.28ms\n",
      "iter 5800/6256/10000: loss 2.1776, time 27.19ms\n",
      "iter 6000/6256/10000: loss 2.1926, time 27.87ms\n",
      "iter 6200/6256/10000: loss 2.1592, time 26.57ms\n",
      "iter 6400/7038/10000: loss 2.2169, time 24.73ms\n",
      "iter 6600/7038/10000: loss 2.1852, time 27.04ms\n",
      "iter 6800/7038/10000: loss 2.1658, time 24.96ms\n",
      "iter 7000/7038/10000: loss 2.2083, time 30.74ms\n",
      "iter 7200/7820/10000: loss 2.1858, time 25.88ms\n",
      "iter 7400/7820/10000: loss 2.1801, time 27.39ms\n",
      "iter 7600/7820/10000: loss 2.1775, time 25.29ms\n",
      "iter 7800/7820/10000: loss 2.1840, time 24.72ms\n",
      "iter 8000/8602/10000: loss 2.1717, time 31.15ms\n",
      "iter 8200/8602/10000: loss 2.2099, time 25.33ms\n",
      "iter 8400/8602/10000: loss 2.1991, time 32.34ms\n",
      "iter 8600/8602/10000: loss 2.2251, time 26.86ms\n",
      "iter 8800/9384/10000: loss 2.1482, time 28.36ms\n",
      "iter 9000/9384/10000: loss 2.2315, time 24.72ms\n",
      "iter 9200/9384/10000: loss 2.2373, time 26.28ms\n",
      "iter 9400/10000/10000: loss 2.2543, time 29.20ms\n",
      "iter 9600/10000/10000: loss 2.1433, time 32.31ms\n",
      "iter 9800/10000/10000: loss 2.1811, time 30.00ms\n",
      "step 10000: train loss 2.1791, val loss 2.1769\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1779, time 5666.75ms\n",
      "## train_loss: 2.1783, val_loss: 2.1784, Time taken: 295.96131777763367s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float32', 'dropout': 0.01, 'bias': False}\n",
      "## Tuning generation 2: bias\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=8, n_embd=64, dropout=0.0, bias=True)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 20, with 1,802 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7737, val loss 2.7741\n",
      "iter 0/782/10000: loss 2.7781, time 6276.41ms\n",
      "iter 200/782/10000: loss 2.4161, time 26.11ms\n",
      "iter 400/782/10000: loss 2.3832, time 28.78ms\n",
      "iter 600/782/10000: loss 2.3024, time 29.71ms\n",
      "iter 800/1564/10000: loss 2.2932, time 25.66ms\n",
      "iter 1000/1564/10000: loss 2.3446, time 31.60ms\n",
      "iter 1200/1564/10000: loss 2.2544, time 27.30ms\n",
      "iter 1400/1564/10000: loss 2.2968, time 24.53ms\n",
      "iter 1600/2346/10000: loss 2.2869, time 26.11ms\n",
      "iter 1800/2346/10000: loss 2.2254, time 27.98ms\n",
      "iter 2000/2346/10000: loss 2.2678, time 26.18ms\n",
      "iter 2200/2346/10000: loss 2.1894, time 35.36ms\n",
      "iter 2400/3128/10000: loss 2.2186, time 25.38ms\n",
      "iter 2600/3128/10000: loss 2.2498, time 25.13ms\n",
      "iter 2800/3128/10000: loss 2.2650, time 24.59ms\n",
      "iter 3000/3128/10000: loss 2.2520, time 26.00ms\n",
      "iter 3200/3910/10000: loss 2.2346, time 28.25ms\n",
      "iter 3400/3910/10000: loss 2.2165, time 25.64ms\n",
      "iter 3600/3910/10000: loss 2.2146, time 25.26ms\n",
      "iter 3800/3910/10000: loss 2.2041, time 25.81ms\n",
      "iter 4000/4692/10000: loss 2.1947, time 27.12ms\n",
      "iter 4200/4692/10000: loss 2.2504, time 25.21ms\n",
      "iter 4400/4692/10000: loss 2.1798, time 26.73ms\n",
      "iter 4600/4692/10000: loss 2.2031, time 26.89ms\n",
      "iter 4800/5474/10000: loss 2.2146, time 24.93ms\n",
      "iter 5000/5474/10000: loss 2.1818, time 25.26ms\n",
      "iter 5200/5474/10000: loss 2.1536, time 27.29ms\n",
      "iter 5400/5474/10000: loss 2.2017, time 28.59ms\n",
      "iter 5600/6256/10000: loss 2.2038, time 24.80ms\n",
      "iter 5800/6256/10000: loss 2.2056, time 25.48ms\n",
      "iter 6000/6256/10000: loss 2.2062, time 26.49ms\n",
      "iter 6200/6256/10000: loss 2.1390, time 25.30ms\n",
      "iter 6400/7038/10000: loss 2.2010, time 30.47ms\n",
      "iter 6600/7038/10000: loss 2.1962, time 25.76ms\n",
      "iter 6800/7038/10000: loss 2.1703, time 28.30ms\n",
      "iter 7000/7038/10000: loss 2.1837, time 25.69ms\n",
      "iter 7200/7820/10000: loss 2.2379, time 32.73ms\n",
      "iter 7400/7820/10000: loss 2.1718, time 26.39ms\n",
      "iter 7600/7820/10000: loss 2.1923, time 28.41ms\n",
      "iter 7800/7820/10000: loss 2.1672, time 25.17ms\n",
      "iter 8000/8602/10000: loss 2.2158, time 34.01ms\n",
      "iter 8200/8602/10000: loss 2.2381, time 25.68ms\n",
      "iter 8400/8602/10000: loss 2.1419, time 31.23ms\n",
      "iter 8600/8602/10000: loss 2.1285, time 25.67ms\n",
      "iter 8800/9384/10000: loss 2.2061, time 21.10ms\n",
      "iter 9000/9384/10000: loss 2.2194, time 30.97ms\n",
      "iter 9200/9384/10000: loss 2.1855, time 27.34ms\n",
      "iter 9400/10000/10000: loss 2.1839, time 29.21ms\n",
      "iter 9600/10000/10000: loss 2.1552, time 25.19ms\n",
      "iter 9800/10000/10000: loss 2.2177, time 24.13ms\n",
      "step 10000: train loss 2.1748, val loss 2.1754\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1687, time 7024.15ms\n",
      "## train_loss: 2.1762, val_loss: 2.1766, Time taken: 308.01202416419983s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 2, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float32', 'dropout': 0.0, 'bias': True}\n",
      "## Tuning generation 3: model_name\n",
      "## Tuning generation 3: model_version\n",
      "## Tuning generation 3: n_max_context\n",
      "## Tuning generation 3: n_layer\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=4, n_head=8, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 19, with 200,064 parameters\n",
      "num non-decayed parameter tensors: 11, with 586 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.7409, val loss 2.7409\n",
      "iter 0/782/10000: loss 2.7346, time 7011.72ms\n",
      "iter 200/782/10000: loss 2.4051, time 32.79ms\n",
      "iter 400/782/10000: loss 2.3613, time 57.28ms\n",
      "iter 600/782/10000: loss 2.3492, time 30.27ms\n",
      "iter 800/1564/10000: loss 2.3397, time 30.43ms\n",
      "iter 1000/1564/10000: loss 2.2984, time 37.49ms\n",
      "iter 1200/1564/10000: loss 2.3317, time 32.55ms\n",
      "iter 1400/1564/10000: loss 2.2908, time 32.96ms\n",
      "iter 1600/2346/10000: loss 2.2669, time 32.98ms\n",
      "iter 1800/2346/10000: loss 2.2944, time 34.95ms\n",
      "iter 2000/2346/10000: loss 2.2907, time 29.72ms\n",
      "iter 2200/2346/10000: loss 2.2653, time 29.82ms\n",
      "iter 2400/3128/10000: loss 2.2215, time 30.13ms\n",
      "iter 2600/3128/10000: loss 2.2492, time 31.22ms\n",
      "iter 2800/3128/10000: loss 2.1707, time 31.96ms\n",
      "iter 3000/3128/10000: loss 2.2137, time 31.04ms\n",
      "iter 3200/3910/10000: loss 2.1760, time 30.00ms\n",
      "iter 3400/3910/10000: loss 2.1824, time 32.50ms\n",
      "iter 3600/3910/10000: loss 2.2261, time 32.60ms\n",
      "iter 3800/3910/10000: loss 2.2466, time 34.59ms\n",
      "iter 4000/4692/10000: loss 2.2242, time 32.21ms\n",
      "iter 4200/4692/10000: loss 2.2442, time 34.41ms\n",
      "iter 4400/4692/10000: loss 2.2274, time 34.10ms\n",
      "iter 4600/4692/10000: loss 2.1929, time 34.24ms\n",
      "iter 4800/5474/10000: loss 2.2163, time 33.28ms\n",
      "iter 5000/5474/10000: loss 2.1997, time 32.87ms\n",
      "iter 5200/5474/10000: loss 2.1890, time 32.55ms\n",
      "iter 5400/5474/10000: loss 2.2707, time 33.22ms\n",
      "iter 5600/6256/10000: loss 2.1587, time 31.45ms\n",
      "iter 5800/6256/10000: loss 2.1644, time 31.74ms\n",
      "iter 6000/6256/10000: loss 2.2070, time 35.65ms\n",
      "iter 6200/6256/10000: loss 2.1491, time 30.97ms\n",
      "iter 6400/7038/10000: loss 2.1739, time 34.68ms\n",
      "iter 6600/7038/10000: loss 2.1383, time 32.32ms\n",
      "iter 6800/7038/10000: loss 2.2280, time 35.77ms\n",
      "iter 7000/7038/10000: loss 2.1808, time 32.74ms\n",
      "iter 7200/7820/10000: loss 2.1595, time 31.86ms\n",
      "iter 7400/7820/10000: loss 2.2395, time 32.69ms\n",
      "iter 7600/7820/10000: loss 2.1665, time 35.06ms\n",
      "iter 7800/7820/10000: loss 2.2117, time 32.12ms\n",
      "iter 8000/8602/10000: loss 2.1102, time 33.26ms\n",
      "iter 8200/8602/10000: loss 2.2012, time 39.16ms\n",
      "iter 8400/8602/10000: loss 2.2374, time 33.53ms\n",
      "iter 8600/8602/10000: loss 2.1715, time 40.85ms\n",
      "iter 8800/9384/10000: loss 2.1656, time 37.99ms\n",
      "iter 9000/9384/10000: loss 2.1322, time 33.75ms\n",
      "iter 9200/9384/10000: loss 2.1670, time 32.45ms\n",
      "iter 9400/10000/10000: loss 2.1816, time 27.54ms\n",
      "iter 9600/10000/10000: loss 2.1606, time 35.15ms\n",
      "iter 9800/10000/10000: loss 2.1777, time 27.14ms\n",
      "step 10000: train loss 2.1602, val loss 2.1555\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/c4-tuning/0.1\n",
      "iter 10000/10000/10000: loss 2.1801, time 7211.53ms\n",
      "## train_loss: 2.1569, val_loss: 2.1591, Time taken: 373.9418308734894s, overrides={'model_name': 'c4-tuning', 'model_version': '0.1', 'n_max_context': 44, 'n_layer': 4, 'n_head': 8, 'n_embd': 64, 'max_iters': 10000, 'max_epochs': 1000000, 'eval_iters': 200, 'log_interval': 200, 'eval_interval': 10000, 'gradient_accumulation_steps': 1, 'batch_size': 256, 'learning_rate': 0.01, 'decay_lr': True, 'lr_decay_iters': 100, 'min_lr': 0.005, 'warmup_iters': 0, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'dtype': 'float32', 'dropout': 0.0, 'bias': False}\n",
      "## Tuning generation 3: n_head\n",
      "model_config=TransformerConfig(n_max_context=44, n_layer=2, n_head=16, n_embd=64, dropout=0.0, bias=False)\n",
      "train_config=TrainConfig(model_name='c4-tuning', model_version='0.1', eval_interval=10000, log_interval=200, eval_iters=200, eval_only=False, always_save_checkpoint=True, wandb_log=False, gradient_accumulation_steps=1, batch_size=256, learning_rate=0.01, max_epochs=1000000, max_iters=10000, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=100, min_lr=0.005, device='cuda', dtype='float32', compile=False)\n",
      "num decayed parameter tensors: 11, with 101,760 parameters\n",
      "num non-decayed parameter tensors: 7, with 330 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m tuner = Tuner(\n\u001b[32m      2\u001b[39m     tune_options=tune_options.copy(), \n\u001b[32m      3\u001b[39m     initial_params=initial_params.copy(),\n\u001b[32m      4\u001b[39m     computed_tune_options=computed_tune_options.copy(),\n\u001b[32m      5\u001b[39m     cache_version=TUNER_VERSION,\n\u001b[32m      6\u001b[39m     target_improvement_per_minute=\u001b[32m0.01\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautotune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/models/tuner.py:160\u001b[39m, in \u001b[36mTuner.autotune\u001b[39m\u001b[34m(self, num_generations)\u001b[39m\n\u001b[32m    158\u001b[39m prev_best_loss_elapsed = \u001b[38;5;28mself\u001b[39m.best_loss_elapsed\n\u001b[32m    159\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Tuning generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.generation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m is_improved = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtune_hyperparameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_improved:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Tuning generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.generation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m improved, val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.best_params[param_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, best=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.best_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, delta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_best_loss\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.best_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elapsed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.best_loss_elapsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms delta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.best_loss_elapsed\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mprev_best_loss_elapsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/models/tuner.py:200\u001b[39m, in \u001b[36mTuner.tune_hyperparameter\u001b[39m\u001b[34m(self, param_name)\u001b[39m\n\u001b[32m    197\u001b[39m current_idx = \u001b[38;5;28mself\u001b[39m.tune_options[param_name].index(params[param_name])\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Try tuning from current_idx updwrds.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tune_hyperparameter_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurrent_idx\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtune_options\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Try tuning from current_idx downwards.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/models/tuner.py:176\u001b[39m, in \u001b[36mTuner._tune_hyperparameter_range\u001b[39m\u001b[34m(self, params, param_name, idx_list)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idx_list:\n\u001b[32m    175\u001b[39m     params[param_name] = \u001b[38;5;28mself\u001b[39m.tune_options[param_name][idx]\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     loss, elapsed, loss_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_update_best_param(loss, elapsed, params, loss_dict):\n\u001b[32m    178\u001b[39m         best_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/models/tuner.py:83\u001b[39m, in \u001b[36mTuner.train_and_compute_loss\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         loss_dict, elapsed = \u001b[43mtrain_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m         loss_dict[\u001b[33m'\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m'\u001b[39m] = elapsed\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_with\u001b[39m\u001b[34m(**overrides)\u001b[39m\n\u001b[32m     29\u001b[39m model = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=\u001b[32m42\u001b[39m)\n\u001b[32m     31\u001b[39m training_splits = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mgen-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m generation_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_GENERATIONS+\u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model, trainer = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m loss_dict = trainer.estimate_loss()\n\u001b[32m     35\u001b[39m loss_dict = {k: \u001b[38;5;28mfloat\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items()}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, training_splits, train_config)\u001b[39m\n\u001b[32m     33\u001b[39m trajectory_loader = build_trajectory_loader(\n\u001b[32m     34\u001b[39m     DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n\u001b[32m     35\u001b[39m     device=device, workers=num_workers, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     37\u001b[39m trainer = Trainer(\n\u001b[32m     38\u001b[39m     model=model,\n\u001b[32m     39\u001b[39m     train_config=train_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     device=device\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/train.py:133\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_config.max_epochs):\n\u001b[32m    132\u001b[39m         \u001b[38;5;66;03m# print(f\"Training epoch {epoch_id} of {self.train_config.max_epochs}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         \u001b[38;5;66;03m# termination conditions\u001b[39;00m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_num > \u001b[38;5;28mself\u001b[39m.train_config.max_iters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/train.py:153\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# evaluate the loss on train/val sets and write checkpoints\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_num % \u001b[38;5;28mself\u001b[39m.train_config.eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.iter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_config.wandb_log:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/rgi3/src/rgi/rgizero/train.py:108\u001b[39m, in \u001b[36mTrainer.estimate_loss\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx:\n\u001b[32m    107\u001b[39m             logits, loss = \u001b[38;5;28mself\u001b[39m.model(*data_batch)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m         losses[k] = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     out[split] = losses.mean()\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()  \u001b[38;5;66;03m# put model back into training mode\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx STOP HERE xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
