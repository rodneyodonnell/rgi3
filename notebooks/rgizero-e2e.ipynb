{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"small\"  # \"tiny\" or \"small\" or\"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 2000\n",
    "MAX_TRAINING_ITERS = 10_000\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 5\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MAX_TRAINING_ITERS = 1_000_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer]):\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        t0 = time.time()\n",
    "        player = player_factory()\n",
    "        game_result = await play_game_async(game, [player, player])\n",
    "        t1 = time.time()\n",
    "        game_result['time'] = t1 - t0\n",
    "        return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = 50,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 64,\n",
    "\n",
    "    learning_rate = 1e-3,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = 5000,  # make equal to max_iters usually\n",
    "    min_lr = 1e-4,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 100,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n",
    "        device=device, workers=num_workers)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 27850\n",
      "  Avg trajectory length: 13.93\n",
      "  Trajectory length - min: 7, max: 35, mean: 13.93\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=74.10% win[2]=25.90%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=254 win[1]=64.17% counts=Counter({1: 163, 2: 91})\n",
      "  a=2: n=271 win[1]=73.80% counts=Counter({1: 200, 2: 71})\n",
      "  a=3: n=239 win[1]=79.50% counts=Counter({1: 190, 2: 49})\n",
      "  a=4: n=296 win[1]=85.14% counts=Counter({1: 252, 2: 44})\n",
      "  a=5: n=332 win[1]=79.82% counts=Counter({1: 265, 2: 67})\n",
      "  a=6: n=304 win[1]=75.00% counts=Counter({1: 228, 2: 76})\n",
      "  a=7: n=304 win[1]=60.53% counts=Counter({1: 184, 2: 120})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-1.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-2\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33385\n",
      "  Avg trajectory length: 16.69\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.69\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=34.45% win[2]=65.35%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=352 win[1]=27.27% counts=Counter({2: 255, 1: 96, None: 1})\n",
      "  a=2: n=403 win[1]=36.48% counts=Counter({2: 253, 1: 147, None: 3})\n",
      "  a=3: n=213 win[1]=32.39% counts=Counter({2: 144, 1: 69})\n",
      "  a=4: n=117 win[1]=49.57% counts=Counter({2: 59, 1: 58})\n",
      "  a=5: n=327 win[1]=40.06% counts=Counter({2: 196, 1: 131})\n",
      "  a=6: n=335 win[1]=33.73% counts=Counter({2: 222, 1: 113})\n",
      "  a=7: n=253 win[1]=29.64% counts=Counter({2: 178, 1: 75})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-2.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 34638\n",
      "  Avg trajectory length: 17.32\n",
      "  Trajectory length - min: 7, max: 42, mean: 17.32\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=70.40% win[2]=28.85%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=192 win[1]=60.94% counts=Counter({1: 117, 2: 74, None: 1})\n",
      "  a=2: n=168 win[1]=66.07% counts=Counter({1: 111, 2: 55, None: 2})\n",
      "  a=3: n=193 win[1]=77.20% counts=Counter({1: 149, 2: 43, None: 1})\n",
      "  a=4: n= 55 win[1]=81.82% counts=Counter({1: 45, 2: 10})\n",
      "  a=5: n=279 win[1]=77.06% counts=Counter({1: 215, 2: 58, None: 6})\n",
      "  a=6: n=549 win[1]=71.58% counts=Counter({1: 393, 2: 154, None: 2})\n",
      "  a=7: n=564 win[1]=67.02% counts=Counter({1: 378, 2: 183, None: 3})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-3.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-4\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 30542\n",
      "  Avg trajectory length: 15.27\n",
      "  Trajectory length - min: 7, max: 42, mean: 15.27\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=81.60% win[2]=18.20%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=127 win[1]=77.95% counts=Counter({1: 99, 2: 28})\n",
      "  a=2: n=171 win[1]=75.44% counts=Counter({1: 129, 2: 42})\n",
      "  a=3: n= 96 win[1]=77.08% counts=Counter({1: 74, 2: 21, None: 1})\n",
      "  a=4: n=164 win[1]=92.07% counts=Counter({1: 151, 2: 13})\n",
      "  a=5: n=289 win[1]=89.27% counts=Counter({1: 258, 2: 31})\n",
      "  a=6: n=309 win[1]=78.32% counts=Counter({1: 242, 2: 66, None: 1})\n",
      "  a=7: n=844 win[1]=80.45% counts=Counter({1: 679, 2: 163, None: 2})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-4.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-5\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 32700\n",
      "  Avg trajectory length: 16.35\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.35\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=79.20% win[2]=20.55%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=211 win[1]=67.77% counts=Counter({1: 143, 2: 67, None: 1})\n",
      "  a=2: n=281 win[1]=77.58% counts=Counter({1: 218, 2: 62, None: 1})\n",
      "  a=3: n=136 win[1]=79.41% counts=Counter({1: 108, 2: 28})\n",
      "  a=5: n=379 win[1]=84.96% counts=Counter({1: 322, 2: 57})\n",
      "  a=6: n=385 win[1]=80.78% counts=Counter({1: 311, 2: 72, None: 2})\n",
      "  a=7: n=608 win[1]=79.28% counts=Counter({1: 482, 2: 125, None: 1})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-5.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 6 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-6\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33445\n",
      "  Avg trajectory length: 16.72\n",
      "  Trajectory length - min: 7, max: 40, mean: 16.72\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=74.45% win[2]=25.55%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=248 win[1]=68.15% counts=Counter({1: 169, 2: 79})\n",
      "  a=2: n=277 win[1]=74.37% counts=Counter({1: 206, 2: 71})\n",
      "  a=3: n=194 win[1]=77.32% counts=Counter({1: 150, 2: 44})\n",
      "  a=4: n=137 win[1]=83.94% counts=Counter({1: 115, 2: 22})\n",
      "  a=5: n=398 win[1]=79.90% counts=Counter({1: 318, 2: 80})\n",
      "  a=6: n=320 win[1]=71.88% counts=Counter({1: 230, 2: 90})\n",
      "  a=7: n=426 win[1]=70.66% counts=Counter({1: 301, 2: 125})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-6.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 7 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-7\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 32954\n",
      "  Avg trajectory length: 16.48\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.48\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=71.80% win[2]=28.10%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=273 win[1]=65.57% counts=Counter({1: 179, 2: 94})\n",
      "  a=2: n=262 win[1]=70.23% counts=Counter({1: 184, 2: 78})\n",
      "  a=3: n=151 win[1]=74.83% counts=Counter({1: 113, 2: 38})\n",
      "  a=4: n=194 win[1]=85.57% counts=Counter({1: 166, 2: 28})\n",
      "  a=5: n=424 win[1]=72.64% counts=Counter({1: 308, 2: 116})\n",
      "  a=6: n=315 win[1]=73.33% counts=Counter({1: 231, 2: 83, None: 1})\n",
      "  a=7: n=381 win[1]=66.93% counts=Counter({1: 255, 2: 125, None: 1})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-7.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 8 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-8\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 34211\n",
      "  Avg trajectory length: 17.11\n",
      "  Trajectory length - min: 7, max: 42, mean: 17.11\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=69.65% win[2]=30.15%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=322 win[1]=62.11% counts=Counter({1: 200, 2: 120, None: 2})\n",
      "  a=2: n=306 win[1]=71.90% counts=Counter({1: 220, 2: 85, None: 1})\n",
      "  a=3: n=219 win[1]=68.49% counts=Counter({1: 150, 2: 69})\n",
      "  a=4: n=160 win[1]=85.62% counts=Counter({1: 137, 2: 23})\n",
      "  a=5: n=451 win[1]=71.18% counts=Counter({1: 321, 2: 129, None: 1})\n",
      "  a=6: n=327 win[1]=69.42% counts=Counter({1: 227, 2: 100})\n",
      "  a=7: n=215 win[1]=64.19% counts=Counter({1: 138, 2: 77})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-8.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 9 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-9\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33852\n",
      "  Avg trajectory length: 16.93\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.93\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=53.30% win[2]=46.60%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=341 win[1]=48.09% counts=Counter({2: 176, 1: 164, None: 1})\n",
      "  a=2: n=334 win[1]=50.90% counts=Counter({1: 170, 2: 163, None: 1})\n",
      "  a=3: n=268 win[1]=55.60% counts=Counter({1: 149, 2: 119})\n",
      "  a=4: n= 92 win[1]=92.39% counts=Counter({1: 85, 2: 7})\n",
      "  a=5: n=447 win[1]=54.81% counts=Counter({1: 245, 2: 202})\n",
      "  a=6: n=238 win[1]=55.04% counts=Counter({1: 131, 2: 107})\n",
      "  a=7: n=280 win[1]=43.57% counts=Counter({2: 158, 1: 122})\n",
      "Training model on gen-9\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 50\n",
      "step 0: train loss 0.8983, val loss 0.8953\n",
      "iter 0: loss 0.9375, time 386.77ms\n",
      "iter 100: loss 0.9154, time 8.73ms\n",
      "iter 200: loss 0.8583, time 10.10ms\n",
      "Training epoch 1 of 50\n",
      "iter 300: loss 0.8351, time 9.41ms\n",
      "iter 400: loss 0.8895, time 9.19ms\n",
      "iter 500: loss 0.8491, time 8.34ms\n",
      "Training epoch 2 of 50\n",
      "iter 600: loss 0.8123, time 9.38ms\n",
      "iter 700: loss 0.7667, time 9.04ms\n",
      "iter 800: loss 0.7977, time 9.83ms\n",
      "Training epoch 3 of 50\n",
      "iter 900: loss 0.8752, time 8.44ms\n",
      "step 1000: train loss 0.8719, val loss 0.8747\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 1000: loss 0.8570, time 165.83ms\n",
      "iter 1100: loss 0.8002, time 9.09ms\n",
      "Training epoch 4 of 50\n",
      "iter 1200: loss 0.9117, time 9.35ms\n",
      "iter 1300: loss 0.7786, time 8.11ms\n",
      "iter 1400: loss 0.8378, time 9.06ms\n",
      "Training epoch 5 of 50\n",
      "iter 1500: loss 0.8583, time 8.68ms\n",
      "iter 1600: loss 0.9664, time 9.21ms\n",
      "Training epoch 6 of 50\n",
      "iter 1700: loss 0.7678, time 8.35ms\n",
      "iter 1800: loss 0.8059, time 8.77ms\n",
      "iter 1900: loss 0.8887, time 9.42ms\n",
      "Training epoch 7 of 50\n",
      "step 2000: train loss 0.8552, val loss 0.8678\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 2000: loss 0.9214, time 172.47ms\n",
      "iter 2100: loss 0.8803, time 8.56ms\n",
      "iter 2200: loss 0.8888, time 8.70ms\n",
      "Training epoch 8 of 50\n",
      "iter 2300: loss 0.8224, time 8.80ms\n",
      "iter 2400: loss 0.8435, time 8.62ms\n",
      "iter 2500: loss 0.8464, time 9.72ms\n",
      "Training epoch 9 of 50\n",
      "iter 2600: loss 0.9505, time 9.23ms\n",
      "iter 2700: loss 0.8247, time 8.91ms\n",
      "iter 2800: loss 0.8148, time 8.84ms\n",
      "Training epoch 10 of 50\n",
      "iter 2900: loss 0.7978, time 8.32ms\n",
      "step 3000: train loss 0.8448, val loss 0.8538\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 3000: loss 0.8707, time 157.27ms\n",
      "iter 3100: loss 0.8160, time 9.05ms\n",
      "Training epoch 11 of 50\n",
      "iter 3200: loss 0.8032, time 8.90ms\n",
      "iter 3300: loss 0.8552, time 10.26ms\n",
      "Training epoch 12 of 50\n",
      "iter 3400: loss 0.8412, time 8.70ms\n",
      "iter 3500: loss 0.8663, time 8.97ms\n",
      "iter 3600: loss 0.8600, time 8.77ms\n",
      "Training epoch 13 of 50\n",
      "iter 3700: loss 0.8762, time 8.96ms\n",
      "iter 3800: loss 0.9388, time 9.39ms\n",
      "iter 3900: loss 0.9027, time 8.92ms\n",
      "Training epoch 14 of 50\n",
      "step 4000: train loss 0.8519, val loss 0.8505\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 4000: loss 0.9184, time 159.74ms\n",
      "iter 4100: loss 0.8360, time 9.78ms\n",
      "iter 4200: loss 0.8878, time 9.17ms\n",
      "Training epoch 15 of 50\n",
      "iter 4300: loss 0.8329, time 9.40ms\n",
      "iter 4400: loss 0.8206, time 8.39ms\n",
      "iter 4500: loss 0.8024, time 9.17ms\n",
      "Training epoch 16 of 50\n",
      "iter 4600: loss 0.8143, time 9.02ms\n",
      "iter 4700: loss 0.8143, time 9.07ms\n",
      "Training epoch 17 of 50\n",
      "iter 4800: loss 0.8126, time 8.22ms\n",
      "iter 4900: loss 0.8572, time 8.84ms\n",
      "step 5000: train loss 0.8618, val loss 0.8334\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 5000: loss 0.8802, time 186.71ms\n",
      "Training epoch 18 of 50\n",
      "iter 5100: loss 0.9677, time 9.68ms\n",
      "iter 5200: loss 0.9204, time 8.62ms\n",
      "iter 5300: loss 0.8655, time 8.80ms\n",
      "Training epoch 19 of 50\n",
      "iter 5400: loss 0.8117, time 9.16ms\n",
      "iter 5500: loss 0.8569, time 8.71ms\n",
      "iter 5600: loss 0.7767, time 8.34ms\n",
      "Training epoch 20 of 50\n",
      "iter 5700: loss 0.8377, time 9.52ms\n",
      "iter 5800: loss 0.9254, time 9.05ms\n",
      "iter 5900: loss 0.8649, time 8.88ms\n",
      "Training epoch 21 of 50\n",
      "step 6000: train loss 0.8618, val loss 0.8465\n",
      "iter 6000: loss 0.8273, time 143.91ms\n",
      "iter 6100: loss 0.8351, time 8.98ms\n",
      "iter 6200: loss 0.8751, time 9.50ms\n",
      "Training epoch 22 of 50\n",
      "iter 6300: loss 0.9045, time 8.35ms\n",
      "iter 6400: loss 0.8512, time 9.54ms\n",
      "Training epoch 23 of 50\n",
      "iter 6500: loss 0.9013, time 9.03ms\n",
      "iter 6600: loss 0.8572, time 8.79ms\n",
      "iter 6700: loss 0.8566, time 8.57ms\n",
      "Training epoch 24 of 50\n",
      "iter 6800: loss 0.9093, time 10.38ms\n",
      "iter 6900: loss 0.8388, time 8.80ms\n",
      "step 7000: train loss 0.8433, val loss 0.8665\n",
      "iter 7000: loss 0.7880, time 144.15ms\n",
      "Training epoch 25 of 50\n",
      "iter 7100: loss 0.8821, time 8.56ms\n",
      "iter 7200: loss 0.8383, time 10.07ms\n",
      "iter 7300: loss 0.8740, time 9.60ms\n",
      "Training epoch 26 of 50\n",
      "iter 7400: loss 0.9133, time 9.06ms\n",
      "iter 7500: loss 0.7803, time 8.44ms\n",
      "iter 7600: loss 0.9170, time 9.85ms\n",
      "Training epoch 27 of 50\n",
      "iter 7700: loss 0.8181, time 9.35ms\n",
      "iter 7800: loss 0.8299, time 8.91ms\n",
      "Training epoch 28 of 50\n",
      "iter 7900: loss 0.9191, time 8.47ms\n",
      "step 8000: train loss 0.8633, val loss 0.8361\n",
      "iter 8000: loss 0.9082, time 141.24ms\n",
      "iter 8100: loss 0.8699, time 8.80ms\n",
      "Training epoch 29 of 50\n",
      "iter 8200: loss 0.7433, time 9.25ms\n",
      "iter 8300: loss 0.8697, time 8.66ms\n",
      "iter 8400: loss 0.8513, time 9.10ms\n",
      "Training epoch 30 of 50\n",
      "iter 8500: loss 0.7688, time 8.79ms\n",
      "iter 8600: loss 0.8623, time 9.01ms\n",
      "iter 8700: loss 0.8661, time 8.80ms\n",
      "Training epoch 31 of 50\n",
      "iter 8800: loss 0.8333, time 8.92ms\n",
      "iter 8900: loss 0.8587, time 9.36ms\n",
      "step 9000: train loss 0.8597, val loss 0.8323\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 9000: loss 0.8425, time 157.82ms\n",
      "Training epoch 32 of 50\n",
      "iter 9100: loss 0.9047, time 10.67ms\n",
      "iter 9200: loss 0.8420, time 8.70ms\n",
      "iter 9300: loss 0.7944, time 8.92ms\n",
      "Training epoch 33 of 50\n",
      "iter 9400: loss 0.9107, time 8.52ms\n",
      "iter 9500: loss 0.7409, time 9.79ms\n",
      "Training epoch 34 of 50\n",
      "iter 9600: loss 0.8498, time 9.48ms\n",
      "iter 9700: loss 0.8474, time 9.28ms\n",
      "iter 9800: loss 0.8636, time 8.33ms\n",
      "Training epoch 35 of 50\n",
      "iter 9900: loss 0.7830, time 10.61ms\n",
      "step 10000: train loss 0.8552, val loss 0.8447\n",
      "iter 10000: loss 0.8771, time 155.87ms\n",
      "iter 10100: loss 0.8818, time 8.76ms\n",
      "Training epoch 36 of 50\n",
      "iter 10200: loss 0.8810, time 8.27ms\n",
      "iter 10300: loss 0.8473, time 10.59ms\n",
      "iter 10400: loss 0.8120, time 10.01ms\n",
      "Training epoch 37 of 50\n",
      "iter 10500: loss 0.8402, time 8.93ms\n",
      "iter 10600: loss 0.7760, time 8.28ms\n",
      "iter 10700: loss 0.8669, time 9.11ms\n",
      "Training epoch 38 of 50\n",
      "iter 10800: loss 0.8076, time 9.05ms\n",
      "iter 10900: loss 0.8749, time 8.85ms\n",
      "Training epoch 39 of 50\n",
      "step 11000: train loss 0.8481, val loss 0.8493\n",
      "iter 11000: loss 0.7804, time 130.26ms\n",
      "iter 11100: loss 0.9054, time 9.26ms\n",
      "iter 11200: loss 0.8325, time 9.26ms\n",
      "Training epoch 40 of 50\n",
      "iter 11300: loss 0.8330, time 8.54ms\n",
      "iter 11400: loss 0.8247, time 8.97ms\n",
      "iter 11500: loss 0.8430, time 9.37ms\n",
      "Training epoch 41 of 50\n",
      "iter 11600: loss 0.7799, time 9.05ms\n",
      "iter 11700: loss 0.8819, time 8.94ms\n",
      "iter 11800: loss 0.8423, time 8.49ms\n",
      "Training epoch 42 of 50\n",
      "iter 11900: loss 0.8187, time 11.03ms\n",
      "step 12000: train loss 0.8374, val loss 0.8507\n",
      "iter 12000: loss 0.8241, time 148.02ms\n",
      "iter 12100: loss 0.8740, time 10.02ms\n",
      "Training epoch 43 of 50\n",
      "iter 12200: loss 0.7979, time 8.98ms\n",
      "iter 12300: loss 0.8784, time 9.14ms\n",
      "iter 12400: loss 0.9111, time 8.71ms\n",
      "Training epoch 44 of 50\n",
      "iter 12500: loss 0.7998, time 8.45ms\n",
      "iter 12600: loss 0.8235, time 8.49ms\n",
      "Training epoch 45 of 50\n",
      "iter 12700: loss 0.8486, time 10.83ms\n",
      "iter 12800: loss 0.7812, time 9.07ms\n",
      "iter 12900: loss 0.8852, time 9.75ms\n",
      "Training epoch 46 of 50\n",
      "step 13000: train loss 0.8343, val loss 0.8303\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 13000: loss 0.7572, time 177.87ms\n",
      "iter 13100: loss 0.8350, time 9.00ms\n",
      "iter 13200: loss 0.8998, time 8.97ms\n",
      "Training epoch 47 of 50\n",
      "iter 13300: loss 0.8785, time 9.16ms\n",
      "iter 13400: loss 0.7997, time 16.69ms\n",
      "iter 13500: loss 0.8937, time 8.64ms\n",
      "Training epoch 48 of 50\n",
      "iter 13600: loss 0.8558, time 8.41ms\n",
      "iter 13700: loss 0.8171, time 8.40ms\n",
      "iter 13800: loss 0.7546, time 8.18ms\n",
      "Training epoch 49 of 50\n",
      "iter 13900: loss 0.8365, time 8.48ms\n",
      "step 14000: train loss 0.8477, val loss 0.8355\n",
      "iter 14000: loss 0.9054, time 142.45ms\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "# 10 hours!\n",
    "for generation_id in range(1, 10):\n",
    "    current_model = model_dict[generation_id-1]\n",
    "    results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "    results_dict[generation_id] = results_i\n",
    "    trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "    model_dict[generation_id] = model_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dd) = 2644\n"
     ]
    }
   ],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    # for g in ['*', gen]:\n",
    "    for g in ['*']:\n",
    "        dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=()\n",
      "gen=*: tensor([12197.,  5803.]), win_pct=67.76%, sum=18000.0\n",
      "legal_policy=[0.31690302 0.11800605 0.09656373 0.16159788 0.14568563 0.15533988 0.00590382]\n",
      "player_values=[-0.17814368  0.17814362]\n",
      "player_probs=[0.41092816 0.5890718 ]\n",
      "\n",
      "prefix=(4,)\n",
      "gen=*: tensor([1009.,  206.]), win_pct=83.05%, sum=1215.0\n",
      "legal_policy=[0.30121717 0.10495555 0.10609342 0.23583972 0.0955383  0.15200439 0.00435143]\n",
      "player_values=[-0.1383794   0.13837945]\n",
      "player_probs=[0.4308103 0.5691897]\n",
      "\n",
      "prefix=(4, 1)\n",
      "gen=*: tensor([139.,  21.]), win_pct=86.88%, sum=160.0\n",
      "legal_policy=[0.44906583 0.10687531 0.09948325 0.13355032 0.07262875 0.13427578 0.00412076]\n",
      "player_values=[-0.31353265  0.3135326 ]\n",
      "player_probs=[0.34323367 0.6567663 ]\n",
      "\n",
      "prefix=(4, 2)\n",
      "gen=*: tensor([135.,  23.]), win_pct=85.44%, sum=158.0\n",
      "legal_policy=[0.36684945 0.13270721 0.11168871 0.1284682  0.09794673 0.15918627 0.00315345]\n",
      "player_values=[-0.18065953  0.18065953]\n",
      "player_probs=[0.40967023 0.59032977]\n",
      "\n",
      "prefix=(4, 3)\n",
      "gen=*: tensor([74., 39.]), win_pct=65.49%, sum=113.0\n",
      "legal_policy=[0.40739018 0.12191261 0.11569872 0.12833619 0.08108643 0.14182906 0.00374683]\n",
      "player_values=[-0.23110974  0.23110974]\n",
      "player_probs=[0.38444513 0.61555487]\n",
      "\n",
      "prefix=(4, 4)\n",
      "gen=*: tensor([285.,  52.]), win_pct=84.57%, sum=337.0\n",
      "legal_policy=[0.28712505 0.1059579  0.11229789 0.23995765 0.09456173 0.15476477 0.00533502]\n",
      "player_values=[-0.10271573  0.10271573]\n",
      "player_probs=[0.44864213 0.55135787]\n",
      "\n",
      "prefix=(4, 5)\n",
      "gen=*: tensor([114.,  28.]), win_pct=80.28%, sum=142.0\n",
      "legal_policy=[0.32840225 0.12430914 0.11273851 0.16011092 0.13939379 0.12997575 0.00506963]\n",
      "player_values=[-0.18259865  0.18259859]\n",
      "player_probs=[0.40870067 0.5912993 ]\n",
      "\n",
      "prefix=(4, 6)\n",
      "gen=*: tensor([95., 16.]), win_pct=85.59%, sum=111.0\n",
      "legal_policy=[0.3370632  0.11629733 0.10595311 0.16440855 0.09517819 0.17752396 0.00357568]\n",
      "player_values=[-0.10998291  0.10998285]\n",
      "player_probs=[0.44500855 0.5549914 ]\n",
      "\n",
      "prefix=(4, 7)\n",
      "gen=*: tensor([167.,  27.]), win_pct=86.08%, sum=194.0\n",
      "legal_policy=[0.5551823  0.08463557 0.06117386 0.07323387 0.07625923 0.12546241 0.02405274]\n",
      "player_values=[-0.16499877  0.16499877]\n",
      "player_probs=[0.41750062 0.5824994 ]\n"
     ]
    }
   ],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "prefix_list = [(), (4,), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7)]\n",
    "for prefix in prefix_list:\n",
    "    print(f\"\\nprefix={prefix}\")\n",
    "    for gen, counts in dd[prefix].items():\n",
    "        print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "    eval_result = eval_prefix(current_model, game, prefix)\n",
    "    print(f'legal_policy={eval_result.legal_policy}')\n",
    "    print(f'player_values={eval_result.player_values}')\n",
    "    print(f'player_probs={(eval_result.player_values+1)/2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=()\n",
      "  gen=*: tensor([12197.,  5803.]), win_pct=67.76%, sum=18000.0\n",
      "  gen=1: tensor([1482.,  518.]), win_pct=74.10%, sum=2000.0\n",
      "  gen=2: tensor([ 691., 1309.]), win_pct=34.55%, sum=2000.0\n",
      "  gen=3: tensor([1415.5000,  584.5000]), win_pct=70.78%, sum=2000.0\n",
      "  gen=4: tensor([1634.,  366.]), win_pct=81.70%, sum=2000.0\n",
      "  gen=5: tensor([1586.5000,  413.5000]), win_pct=79.32%, sum=2000.0\n",
      "  gen=6: tensor([1489.,  511.]), win_pct=74.45%, sum=2000.0\n",
      "  gen=7: tensor([1437.,  563.]), win_pct=71.85%, sum=2000.0\n",
      "  gen=8: tensor([1395.,  605.]), win_pct=69.75%, sum=2000.0\n",
      "  gen=9: tensor([1067.,  933.]), win_pct=53.35%, sum=2000.0\n",
      "  Training data values (transformed): mean=[ 0.35522223 -0.35522223], std=[0.93375266 0.93375266]\n",
      "  Found 18000 examples\n",
      "  Player 1 win rate from stored values: 67.66%\n",
      "  MODEL: values=[-0.17814368  0.17814362], legal_policy=[0.31690302 0.11800605 0.09656373 0.16159788 0.14568563 0.15533988 0.00590382]\n",
      "\n",
      "prefix=(4,)\n",
      "  gen=*: tensor([1009.,  206.]), win_pct=83.05%, sum=1215.0\n",
      "  gen=1: tensor([252.,  44.]), win_pct=85.14%, sum=296.0\n",
      "  gen=2: tensor([58., 59.]), win_pct=49.57%, sum=117.0\n",
      "  gen=3: tensor([45., 10.]), win_pct=81.82%, sum=55.0\n",
      "  gen=4: tensor([151.,  13.]), win_pct=92.07%, sum=164.0\n",
      "  gen=6: tensor([115.,  22.]), win_pct=83.94%, sum=137.0\n",
      "  gen=7: tensor([166.,  28.]), win_pct=85.57%, sum=194.0\n",
      "  gen=8: tensor([137.,  23.]), win_pct=85.62%, sum=160.0\n",
      "  gen=9: tensor([85.,  7.]), win_pct=92.39%, sum=92.0\n",
      "  Training data values (transformed): mean=[ 0.66090536 -0.66090536], std=[0.75047135 0.75047135]\n",
      "  Found 1215 examples\n",
      "  Player 1 win rate from stored values: 83.05%\n",
      "  MODEL: values=[-0.1383794   0.13837945], legal_policy=[0.30121717 0.10495555 0.10609342 0.23583972 0.0955383  0.15200439 0.00435143]\n",
      "\n",
      "prefix=(4, 1)\n",
      "  gen=*: tensor([139.,  21.]), win_pct=86.88%, sum=160.0\n",
      "  gen=1: tensor([34.,  3.]), win_pct=91.89%, sum=37.0\n",
      "  gen=2: tensor([11.,  9.]), win_pct=55.00%, sum=20.0\n",
      "  gen=3: tensor([3., 0.]), win_pct=100.00%, sum=3.0\n",
      "  gen=4: tensor([16.,  0.]), win_pct=100.00%, sum=16.0\n",
      "  gen=6: tensor([22.,  4.]), win_pct=84.62%, sum=26.0\n",
      "  gen=7: tensor([29.,  3.]), win_pct=90.62%, sum=32.0\n",
      "  gen=8: tensor([15.,  2.]), win_pct=88.24%, sum=17.0\n",
      "  gen=9: tensor([9., 0.]), win_pct=100.00%, sum=9.0\n",
      "  Training data values (transformed): mean=[ 0.7375 -0.7375], std=[0.67534685 0.67534685]\n",
      "  Found 160 examples\n",
      "  Player 1 win rate from stored values: 86.88%\n",
      "  MODEL: values=[-0.31353265  0.3135326 ], legal_policy=[0.44906583 0.10687531 0.09948325 0.13355032 0.07262875 0.13427578 0.00412076]\n",
      "\n",
      "prefix=(4, 2)\n",
      "  gen=*: tensor([135.,  23.]), win_pct=85.44%, sum=158.0\n",
      "  gen=1: tensor([32.,  4.]), win_pct=88.89%, sum=36.0\n",
      "  gen=2: tensor([9., 3.]), win_pct=75.00%, sum=12.0\n",
      "  gen=3: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "  gen=4: tensor([6., 3.]), win_pct=66.67%, sum=9.0\n",
      "  gen=6: tensor([23.,  6.]), win_pct=79.31%, sum=29.0\n",
      "  gen=7: tensor([29.,  2.]), win_pct=93.55%, sum=31.0\n",
      "  gen=8: tensor([22.,  5.]), win_pct=81.48%, sum=27.0\n",
      "  gen=9: tensor([13.,  0.]), win_pct=100.00%, sum=13.0\n",
      "  Training data values (transformed): mean=[ 0.70886075 -0.70886075], std=[0.7053486 0.7053486]\n",
      "  Found 158 examples\n",
      "  Player 1 win rate from stored values: 85.44%\n",
      "  MODEL: values=[-0.18065953  0.18065953], legal_policy=[0.36684945 0.13270721 0.11168871 0.1284682  0.09794673 0.15918627 0.00315345]\n",
      "\n",
      "prefix=(4, 3)\n",
      "  gen=*: tensor([74., 39.]), win_pct=65.49%, sum=113.0\n",
      "  gen=1: tensor([28.,  9.]), win_pct=75.68%, sum=37.0\n",
      "  gen=2: tensor([ 8., 17.]), win_pct=32.00%, sum=25.0\n",
      "  gen=3: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "  gen=4: tensor([3., 1.]), win_pct=75.00%, sum=4.0\n",
      "  gen=6: tensor([7., 3.]), win_pct=70.00%, sum=10.0\n",
      "  gen=7: tensor([16.,  6.]), win_pct=72.73%, sum=22.0\n",
      "  gen=8: tensor([9., 2.]), win_pct=81.82%, sum=11.0\n",
      "  gen=9: tensor([3., 0.]), win_pct=100.00%, sum=3.0\n",
      "  Training data values (transformed): mean=[ 0.30973452 -0.30973452], std=[0.95082283 0.95082283]\n",
      "  Found 113 examples\n",
      "  Player 1 win rate from stored values: 65.49%\n",
      "  MODEL: values=[-0.23110974  0.23110974], legal_policy=[0.40739018 0.12191261 0.11569872 0.12833619 0.08108643 0.14182906 0.00374683]\n",
      "\n",
      "prefix=(4, 4)\n",
      "  gen=*: tensor([285.,  52.]), win_pct=84.57%, sum=337.0\n",
      "  gen=1: tensor([45.,  7.]), win_pct=86.54%, sum=52.0\n",
      "  gen=2: tensor([17., 16.]), win_pct=51.52%, sum=33.0\n",
      "  gen=3: tensor([38.,  9.]), win_pct=80.85%, sum=47.0\n",
      "  gen=4: tensor([76.,  5.]), win_pct=93.83%, sum=81.0\n",
      "  gen=6: tensor([6., 1.]), win_pct=85.71%, sum=7.0\n",
      "  gen=7: tensor([25.,  4.]), win_pct=86.21%, sum=29.0\n",
      "  gen=8: tensor([29.,  6.]), win_pct=82.86%, sum=35.0\n",
      "  gen=9: tensor([49.,  4.]), win_pct=92.45%, sum=53.0\n",
      "  Training data values (transformed): mean=[ 0.6913947 -0.6913947], std=[0.7224759 0.7224759]\n",
      "  Found 337 examples\n",
      "  Player 1 win rate from stored values: 84.57%\n",
      "  MODEL: values=[-0.10271573  0.10271573], legal_policy=[0.28712505 0.1059579  0.11229789 0.23995765 0.09456173 0.15476477 0.00533502]\n",
      "\n",
      "prefix=(4, 5)\n",
      "  gen=*: tensor([114.,  28.]), win_pct=80.28%, sum=142.0\n",
      "  gen=1: tensor([39., 11.]), win_pct=78.00%, sum=50.0\n",
      "  gen=2: tensor([6., 2.]), win_pct=75.00%, sum=8.0\n",
      "  gen=3: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "  gen=4: tensor([16.,  1.]), win_pct=94.12%, sum=17.0\n",
      "  gen=6: tensor([16.,  3.]), win_pct=84.21%, sum=19.0\n",
      "  gen=7: tensor([16.,  5.]), win_pct=76.19%, sum=21.0\n",
      "  gen=8: tensor([18.,  4.]), win_pct=81.82%, sum=22.0\n",
      "  gen=9: tensor([2., 2.]), win_pct=50.00%, sum=4.0\n",
      "  Training data values (transformed): mean=[ 0.6056338 -0.6056338], std=[0.7957436 0.7957436]\n",
      "  Found 142 examples\n",
      "  Player 1 win rate from stored values: 80.28%\n",
      "  MODEL: values=[-0.18259865  0.18259859], legal_policy=[0.32840225 0.12430914 0.11273851 0.16011092 0.13939379 0.12997575 0.00506963]\n",
      "\n",
      "prefix=(4, 6)\n",
      "  gen=*: tensor([95., 16.]), win_pct=85.59%, sum=111.0\n",
      "  gen=1: tensor([37.,  2.]), win_pct=94.87%, sum=39.0\n",
      "  gen=2: tensor([ 3., 10.]), win_pct=23.08%, sum=13.0\n",
      "  gen=4: tensor([8., 1.]), win_pct=88.89%, sum=9.0\n",
      "  gen=6: tensor([11.,  1.]), win_pct=91.67%, sum=12.0\n",
      "  gen=7: tensor([21.,  1.]), win_pct=95.45%, sum=22.0\n",
      "  gen=8: tensor([13.,  1.]), win_pct=92.86%, sum=14.0\n",
      "  gen=9: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "  Training data values (transformed): mean=[ 0.7117117 -0.7117117], std=[0.7024719 0.7024719]\n",
      "  Found 111 examples\n",
      "  Player 1 win rate from stored values: 85.59%\n",
      "  MODEL: values=[-0.10998291  0.10998285], legal_policy=[0.3370632  0.11629733 0.10595311 0.16440855 0.09517819 0.17752396 0.00357568]\n",
      "\n",
      "prefix=(4, 7)\n",
      "  gen=*: tensor([167.,  27.]), win_pct=86.08%, sum=194.0\n",
      "  gen=1: tensor([37.,  8.]), win_pct=82.22%, sum=45.0\n",
      "  gen=2: tensor([4., 2.]), win_pct=66.67%, sum=6.0\n",
      "  gen=3: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "  gen=4: tensor([26.,  2.]), win_pct=92.86%, sum=28.0\n",
      "  gen=6: tensor([30.,  4.]), win_pct=88.24%, sum=34.0\n",
      "  gen=7: tensor([30.,  7.]), win_pct=81.08%, sum=37.0\n",
      "  gen=8: tensor([31.,  3.]), win_pct=91.18%, sum=34.0\n",
      "  gen=9: tensor([7., 1.]), win_pct=87.50%, sum=8.0\n",
      "  Training data values (transformed): mean=[ 0.72164947 -0.72164947], std=[0.6922582 0.6922582]\n",
      "  Found 194 examples\n",
      "  Player 1 win rate from stored values: 86.08%\n",
      "  MODEL: values=[-0.16499877  0.16499877], legal_policy=[0.5551823  0.08463557 0.06117386 0.07323387 0.07625923 0.12546241 0.02405274]\n"
     ]
    }
   ],
   "source": [
    "# Check what values are stored in the training data for these prefixes\n",
    "def check_training_values(prefix):\n",
    "    \"\"\"See what values are actually stored in training data for this prefix.\"\"\"\n",
    "    matching_values = []\n",
    "    \n",
    "    for gen, td in enumerate(td_array, 1):\n",
    "        for trajectory in td:\n",
    "            actions = trajectory.action.numpy()\n",
    "            if len(prefix) == 0 or np.array_equal(actions[:len(prefix)], prefix):\n",
    "                value_at_position = trajectory.value[len(prefix)].numpy()\n",
    "                # Transform to [-1, 1] range to match model output\n",
    "                transformed_value = value_at_position * 2 - 1\n",
    "                matching_values.append((gen, transformed_value))\n",
    "    \n",
    "    if matching_values:\n",
    "        values_array = np.array([v for _, v in matching_values])\n",
    "        print(f\"  Training data values (transformed): mean={values_array.mean(axis=0)}, std={values_array.std(axis=0)}\")\n",
    "        print(f\"  Found {len(matching_values)} examples\")\n",
    "        \n",
    "        # Also show empirical win rate from these values\n",
    "        player1_wins = (values_array[:, 0] > values_array[:, 1]).sum()\n",
    "        win_pct = 100 * player1_wins / len(values_array)\n",
    "        print(f\"  Player 1 win rate from stored values: {win_pct:.2f}%\")\n",
    "\n",
    "# Add this to your debug loop\n",
    "prefix_list = [(), (4,), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7)]\n",
    "\n",
    "for prefix in prefix_list:\n",
    "    print(f\"\\nprefix={prefix}\")\n",
    "    \n",
    "    # Empirical win rates from games\n",
    "    for gen, counts in dd[prefix].items():\n",
    "        print(f\"  gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "    \n",
    "    # What's actually stored in training data\n",
    "    check_training_values(prefix)\n",
    "    \n",
    "    # Model prediction\n",
    "    result = eval_prefix(current_model, game, prefix)\n",
    "    print(f\"  MODEL: values={result.player_values}, legal_policy={result.legal_policy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
