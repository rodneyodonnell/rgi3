{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"small\"  # \"tiny\" or \"small\" or\"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 2000\n",
    "MAX_TRAINING_ITERS = 10_000\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer]):\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        t0 = time.time()\n",
    "        player = player_factory()\n",
    "        game_result = await play_game_async(game, [player, player])\n",
    "        t1 = time.time()\n",
    "        game_result['time'] = t1 - t0\n",
    "        return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 250,  # keep frequent because we'll overfit\n",
    "    eval_iters = 200,\n",
    "    log_interval = 10_000,  # don't print too too often\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 64,\n",
    "\n",
    "    learning_rate = 1e-3,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = 5000,  # make equal to max_iters usually\n",
    "    min_lr = 1e-4,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 100,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=1,\n",
    "        device=device, workers=num_workers)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:31<00:00,  7.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=74.10% win[2]=25.90%, n=2000\n",
      "Game Length min: 7, max: 35, mean: 13.93\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=254 win[1]=64.17% counts=Counter({1: 163, 2: 91})\n",
      "  a=2: n=271 win[1]=73.80% counts=Counter({1: 200, 2: 71})\n",
      "  a=3: n=239 win[1]=79.50% counts=Counter({1: 190, 2: 49})\n",
      "  a=4: n=296 win[1]=85.14% counts=Counter({1: 252, 2: 44})\n",
      "  a=5: n=332 win[1]=79.82% counts=Counter({1: 265, 2: 67})\n",
      "  a=6: n=304 win[1]=75.00% counts=Counter({1: 228, 2: 76})\n",
      "  a=7: n=304 win[1]=60.53% counts=Counter({1: 184, 2: 120})\n",
      "Training model on gen-1\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8803, val loss 0.8661\n",
      "iter 0: loss 1.2963, time 752.56ms\n",
      "step 250: train loss 0.7977, val loss 0.8460\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.8031, val loss 0.8179\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 750: train loss 0.7771, val loss 0.8209\n",
      "step 1000: train loss 0.8332, val loss 0.8128\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 1250: train loss 0.8218, val loss 0.7840\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 1500: train loss 0.8219, val loss 0.8751\n",
      "step 1750: train loss 0.8511, val loss 0.8155\n",
      "Training epoch 1 of 10\n",
      "step 2000: train loss 0.8141, val loss 0.7831\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2250: train loss 0.8285, val loss 0.8316\n",
      "step 2500: train loss 0.7928, val loss 0.8117\n",
      "step 2750: train loss 0.8503, val loss 0.7456\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 3000: train loss 0.7860, val loss 0.8500\n",
      "step 3250: train loss 0.8126, val loss 0.8059\n",
      "step 3500: train loss 0.7916, val loss 0.8128\n",
      "step 3750: train loss 0.7717, val loss 0.8434\n",
      "Training epoch 2 of 10\n",
      "step 4000: train loss 0.8875, val loss 0.8054\n",
      "step 4250: train loss 0.8465, val loss 0.7687\n",
      "step 4500: train loss 0.7889, val loss 0.8178\n",
      "step 4750: train loss 0.8024, val loss 0.7620\n",
      "step 5000: train loss 0.8263, val loss 0.7783\n",
      "step 5250: train loss 0.8330, val loss 0.7977\n",
      "step 5500: train loss 0.8027, val loss 0.8823\n",
      "step 5750: train loss 0.8002, val loss 0.7908\n",
      "Training epoch 3 of 10\n",
      "step 6000: train loss 0.7873, val loss 0.8141\n",
      "step 6250: train loss 0.8783, val loss 0.8113\n",
      "step 6500: train loss 0.7972, val loss 0.8055\n",
      "step 6750: train loss 0.8416, val loss 0.7594\n",
      "step 7000: train loss 0.8316, val loss 0.8184\n",
      "step 7250: train loss 0.7542, val loss 0.7956\n",
      "step 7500: train loss 0.8455, val loss 0.8593\n",
      "step 7750: train loss 0.8350, val loss 0.7969\n",
      "Training epoch 4 of 10\n",
      "step 8000: train loss 0.7697, val loss 0.8001\n",
      "step 8250: train loss 0.8451, val loss 0.7864\n",
      "step 8500: train loss 0.8001, val loss 0.8270\n",
      "step 8750: train loss 0.8381, val loss 0.8873\n",
      "step 9000: train loss 0.7842, val loss 0.8593\n",
      "step 9250: train loss 0.8306, val loss 0.7703\n",
      "step 9500: train loss 0.8222, val loss 0.7905\n",
      "step 9750: train loss 0.7840, val loss 0.8070\n",
      "Training epoch 5 of 10\n",
      "step 10000: train loss 0.8650, val loss 0.8246\n",
      "iter 10000: loss 0.6192, time 657.77ms\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:08<00:00,  6.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=34.45% win[2]=65.35%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 16.69\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=352 win[1]=27.27% counts=Counter({2: 255, 1: 96, None: 1})\n",
      "  a=2: n=403 win[1]=36.48% counts=Counter({2: 253, 1: 147, None: 3})\n",
      "  a=3: n=213 win[1]=32.39% counts=Counter({2: 144, 1: 69})\n",
      "  a=4: n=117 win[1]=49.57% counts=Counter({2: 59, 1: 58})\n",
      "  a=5: n=327 win[1]=40.06% counts=Counter({2: 196, 1: 131})\n",
      "  a=6: n=335 win[1]=33.73% counts=Counter({2: 222, 1: 113})\n",
      "  a=7: n=253 win[1]=29.64% counts=Counter({2: 178, 1: 75})\n",
      "Training model on gen-2\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.9892, val loss 1.0391\n",
      "iter 0: loss 1.1576, time 757.64ms\n",
      "step 250: train loss 0.8800, val loss 0.8954\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.9098, val loss 0.9666\n",
      "step 750: train loss 0.8934, val loss 0.9316\n",
      "step 1000: train loss 0.8420, val loss 0.9196\n",
      "step 1250: train loss 0.8397, val loss 0.8369\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 1500: train loss 0.9179, val loss 0.9174\n",
      "step 1750: train loss 0.9645, val loss 0.9410\n",
      "step 2000: train loss 0.8911, val loss 0.8334\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2250: train loss 0.8359, val loss 0.8741\n",
      "step 2500: train loss 0.8594, val loss 0.8619\n",
      "step 2750: train loss 0.8249, val loss 0.8531\n",
      "step 3000: train loss 0.8268, val loss 0.8734\n",
      "step 3250: train loss 0.8749, val loss 0.9004\n",
      "step 3500: train loss 0.8711, val loss 0.9095\n",
      "step 3750: train loss 0.8471, val loss 0.8735\n",
      "Training epoch 1 of 10\n",
      "step 4000: train loss 0.8759, val loss 0.9549\n",
      "step 4250: train loss 0.8989, val loss 0.8708\n",
      "step 4500: train loss 0.8253, val loss 0.8506\n",
      "step 4750: train loss 0.8888, val loss 0.8529\n",
      "step 5000: train loss 0.8627, val loss 0.8796\n",
      "step 5250: train loss 0.8368, val loss 0.8667\n",
      "step 5500: train loss 0.8750, val loss 0.8723\n",
      "step 5750: train loss 0.8832, val loss 0.8682\n",
      "step 6000: train loss 0.9414, val loss 0.9140\n",
      "step 6250: train loss 0.9032, val loss 0.9377\n",
      "step 6500: train loss 0.9114, val loss 0.8813\n",
      "step 6750: train loss 0.8988, val loss 0.8638\n",
      "step 7000: train loss 0.8586, val loss 0.8732\n",
      "step 7250: train loss 0.8571, val loss 0.8364\n",
      "step 7500: train loss 0.8890, val loss 0.8549\n",
      "step 7750: train loss 0.8697, val loss 0.9205\n",
      "Training epoch 2 of 10\n",
      "step 8000: train loss 0.9212, val loss 0.9196\n",
      "step 8250: train loss 0.8823, val loss 0.8533\n",
      "step 8500: train loss 0.8523, val loss 0.8485\n",
      "step 8750: train loss 0.9208, val loss 0.8742\n",
      "step 9000: train loss 0.8912, val loss 0.8784\n",
      "step 9250: train loss 0.8717, val loss 0.8515\n",
      "step 9500: train loss 0.8949, val loss 0.8487\n",
      "step 9750: train loss 0.8355, val loss 0.8811\n",
      "Training epoch 3 of 10\n",
      "step 10000: train loss 0.8549, val loss 0.9306\n",
      "iter 10000: loss 0.8011, time 678.84ms\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:55<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=70.40% win[2]=28.85%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 17.32\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=192 win[1]=60.94% counts=Counter({1: 117, 2: 74, None: 1})\n",
      "  a=2: n=168 win[1]=66.07% counts=Counter({1: 111, 2: 55, None: 2})\n",
      "  a=3: n=193 win[1]=77.20% counts=Counter({1: 149, 2: 43, None: 1})\n",
      "  a=4: n= 55 win[1]=81.82% counts=Counter({1: 45, 2: 10})\n",
      "  a=5: n=279 win[1]=77.06% counts=Counter({1: 215, 2: 58, None: 6})\n",
      "  a=6: n=549 win[1]=71.58% counts=Counter({1: 393, 2: 154, None: 2})\n",
      "  a=7: n=564 win[1]=67.02% counts=Counter({1: 378, 2: 183, None: 3})\n",
      "Training model on gen-3\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8756, val loss 0.8738\n",
      "iter 0: loss 1.2976, time 757.87ms\n",
      "step 250: train loss 0.9521, val loss 0.9340\n",
      "step 500: train loss 0.8912, val loss 0.8923\n",
      "step 750: train loss 0.8866, val loss 0.8995\n",
      "step 1000: train loss 0.9472, val loss 0.8900\n",
      "step 1250: train loss 0.9515, val loss 0.8883\n",
      "step 1500: train loss 0.9759, val loss 0.8986\n",
      "step 1750: train loss 0.8483, val loss 0.9153\n",
      "step 2000: train loss 0.9114, val loss 0.8550\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2250: train loss 0.9077, val loss 0.8649\n",
      "step 2500: train loss 0.8710, val loss 0.9290\n",
      "step 2750: train loss 0.9230, val loss 0.8499\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 3000: train loss 0.8787, val loss 0.9296\n",
      "step 3250: train loss 0.8937, val loss 0.8853\n",
      "step 3500: train loss 0.8676, val loss 0.8901\n",
      "step 3750: train loss 0.9016, val loss 0.8966\n",
      "step 4000: train loss 0.8971, val loss 0.8975\n",
      "step 4250: train loss 0.9137, val loss 0.8522\n",
      "step 4500: train loss 0.8807, val loss 0.9284\n",
      "step 4750: train loss 0.9045, val loss 0.9179\n",
      "step 5000: train loss 0.8930, val loss 0.8497\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 5250: train loss 0.9002, val loss 0.9023\n",
      "step 5500: train loss 0.9054, val loss 0.9548\n",
      "step 5750: train loss 0.8470, val loss 0.9222\n",
      "Training epoch 1 of 10\n",
      "step 6000: train loss 0.8998, val loss 0.9210\n",
      "step 6250: train loss 0.9108, val loss 0.9167\n",
      "step 6500: train loss 0.8460, val loss 0.9064\n",
      "step 6750: train loss 0.8790, val loss 0.9253\n",
      "step 7000: train loss 0.9022, val loss 0.8630\n",
      "step 7250: train loss 0.8880, val loss 0.9306\n",
      "step 7500: train loss 0.8783, val loss 0.9106\n",
      "step 7750: train loss 0.8638, val loss 0.8609\n",
      "step 8000: train loss 0.8335, val loss 0.8520\n",
      "step 8250: train loss 0.8993, val loss 0.8645\n",
      "step 8500: train loss 0.9081, val loss 0.8908\n",
      "step 8750: train loss 0.8823, val loss 0.8904\n",
      "step 9000: train loss 0.9067, val loss 0.9312\n",
      "step 9250: train loss 0.8772, val loss 0.9227\n",
      "step 9500: train loss 0.9137, val loss 0.8840\n",
      "step 9750: train loss 0.8543, val loss 0.8938\n",
      "Training epoch 2 of 10\n",
      "step 10000: train loss 0.8561, val loss 0.8796\n",
      "iter 10000: loss 0.7405, time 650.20ms\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:42<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=81.60% win[2]=18.20%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 15.27\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=127 win[1]=77.95% counts=Counter({1: 99, 2: 28})\n",
      "  a=2: n=171 win[1]=75.44% counts=Counter({1: 129, 2: 42})\n",
      "  a=3: n= 96 win[1]=77.08% counts=Counter({1: 74, 2: 21, None: 1})\n",
      "  a=4: n=164 win[1]=92.07% counts=Counter({1: 151, 2: 13})\n",
      "  a=5: n=289 win[1]=89.27% counts=Counter({1: 258, 2: 31})\n",
      "  a=6: n=309 win[1]=78.32% counts=Counter({1: 242, 2: 66, None: 1})\n",
      "  a=7: n=844 win[1]=80.45% counts=Counter({1: 679, 2: 163, None: 2})\n",
      "Training model on gen-4\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8452, val loss 0.8584\n",
      "iter 0: loss 1.1197, time 795.09ms\n",
      "step 250: train loss 0.8651, val loss 0.8691\n",
      "step 500: train loss 0.9404, val loss 0.9036\n",
      "step 750: train loss 0.8824, val loss 0.9169\n",
      "step 1000: train loss 0.8844, val loss 0.8767\n",
      "step 1250: train loss 0.8741, val loss 0.9022\n",
      "step 1500: train loss 0.8872, val loss 0.9983\n",
      "step 1750: train loss 0.9085, val loss 0.8158\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2000: train loss 0.8419, val loss 0.8328\n",
      "step 2250: train loss 0.8635, val loss 0.8474\n",
      "step 2500: train loss 0.8663, val loss 0.9438\n",
      "step 2750: train loss 0.8503, val loss 0.8673\n",
      "step 3000: train loss 0.8814, val loss 0.8655\n",
      "step 3250: train loss 0.8637, val loss 0.8386\n",
      "step 3500: train loss 0.8467, val loss 0.9268\n",
      "step 3750: train loss 0.8404, val loss 0.8549\n",
      "step 4000: train loss 0.8916, val loss 0.8726\n",
      "step 4250: train loss 0.8884, val loss 0.7809\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4500: train loss 0.7781, val loss 0.8650\n",
      "step 4750: train loss 0.8480, val loss 0.8052\n",
      "step 5000: train loss 0.8707, val loss 0.8883\n",
      "step 5250: train loss 0.8476, val loss 0.8936\n",
      "step 5500: train loss 0.9075, val loss 0.8326\n",
      "step 5750: train loss 0.9110, val loss 0.9294\n",
      "step 6000: train loss 0.8977, val loss 0.8225\n",
      "step 6250: train loss 0.8793, val loss 0.8456\n",
      "step 6500: train loss 0.8905, val loss 0.8774\n",
      "step 6750: train loss 0.8775, val loss 0.8195\n",
      "step 7000: train loss 0.8412, val loss 0.8947\n",
      "step 7250: train loss 0.8382, val loss 0.9239\n",
      "step 7500: train loss 0.8440, val loss 1.0062\n",
      "step 7750: train loss 0.8123, val loss 0.8381\n",
      "Training epoch 1 of 10\n",
      "step 8000: train loss 0.7947, val loss 0.8306\n",
      "step 8250: train loss 0.8827, val loss 0.8275\n",
      "step 8500: train loss 0.8225, val loss 0.9193\n",
      "step 8750: train loss 0.8312, val loss 0.8373\n",
      "step 9000: train loss 0.8390, val loss 0.8075\n",
      "step 9250: train loss 0.8469, val loss 0.8373\n",
      "step 9500: train loss 0.9188, val loss 0.8546\n",
      "step 9750: train loss 0.8278, val loss 0.8907\n",
      "Training epoch 2 of 10\n",
      "step 10000: train loss 0.8520, val loss 0.9544\n",
      "iter 10000: loss 0.8251, time 766.14ms\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:29<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=79.20% win[2]=20.55%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 16.35\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=211 win[1]=67.77% counts=Counter({1: 143, 2: 67, None: 1})\n",
      "  a=2: n=281 win[1]=77.58% counts=Counter({1: 218, 2: 62, None: 1})\n",
      "  a=3: n=136 win[1]=79.41% counts=Counter({1: 108, 2: 28})\n",
      "  a=5: n=379 win[1]=84.96% counts=Counter({1: 322, 2: 57})\n",
      "  a=6: n=385 win[1]=80.78% counts=Counter({1: 311, 2: 72, None: 2})\n",
      "  a=7: n=608 win[1]=79.28% counts=Counter({1: 482, 2: 125, None: 1})\n",
      "Training model on gen-5\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8800, val loss 0.8751\n",
      "iter 0: loss 0.7769, time 700.96ms\n",
      "step 250: train loss 0.8825, val loss 0.8552\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.8574, val loss 0.8914\n",
      "step 750: train loss 0.8883, val loss 0.8659\n",
      "step 1000: train loss 0.9721, val loss 0.9549\n",
      "step 1250: train loss 0.8460, val loss 0.9026\n",
      "step 1500: train loss 0.7990, val loss 0.8761\n",
      "step 1750: train loss 0.8322, val loss 0.8573\n",
      "step 2000: train loss 0.8868, val loss 0.9485\n",
      "step 2250: train loss 0.8719, val loss 0.8642\n",
      "step 2500: train loss 0.8710, val loss 0.8786\n",
      "step 2750: train loss 0.9034, val loss 0.8980\n",
      "step 3000: train loss 0.8202, val loss 0.8891\n",
      "step 3250: train loss 0.8758, val loss 0.8583\n",
      "step 3500: train loss 0.8628, val loss 0.8528\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 3750: train loss 0.8589, val loss 0.8533\n",
      "step 4000: train loss 0.8869, val loss 0.8417\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4250: train loss 0.9163, val loss 0.8668\n",
      "step 4500: train loss 0.8711, val loss 0.8129\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4750: train loss 0.8642, val loss 0.8021\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 5000: train loss 0.8851, val loss 0.8618\n",
      "step 5250: train loss 0.9009, val loss 0.8665\n",
      "step 5500: train loss 0.8545, val loss 0.8249\n",
      "step 5750: train loss 0.8701, val loss 0.9004\n",
      "step 6000: train loss 0.8213, val loss 0.8615\n",
      "step 6250: train loss 0.8259, val loss 0.7753\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 6500: train loss 0.8404, val loss 0.8787\n",
      "step 6750: train loss 0.8820, val loss 0.9000\n",
      "step 7000: train loss 0.8600, val loss 0.8859\n",
      "step 7250: train loss 0.8954, val loss 0.8634\n",
      "step 7500: train loss 0.8461, val loss 0.8582\n",
      "step 7750: train loss 0.8452, val loss 0.9402\n",
      "step 8000: train loss 0.8125, val loss 0.8587\n",
      "step 8250: train loss 0.8800, val loss 0.9004\n",
      "step 8500: train loss 0.8299, val loss 0.8628\n",
      "step 8750: train loss 0.8598, val loss 0.8887\n",
      "step 9000: train loss 0.8913, val loss 0.8770\n",
      "step 9250: train loss 0.9016, val loss 0.8418\n",
      "step 9500: train loss 0.8607, val loss 0.8822\n",
      "step 9750: train loss 0.9219, val loss 0.9124\n",
      "Training epoch 1 of 10\n",
      "step 10000: train loss 0.8761, val loss 0.8771\n",
      "iter 10000: loss 1.5947, time 711.07ms\n",
      "\n",
      "\n",
      "## Running generation 6 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [6:08:34<00:00, 11.06s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=74.45% win[2]=25.55%, n=2000\n",
      "Game Length min: 7, max: 40, mean: 16.72\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=248 win[1]=68.15% counts=Counter({1: 169, 2: 79})\n",
      "  a=2: n=277 win[1]=74.37% counts=Counter({1: 206, 2: 71})\n",
      "  a=3: n=194 win[1]=77.32% counts=Counter({1: 150, 2: 44})\n",
      "  a=4: n=137 win[1]=83.94% counts=Counter({1: 115, 2: 22})\n",
      "  a=5: n=398 win[1]=79.90% counts=Counter({1: 318, 2: 80})\n",
      "  a=6: n=320 win[1]=71.88% counts=Counter({1: 230, 2: 90})\n",
      "  a=7: n=426 win[1]=70.66% counts=Counter({1: 301, 2: 125})\n",
      "Training model on gen-6\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8402, val loss 0.8619\n",
      "iter 0: loss 0.5096, time 676.92ms\n",
      "step 250: train loss 0.8390, val loss 0.9075\n",
      "step 500: train loss 0.9353, val loss 0.8651\n",
      "step 750: train loss 0.9134, val loss 0.8200\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 1000: train loss 0.8787, val loss 0.8699\n",
      "step 1250: train loss 0.8958, val loss 0.8796\n",
      "step 1500: train loss 0.8603, val loss 0.8623\n",
      "step 1750: train loss 0.9573, val loss 0.9134\n",
      "step 2000: train loss 0.9615, val loss 0.9064\n",
      "step 2250: train loss 0.9570, val loss 0.9344\n",
      "step 2500: train loss 0.8772, val loss 0.8583\n",
      "step 2750: train loss 0.8921, val loss 0.8904\n",
      "step 3000: train loss 0.8811, val loss 0.8929\n",
      "step 3250: train loss 0.8841, val loss 0.8587\n",
      "step 3500: train loss 0.8887, val loss 0.8270\n",
      "step 3750: train loss 0.8953, val loss 0.8512\n",
      "step 4000: train loss 0.9033, val loss 0.8547\n",
      "step 4250: train loss 0.8774, val loss 0.8582\n",
      "step 4500: train loss 0.8653, val loss 0.9135\n",
      "step 4750: train loss 0.8855, val loss 0.9012\n",
      "step 5000: train loss 0.8634, val loss 0.8879\n",
      "step 5250: train loss 0.8610, val loss 0.8960\n",
      "step 5500: train loss 0.8675, val loss 0.8930\n",
      "step 5750: train loss 0.8094, val loss 0.8862\n",
      "step 6000: train loss 0.8594, val loss 0.8788\n",
      "step 6250: train loss 0.8792, val loss 0.8615\n",
      "step 6500: train loss 0.8540, val loss 0.8719\n",
      "step 6750: train loss 0.8885, val loss 0.8170\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 7000: train loss 0.8717, val loss 0.8574\n",
      "step 7250: train loss 0.9166, val loss 0.8666\n",
      "step 7500: train loss 0.8414, val loss 0.8583\n",
      "step 7750: train loss 0.8897, val loss 0.8704\n",
      "step 8000: train loss 0.8436, val loss 0.8671\n",
      "step 8250: train loss 0.9023, val loss 0.8564\n",
      "step 8500: train loss 0.8845, val loss 0.8547\n",
      "step 8750: train loss 0.8655, val loss 0.9572\n",
      "step 9000: train loss 0.8961, val loss 0.8644\n",
      "step 9250: train loss 0.8023, val loss 0.8776\n",
      "step 9500: train loss 0.8739, val loss 0.9142\n",
      "step 9750: train loss 0.8799, val loss 0.8636\n",
      "Training epoch 1 of 10\n",
      "step 10000: train loss 0.8713, val loss 0.8771\n",
      "iter 10000: loss 0.9548, time 653.18ms\n",
      "\n",
      "\n",
      "## Running generation 7 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:37<00:00,  5.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=71.80% win[2]=28.10%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 16.48\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=273 win[1]=65.57% counts=Counter({1: 179, 2: 94})\n",
      "  a=2: n=262 win[1]=70.23% counts=Counter({1: 184, 2: 78})\n",
      "  a=3: n=151 win[1]=74.83% counts=Counter({1: 113, 2: 38})\n",
      "  a=4: n=194 win[1]=85.57% counts=Counter({1: 166, 2: 28})\n",
      "  a=5: n=424 win[1]=72.64% counts=Counter({1: 308, 2: 116})\n",
      "  a=6: n=315 win[1]=73.33% counts=Counter({1: 231, 2: 83, None: 1})\n",
      "  a=7: n=381 win[1]=66.93% counts=Counter({1: 255, 2: 125, None: 1})\n",
      "Training model on gen-7\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.9137, val loss 0.9247\n",
      "iter 0: loss 0.4834, time 748.08ms\n",
      "step 250: train loss 0.9224, val loss 0.8732\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.8249, val loss 0.7689\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 750: train loss 0.8731, val loss 0.8707\n",
      "step 1000: train loss 0.8802, val loss 0.8962\n",
      "step 1250: train loss 0.9247, val loss 0.9047\n",
      "step 1500: train loss 0.8880, val loss 0.8924\n",
      "step 1750: train loss 0.9560, val loss 0.8666\n",
      "step 2000: train loss 0.8969, val loss 0.8955\n",
      "step 2250: train loss 0.8479, val loss 0.9163\n",
      "step 2500: train loss 0.9128, val loss 0.9007\n",
      "step 2750: train loss 0.8993, val loss 0.9161\n",
      "step 3000: train loss 0.8816, val loss 0.8803\n",
      "step 3250: train loss 0.9542, val loss 0.8930\n",
      "step 3500: train loss 0.8719, val loss 0.8808\n",
      "step 3750: train loss 0.8976, val loss 0.9081\n",
      "step 4000: train loss 0.8961, val loss 0.9055\n",
      "step 4250: train loss 0.8759, val loss 0.8899\n",
      "step 4500: train loss 0.9011, val loss 0.8134\n",
      "step 4750: train loss 0.8217, val loss 0.8311\n",
      "step 5000: train loss 0.9262, val loss 0.8865\n",
      "step 5250: train loss 0.9533, val loss 0.8181\n",
      "step 5500: train loss 0.9371, val loss 0.8659\n",
      "step 5750: train loss 0.8484, val loss 0.8739\n",
      "step 6000: train loss 0.8729, val loss 0.8900\n",
      "step 6250: train loss 0.8750, val loss 0.8553\n",
      "step 6500: train loss 0.9557, val loss 0.9469\n",
      "step 6750: train loss 0.9288, val loss 0.8569\n",
      "step 7000: train loss 0.8683, val loss 0.8500\n",
      "step 7250: train loss 0.9061, val loss 0.8852\n",
      "step 7500: train loss 0.8509, val loss 0.9326\n",
      "step 7750: train loss 0.9096, val loss 0.8917\n",
      "step 8000: train loss 0.8869, val loss 0.9111\n",
      "step 8250: train loss 0.8476, val loss 0.9372\n",
      "step 8500: train loss 0.9084, val loss 0.9309\n",
      "step 8750: train loss 0.9493, val loss 0.8868\n",
      "step 9000: train loss 0.9243, val loss 0.8754\n",
      "step 9250: train loss 0.8176, val loss 0.8774\n",
      "step 9500: train loss 0.9411, val loss 0.8903\n",
      "step 9750: train loss 0.9223, val loss 0.8762\n",
      "Training epoch 1 of 10\n",
      "step 10000: train loss 0.8659, val loss 0.8595\n",
      "iter 10000: loss 0.3578, time 654.16ms\n",
      "\n",
      "\n",
      "## Running generation 8 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [35:55<00:00,  1.08s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=69.65% win[2]=30.15%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 17.11\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=322 win[1]=62.11% counts=Counter({1: 200, 2: 120, None: 2})\n",
      "  a=2: n=306 win[1]=71.90% counts=Counter({1: 220, 2: 85, None: 1})\n",
      "  a=3: n=219 win[1]=68.49% counts=Counter({1: 150, 2: 69})\n",
      "  a=4: n=160 win[1]=85.62% counts=Counter({1: 137, 2: 23})\n",
      "  a=5: n=451 win[1]=71.18% counts=Counter({1: 321, 2: 129, None: 1})\n",
      "  a=6: n=327 win[1]=69.42% counts=Counter({1: 227, 2: 100})\n",
      "  a=7: n=215 win[1]=64.19% counts=Counter({1: 138, 2: 77})\n",
      "Training model on gen-8\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.9348, val loss 0.8634\n",
      "iter 0: loss 0.3115, time 649.77ms\n",
      "step 250: train loss 0.9272, val loss 0.8559\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.9016, val loss 0.8975\n",
      "step 750: train loss 0.8659, val loss 0.8878\n",
      "step 1000: train loss 0.9173, val loss 0.9019\n",
      "step 1250: train loss 0.8903, val loss 0.8856\n",
      "step 1500: train loss 0.8691, val loss 0.8976\n",
      "step 1750: train loss 0.8548, val loss 0.8923\n",
      "step 2000: train loss 0.9848, val loss 1.0047\n",
      "step 2250: train loss 0.8444, val loss 0.9101\n",
      "step 2500: train loss 0.8615, val loss 0.9028\n",
      "step 2750: train loss 0.8723, val loss 0.9270\n",
      "step 3000: train loss 0.9232, val loss 0.8793\n",
      "step 3250: train loss 0.8577, val loss 0.8684\n",
      "step 3500: train loss 0.8995, val loss 0.8856\n",
      "step 3750: train loss 0.8670, val loss 0.8950\n",
      "step 4000: train loss 0.8655, val loss 0.8895\n",
      "step 4250: train loss 0.8519, val loss 0.8810\n",
      "step 4500: train loss 0.8823, val loss 0.8548\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4750: train loss 0.8582, val loss 0.9372\n",
      "step 5000: train loss 0.8221, val loss 0.8567\n",
      "step 5250: train loss 0.9238, val loss 0.7830\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 5500: train loss 0.8783, val loss 0.9433\n",
      "step 5750: train loss 0.9136, val loss 0.8898\n",
      "step 6000: train loss 0.8649, val loss 0.8358\n",
      "step 6250: train loss 0.8674, val loss 0.8899\n",
      "step 6500: train loss 0.8394, val loss 0.9010\n",
      "step 6750: train loss 0.9126, val loss 0.9150\n",
      "step 7000: train loss 0.8649, val loss 0.8900\n",
      "step 7250: train loss 0.9235, val loss 0.9102\n",
      "step 7500: train loss 0.8849, val loss 0.8203\n",
      "step 7750: train loss 0.8292, val loss 0.8330\n",
      "step 8000: train loss 0.8783, val loss 0.9132\n",
      "step 8250: train loss 0.9015, val loss 0.8978\n",
      "step 8500: train loss 0.8351, val loss 0.8723\n",
      "step 8750: train loss 0.8910, val loss 0.8646\n",
      "step 9000: train loss 0.8846, val loss 0.9092\n",
      "step 9250: train loss 0.8852, val loss 0.8697\n",
      "step 9500: train loss 0.9086, val loss 0.9070\n",
      "step 9750: train loss 0.8655, val loss 0.9124\n",
      "Training epoch 1 of 10\n",
      "step 10000: train loss 0.8835, val loss 0.8731\n",
      "iter 10000: loss 1.2011, time 698.50ms\n",
      "\n",
      "\n",
      "## Running generation 9 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Playing 2000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:21<00:00,  3.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=53.30% win[2]=46.60%, n=2000\n",
      "Game Length min: 7, max: 42, mean: 16.93\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=341 win[1]=48.09% counts=Counter({2: 176, 1: 164, None: 1})\n",
      "  a=2: n=334 win[1]=50.90% counts=Counter({1: 170, 2: 163, None: 1})\n",
      "  a=3: n=268 win[1]=55.60% counts=Counter({1: 149, 2: 119})\n",
      "  a=4: n= 92 win[1]=92.39% counts=Counter({1: 85, 2: 7})\n",
      "  a=5: n=447 win[1]=54.81% counts=Counter({1: 245, 2: 202})\n",
      "  a=6: n=238 win[1]=55.04% counts=Counter({1: 131, 2: 107})\n",
      "  a=7: n=280 win[1]=43.57% counts=Counter({2: 158, 1: 122})\n",
      "Training model on gen-9\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8265, val loss 0.9363\n",
      "iter 0: loss 0.4596, time 912.02ms\n",
      "step 250: train loss 0.9797, val loss 0.9409\n",
      "step 500: train loss 0.9528, val loss 0.9221\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 750: train loss 0.8662, val loss 0.9361\n",
      "step 1000: train loss 0.9874, val loss 0.9955\n",
      "step 1250: train loss 0.8725, val loss 0.9487\n",
      "step 1500: train loss 0.8995, val loss 0.9715\n",
      "step 1750: train loss 0.8670, val loss 0.8725\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2000: train loss 0.9062, val loss 0.9349\n",
      "step 2250: train loss 0.9109, val loss 0.9068\n",
      "step 2500: train loss 0.9352, val loss 0.9737\n",
      "step 2750: train loss 0.8283, val loss 0.9171\n",
      "step 3000: train loss 0.9343, val loss 0.9359\n",
      "step 3250: train loss 0.9174, val loss 0.8398\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 3500: train loss 0.8985, val loss 0.8792\n",
      "step 3750: train loss 0.9036, val loss 0.8920\n",
      "step 4000: train loss 0.9291, val loss 0.8615\n",
      "step 4250: train loss 0.8824, val loss 0.8494\n",
      "step 4500: train loss 0.8850, val loss 0.8305\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4750: train loss 0.9205, val loss 0.9726\n",
      "step 5000: train loss 0.8830, val loss 0.8847\n",
      "step 5250: train loss 0.9053, val loss 0.8548\n",
      "step 5500: train loss 0.9516, val loss 0.8956\n",
      "step 5750: train loss 0.9008, val loss 0.8347\n",
      "step 6000: train loss 0.8990, val loss 0.9171\n",
      "step 6250: train loss 0.9107, val loss 0.9651\n",
      "step 6500: train loss 0.9169, val loss 0.9708\n",
      "step 6750: train loss 0.9679, val loss 0.8625\n",
      "step 7000: train loss 0.9128, val loss 0.9180\n",
      "step 7250: train loss 0.9207, val loss 0.8833\n",
      "step 7500: train loss 0.9087, val loss 0.9275\n",
      "step 7750: train loss 0.8154, val loss 0.8294\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 8000: train loss 0.8700, val loss 0.8746\n",
      "step 8250: train loss 0.8587, val loss 0.9285\n",
      "step 8500: train loss 0.8612, val loss 0.9047\n",
      "step 8750: train loss 0.9280, val loss 0.9195\n",
      "step 9000: train loss 0.9080, val loss 0.8657\n",
      "step 9250: train loss 0.9750, val loss 0.8963\n",
      "step 9500: train loss 0.8853, val loss 0.8954\n",
      "step 9750: train loss 0.9213, val loss 0.8953\n",
      "Training epoch 1 of 10\n",
      "step 10000: train loss 0.9002, val loss 0.8909\n",
      "iter 10000: loss 0.8617, time 691.26ms\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "# 10 hours!\n",
    "for generation_id in range(1, 10):\n",
    "    current_model = model_dict[generation_id-1]\n",
    "    results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "    results_dict[generation_id] = results_i\n",
    "    trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "    model_dict[generation_id] = model_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dd) = 2644\n"
     ]
    }
   ],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: [0, 0]))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in [0, gen]:\n",
    "        dd[tuple(tuple(d.action[:0].tolist()))][g][int(d.value[0][0])] += 1\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g][int(d.value[0][0])] += 1\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g][int(d.value[0][0])] += 1\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g][int(d.value[0][0])] += 1\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g][int(d.value[0][0])] += 1\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen=0: [557, 1152], win_pct=32.59%, sum=1709\n",
      "gen=1: [49, 190], win_pct=20.50%, sum=239\n",
      "gen=2: [144, 69], win_pct=67.61%, sum=213\n",
      "gen=3: [44, 149], win_pct=22.80%, sum=193\n",
      "gen=4: [22, 74], win_pct=22.92%, sum=96\n",
      "gen=5: [28, 108], win_pct=20.59%, sum=136\n",
      "gen=6: [44, 150], win_pct=22.68%, sum=194\n",
      "gen=7: [38, 113], win_pct=25.17%, sum=151\n",
      "gen=8: [69, 150], win_pct=31.51%, sum=219\n",
      "gen=9: [119, 149], win_pct=44.40%, sum=268\n"
     ]
    }
   ],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "prefix = (3,)\n",
    "for gen, counts in dd[prefix].items():\n",
    "    print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
