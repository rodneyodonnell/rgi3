{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"small\"  # \"tiny\" or \"small\" or\"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 50\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "PLAY_GAMES = True\n",
    "PLAY_GAMES_V2 = True\n",
    "NUM_GAMES = 2000\n",
    "MAX_TRAINING_ITERS = 10000\n",
    "SAVE_FILE_NAME = f'train_{NUM_GAMES}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(base_game.all_actions()))\n",
    "n_max_context = max_game_length + 2\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {base_game.__class__.__name__}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create or load model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    raise NotImplementedError(\"Model loading not implemented\")\n",
    "else:\n",
    "    model = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Play games to generate training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing 2000 games, simulations=50, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:58<00:00, 33.99it/s] \n"
     ]
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, QueuedNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory):\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        t0 = time.time()\n",
    "        player = player_factory()\n",
    "        game_result = await play_game_async(game, [player, player])\n",
    "        t1 = time.time()\n",
    "        game_result['time'] = t1 - t0\n",
    "        return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "\n",
    "serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "\n",
    "master_rng = np.random.default_rng(42)\n",
    "async_evaluator_factory = lambda: AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=NUM_SIMULATIONS)\n",
    "\n",
    "if PLAY_GAMES:\n",
    "    print(f\"Playing {NUM_GAMES} games, simulations={NUM_SIMULATIONS}, model_size={MODEL_SIZE}\")\n",
    "    await async_evaluator.start()\n",
    "    results = asyncio.run(play_games_async(num_games=NUM_GAMES, player_factory=async_player_factory)) # 50 games, 4.3s\n",
    "    await async_evaluator.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner by initial move:\n",
      "  a=1: n=254 win[1]=53.94% counts=Counter({1: 137, 2: 117})\n",
      "  a=2: n=292 win[1]=61.64% counts=Counter({1: 180, 2: 112})\n",
      "  a=3: n=184 win[1]=67.39% counts=Counter({1: 124, 2: 60})\n",
      "  a=4: n=315 win[1]=75.24% counts=Counter({1: 237, 2: 78})\n",
      "  a=5: n=291 win[1]=65.29% counts=Counter({1: 190, 2: 101})\n",
      "  a=6: n=331 win[1]=63.14% counts=Counter({1: 209, 2: 122})\n",
      "  a=7: n=333 win[1]=50.45% counts=Counter({1: 168, 2: 165})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "if PLAY_GAMES:\n",
    "    print(\"Winner by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")\n",
    "\n",
    "## Resunts from 1000 connect4 games with random model.\n",
    "# Winner by initial move:\n",
    "#   a=1: n=121 win[1]=59.50% counts=Counter({1: 72, 2: 49})\n",
    "#   a=2: n=139 win[1]=58.99% counts=Counter({1: 82, 2: 57})\n",
    "#   a=3: n= 95 win[1]=63.16% counts=Counter({1: 60, 2: 35})\n",
    "#   a=4: n=160 win[1]=78.75% counts=Counter({1: 126, 2: 34})\n",
    "#   a=5: n=146 win[1]=65.07% counts=Counter({1: 95, 2: 51})\n",
    "#   a=6: n=176 win[1]=61.36% counts=Counter({1: 108, 2: 68})\n",
    "#   a=7: n=163 win[1]=53.37% counts=Counter({1: 87, 2: 76})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: Vocab(vocab_size=8, itos=['START_OF_GAME', 1, 2, 3, 4, 5, 6, 7], stoi={'START_OF_GAME': 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7})\n"
     ]
    }
   ],
   "source": [
    "all_actions = game.all_actions()\n",
    "# num_players = game.num_players(state_0)\n",
    "print(f\"Vocab: {action_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "if PLAY_GAMES:\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    td_builder.save(DATA_DIR, SAVE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner counts: win[1]=62.25% win[2]=37.75%, n=2000\n",
      "Winner stats_by initial move:\n",
      "  a=1: n=254 win[1]=53.94% counts=Counter({1: 137, 0: 117})\n",
      "  a=2: n=292 win[1]=61.64% counts=Counter({1: 180, 0: 112})\n",
      "  a=3: n=184 win[1]=67.39% counts=Counter({1: 124, 0: 60})\n",
      "  a=4: n=315 win[1]=75.24% counts=Counter({1: 237, 0: 78})\n",
      "  a=5: n=291 win[1]=65.29% counts=Counter({1: 190, 0: 101})\n",
      "  a=6: n=331 win[1]=63.14% counts=Counter({1: 209, 0: 122})\n",
      "  a=7: n=333 win[1]=50.45% counts=Counter({1: 168, 0: 165})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "td = TrajectoryDataset(DATA_DIR, SAVE_FILE_NAME, block_size=n_max_context)\n",
    "\n",
    "# Confirm results are the same as the saved result\n",
    "winner_stats = Counter(result['winner'] for result in results)\n",
    "print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "print(\"Winner stats_by initial move:\")\n",
    "dd = defaultdict(Counter)\n",
    "for _td in td:\n",
    "    action, winner = int(_td.action[0]), int(_td.value[0][0])\n",
    "    dd[action][winner] += 1\n",
    "for action, counts in sorted(dd.items()):\n",
    "    print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[6, 2, 2, 5, 4, 5, 6, 7, 3, 7, 2, 4, 6, 4, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='mps:0'), tensor([[[0.0000, 0.1200, 0.1600, 0.1000, 0.1600, 0.1400, 0.1600, 0.1600],\n",
      "         [0.0000, 0.1400, 0.1600, 0.1600, 0.1200, 0.1400, 0.1400, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1800, 0.1000, 0.1600, 0.1400, 0.1400, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1800, 0.1600, 0.1200, 0.1200, 0.1200, 0.1600],\n",
      "         [0.0000, 0.1200, 0.1400, 0.1000, 0.1600, 0.1800, 0.1600, 0.1400],\n",
      "         [0.0000, 0.1200, 0.1400, 0.1400, 0.1800, 0.1400, 0.1200, 0.1600],\n",
      "         [0.0000, 0.1200, 0.1400, 0.1000, 0.1800, 0.1800, 0.1400, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1600, 0.1600, 0.1200, 0.1400, 0.1400, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1400, 0.1200, 0.1600, 0.1200, 0.1400, 0.1800],\n",
      "         [0.0000, 0.1400, 0.1600, 0.2000, 0.1000, 0.1400, 0.1200, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1400, 0.1200, 0.1600, 0.1400, 0.1200, 0.1800],\n",
      "         [0.0000, 0.1400, 0.1800, 0.1600, 0.1200, 0.1200, 0.1200, 0.1600],\n",
      "         [0.0000, 0.1200, 0.1400, 0.0600, 0.2200, 0.1600, 0.1600, 0.1400],\n",
      "         [0.0000, 0.1400, 0.1600, 0.1600, 0.1200, 0.1400, 0.1400, 0.1400],\n",
      "         [0.0000, 0.0200, 0.0200, 0.0200, 0.0600, 0.0400, 0.8000, 0.0400],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       device='mps:0'), tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]], device='mps:0'))\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0 if DEBUG_MODE else 4\n",
    "trajectory_loader = build_trajectory_loader(\n",
    "    DATA_DIR, 'train_1000', block_size=n_max_context, batch_size=1,\n",
    "    device=device, workers=num_workers)\n",
    "\n",
    "for batch in trajectory_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 10\n",
      "step 0: train loss 0.8588, val loss 0.8588\n",
      "iter 0: loss 0.9355, time 1009.85ms\n",
      "step 250: train loss 0.8253, val loss 0.8253\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 500: train loss 0.8457, val loss 0.8457\n",
      "step 750: train loss 0.8281, val loss 0.8281\n",
      "Training epoch 1 of 10\n",
      "step 1000: train loss 0.8560, val loss 0.8560\n",
      "step 1250: train loss 0.8222, val loss 0.8222\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 1500: train loss 0.8389, val loss 0.8389\n",
      "step 1750: train loss 0.8299, val loss 0.8299\n",
      "Training epoch 2 of 10\n",
      "step 2000: train loss 0.8492, val loss 0.8492\n",
      "step 2250: train loss 0.8217, val loss 0.8217\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 2500: train loss 0.8324, val loss 0.8324\n",
      "step 2750: train loss 0.8265, val loss 0.8265\n",
      "Training epoch 3 of 10\n",
      "step 3000: train loss 0.8378, val loss 0.8378\n",
      "step 3250: train loss 0.8143, val loss 0.8143\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 3500: train loss 0.8214, val loss 0.8214\n",
      "step 3750: train loss 0.8206, val loss 0.8206\n",
      "Training epoch 4 of 10\n",
      "step 4000: train loss 0.8301, val loss 0.8301\n",
      "step 4250: train loss 0.8135, val loss 0.8135\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 4500: train loss 0.8193, val loss 0.8193\n",
      "step 4750: train loss 0.8187, val loss 0.8187\n",
      "Training epoch 5 of 10\n",
      "step 5000: train loss 0.8245, val loss 0.8245\n",
      "step 5250: train loss 0.8117, val loss 0.8117\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 5500: train loss 0.8191, val loss 0.8191\n",
      "step 5750: train loss 0.8186, val loss 0.8186\n",
      "Training epoch 6 of 10\n",
      "step 6000: train loss 0.8248, val loss 0.8248\n",
      "step 6250: train loss 0.8096, val loss 0.8096\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 6500: train loss 0.8191, val loss 0.8191\n",
      "step 6750: train loss 0.8178, val loss 0.8178\n",
      "Training epoch 7 of 10\n",
      "step 7000: train loss 0.8251, val loss 0.8251\n",
      "step 7250: train loss 0.8081, val loss 0.8081\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 7500: train loss 0.8190, val loss 0.8190\n",
      "step 7750: train loss 0.8158, val loss 0.8158\n",
      "Training epoch 8 of 10\n",
      "step 8000: train loss 0.8254, val loss 0.8254\n",
      "step 8250: train loss 0.8069, val loss 0.8069\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 8500: train loss 0.8194, val loss 0.8194\n",
      "step 8750: train loss 0.8142, val loss 0.8142\n",
      "Training epoch 9 of 10\n",
      "step 9000: train loss 0.8260, val loss 0.8260\n",
      "step 9250: train loss 0.8061, val loss 0.8061\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "step 9500: train loss 0.8202, val loss 0.8202\n",
      "step 9750: train loss 0.8133, val loss 0.8133\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 250,  # keep frequent because we'll overfit\n",
    "    eval_iters = 200,\n",
    "    log_interval = 10_000,  # don't print too too often\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 64,\n",
    "\n",
    "    learning_rate = 1e-3,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = 5000,  # make equal to max_iters usually\n",
    "    min_lr = 1e-4,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 100,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=trajectory_loader,\n",
    "    val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionHistoryTransformer(\n",
       "  (action_embedding): Embedding(8, 32)\n",
       "  (transformer): Transformer(\n",
       "    (wpe): Embedding(44, 32)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=32, out_features=96, bias=False)\n",
       "          (c_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (policy_value_head): PolicyValueHead(\n",
       "    (policy): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataclasses\n",
    "# Save model\n",
    "MODEL_PATH = MODEL_DIR / f\"{SAVE_FILE_NAME}-round1.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'model_config': dataclasses.asdict(model.config),\n",
    "    'vocab': action_vocab.to_dict(),\n",
    "    'iter_num': trainer.iter_num,\n",
    "    'best_val_loss': trainer.best_val_loss,\n",
    "    'num_players': game.num_players(state_0),\n",
    "}\n",
    "torch.save(checkpoint, DATA_DIR / f\"{SAVE_FILE_NAME}-round1-checkpoint.pt\")\n",
    "\n",
    "loaded_checkpoint = torch.load(DATA_DIR / f\"{SAVE_FILE_NAME}-round1-checkpoint.pt\")\n",
    "loaded_model = ActionHistoryTransformer(\n",
    "    config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "    action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "    num_players=loaded_checkpoint['num_players']\n",
    ")\n",
    "loaded_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:00<00:00, 33.16it/s] \n"
     ]
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "\n",
    "serial_evaluator_v2 = ActionHistoryTransformerEvaluator(loaded_model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "async_evaluator_v2 = AsyncNetworkEvaluator(base_evaluator=serial_evaluator_v2, max_batch_size=1024, verbose=False)\n",
    "\n",
    "master_rng = np.random.default_rng(42)\n",
    "async_evaluator_factory_v2 = lambda: AsyncNetworkEvaluator(base_evaluator=serial_evaluator_v2, max_batch_size=1024, verbose=False)\n",
    "async_player_factory_v2 = lambda: AlphazeroPlayer(game, async_evaluator_v2, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=NUM_SIMULATIONS)\n",
    "\n",
    "if PLAY_GAMES_V2:\n",
    "    await async_evaluator_v2.start()\n",
    "    results_v2 = asyncio.run(play_games_async(num_games=NUM_GAMES, player_factory=async_player_factory_v2))\n",
    "    await async_evaluator_v2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=61.90% win[2]=38.00%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=289 win[1]=51.90% counts=Counter({1: 150, 2: 139})\n",
      "  a=2: n=257 win[1]=57.59% counts=Counter({1: 148, 2: 109})\n",
      "  a=3: n=310 win[1]=63.23% counts=Counter({1: 196, 2: 114})\n",
      "  a=4: n=261 win[1]=74.33% counts=Counter({1: 194, 2: 66, None: 1})\n",
      "  a=5: n=354 win[1]=67.80% counts=Counter({1: 240, 2: 113, None: 1})\n",
      "  a=6: n=276 win[1]=60.14% counts=Counter({1: 166, 2: 110})\n",
      "  a=7: n=253 win[1]=56.92% counts=Counter({1: 144, 2: 109})\n"
     ]
    }
   ],
   "source": [
    "print(\"Winner Stats:\")\n",
    "winner_stats = Counter(result['winner'] for result in results_v2)\n",
    "print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "print(\"Winner Stats by initial move:\")\n",
    "dd = defaultdict(Counter)\n",
    "if PLAY_GAMES_V2:\n",
    "    for result in results_v2:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")\n",
    "\n",
    "\n",
    "\n",
    "## Stats for simulations=50, elapsed 1m03s.\n",
    "# Winner Stats:\n",
    "# Winner counts: win[1]=61.90% win[2]=38.00%, n=2000\n",
    "# Winner Stats by initial move:\n",
    "#   a=1: n=289 win[1]=51.90% counts=Counter({1: 150, 2: 139})\n",
    "#   a=2: n=257 win[1]=57.59% counts=Counter({1: 148, 2: 109})\n",
    "#   a=3: n=310 win[1]=63.23% counts=Counter({1: 196, 2: 114})\n",
    "#   a=4: n=261 win[1]=74.33% counts=Counter({1: 194, 2: 66, None: 1})\n",
    "#   a=5: n=354 win[1]=67.80% counts=Counter({1: 240, 2: 113, None: 1})\n",
    "#   a=6: n=276 win[1]=60.14% counts=Counter({1: 166, 2: 110})\n",
    "#   a=7: n=253 win[1]=56.92% counts=Counter({1: 144, 2: 109})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13999999, 0.13999999, 0.16      , 0.13999999, 0.16      , 0.13999999, 0.12      ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_v2[0]['legal_policies'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:57<00:00,  6.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=62.50% win[2]=37.50%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=280 win[1]=50.00% counts=Counter({2: 140, 1: 140})\n",
      "  a=2: n=288 win[1]=61.81% counts=Counter({1: 178, 2: 110})\n",
      "  a=3: n=299 win[1]=63.55% counts=Counter({1: 190, 2: 109})\n",
      "  a=4: n=242 win[1]=77.69% counts=Counter({1: 188, 2: 54})\n",
      "  a=5: n=316 win[1]=65.51% counts=Counter({1: 207, 2: 109})\n",
      "  a=6: n=311 win[1]=62.06% counts=Counter({1: 193, 2: 118})\n",
      "  a=7: n=264 win[1]=58.33% counts=Counter({1: 154, 2: 110})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "async def play_generation(model, num_games, simulations=NUM_SIMULATIONS):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")\n",
    "\n",
    "\n",
    "results_v2_100 = asyncio.run(play_generation(loaded_model, num_games=2000, simulations=200))\n",
    "print_game_stats(results_v2_100)\n",
    "\n",
    "## Simulation=100, elapsed 2m12s.\n",
    "# 100%|██████████| 2000/2000 [02:12<00:00, 15.06it/s]\n",
    "# Winner Stats:\n",
    "# Winner counts: win[1]=61.75% win[2]=38.20%, n=2000\n",
    "# Winner Stats by initial move:\n",
    "#   a=1: n=273 win[1]=53.48% counts=Counter({1: 146, 2: 127})\n",
    "#   a=2: n=285 win[1]=57.19% counts=Counter({1: 163, 2: 122})\n",
    "#   a=3: n=298 win[1]=65.10% counts=Counter({1: 194, 2: 104})\n",
    "#   a=4: n=242 win[1]=73.14% counts=Counter({1: 177, 2: 64, None: 1})\n",
    "#   a=5: n=347 win[1]=65.71% counts=Counter({1: 228, 2: 119})\n",
    "#   a=6: n=302 win[1]=60.93% counts=Counter({1: 184, 2: 118})\n",
    "#   a=7: n=253 win[1]=56.52% counts=Counter({1: 143, 2: 110})\n",
    "\n",
    "## Simulations=200, elapsed 4m57s.\n",
    "# 100%|██████████| 2000/2000 [04:57<00:00,  6.72it/s] \n",
    "# Winner Stats:\n",
    "# Winner counts: win[1]=62.50% win[2]=37.50%, n=2000\n",
    "# Winner Stats by initial move:\n",
    "#   a=1: n=280 win[1]=50.00% counts=Counter({2: 140, 1: 140})\n",
    "#   a=2: n=288 win[1]=61.81% counts=Counter({1: 178, 2: 110})\n",
    "#   a=3: n=299 win[1]=63.55% counts=Counter({1: 190, 2: 109})\n",
    "#   a=4: n=242 win[1]=77.69% counts=Counter({1: 188, 2: 54})\n",
    "#   a=5: n=316 win[1]=65.51% counts=Counter({1: 207, 2: 109})\n",
    "#   a=6: n=311 win[1]=62.06% counts=Counter({1: 193, 2: 118})\n",
    "#   a=7: n=264 win[1]=58.33% counts=Counter({1: 154, 2: 110})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
