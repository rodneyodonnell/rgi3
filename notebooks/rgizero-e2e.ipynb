{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.count21 import Count21Game\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.random_player import RandomPlayer\n",
    "\n",
    "\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "import notebook_utils\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "device = notebook_utils.detect_device(require_accelerator=True)\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "\n",
    "# Wrap our game with history tracking\n",
    "# base_game, max_game_length = Count21Game(), 21\n",
    "# base_game, max_game_length = Connect4Game(), 7*6\n",
    "base_game, max_game_length = Connect4Game(connect_length=5), 7*6  # Make it harder to connect! This helps test variable policy and longer games.\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {base_game.__class__.__name__}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Confirm we can self-play a game with a Random Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer, play_game, NetworkEvaluatorResult, NetworkEvaluator\n",
    "from typing import override, Any\n",
    "\n",
    "class RandomEvaluator(NetworkEvaluator):\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    @override\n",
    "    def evaluate(self, game, state, legal_actions: list[Any]):\n",
    "        policy = self.rng.random(len(legal_actions))\n",
    "        values = self.rng.random(game.num_players(state))\n",
    "        return NetworkEvaluatorResult(policy, values)\n",
    "\n",
    "evaluator = RandomEvaluator()\n",
    "player = AlphazeroPlayer(game, evaluator)\n",
    "game_result = play_game(game, [player, player])\n",
    "\n",
    "game_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "\n",
    "all_actions = game.all_actions()\n",
    "num_players = game.num_players(state_0)\n",
    "\n",
    "vocab = Vocab(itos=(TOKENS.START_OF_GAME,) + all_actions)\n",
    "print(f\"Vocab: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "td_builder = TrajectoryDatasetBuilder(vocab)\n",
    "add_trajectory(game_result, vocab, td_builder)\n",
    "\n",
    "td_builder.save(DATA_DIR, 'train_1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TrajectoryDataset(DATA_DIR, 'train_1000', 5)\n",
    "td[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = build_trajectory_loader(\n",
    "    DATA_DIR, 'train_1000', block_size=5, batch_size=1,\n",
    "    device_is_cuda=False, workers=4)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model (random weights) and play a single game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "tiny_config: TransformerConfig = TransformerConfig(n_max_context=100, n_layer=2, n_head=2, n_embd=8)\n",
    "tiny_model = ActionHistoryTransformer(config=tiny_config, action_vocab_size=vocab.vocab_size, num_players=num_players)\n",
    "tiny_model.to(device)\n",
    "tiny_evaluator = ActionHistoryTransformerEvaluator(tiny_model, device=device, block_size=5, vocab=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions = game.legal_actions(state_0)\n",
    "tiny_evaluator.evaluate(game, state_0, legal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_player = AlphazeroPlayer(game, tiny_evaluator)\n",
    "tiny_game_result = play_game(game, [tiny_player, tiny_player])\n",
    "\n",
    "tiny_game_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_row = torch.rand(1, 2)\n",
    "b_row = torch.rand(1, 2)\n",
    "print(f'a_row: {a_row}')\n",
    "print(f'b_row: {b_row}')\n",
    "print(f'loss: {F.cross_entropy(a_row, b_row)}')\n",
    "\n",
    "tile_shape = (3, 1)\n",
    "a_tiled = torch.tile(a_row, tile_shape)\n",
    "b_tiled = torch.tile(b_row, tile_shape)\n",
    "print(f'a_tiled: {a_tiled}')\n",
    "print(f'b_tiled: {b_tiled}')\n",
    "print(f'loss: {F.cross_entropy(a_tiled, b_tiled)}')\n",
    "\n",
    "print(f'loss: {F.cross_entropy(a_tiled, b_tiled, reduction=\"sum\")}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
