{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"small\"  # \"tiny\" or \"small\" or\"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 2000\n",
    "MAX_TRAINING_ITERS = 10_000\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 5\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MAX_TRAINING_ITERS = 1_000_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer]):\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        t0 = time.time()\n",
    "        player = player_factory()\n",
    "        game_result = await play_game_async(game, [player, player])\n",
    "        t1 = time.time()\n",
    "        game_result['time'] = t1 - t0\n",
    "        return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=1024, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = 500,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 2048,\n",
    "\n",
    "    learning_rate = 1e-3,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = 5000,  # make equal to max_iters usually\n",
    "    min_lr = 1e-4,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 100,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n",
    "        device=device, workers=num_workers, shuffle=True)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 27850\n",
      "  Avg trajectory length: 13.93\n",
      "  Trajectory length - min: 7, max: 35, mean: 13.93\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=74.10% win[2]=25.90%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=254 win[1]=64.17% counts=Counter({1: 163, 2: 91})\n",
      "  a=2: n=271 win[1]=73.80% counts=Counter({1: 200, 2: 71})\n",
      "  a=3: n=239 win[1]=79.50% counts=Counter({1: 190, 2: 49})\n",
      "  a=4: n=296 win[1]=85.14% counts=Counter({1: 252, 2: 44})\n",
      "  a=5: n=332 win[1]=79.82% counts=Counter({1: 265, 2: 67})\n",
      "  a=6: n=304 win[1]=75.00% counts=Counter({1: 228, 2: 76})\n",
      "  a=7: n=304 win[1]=60.53% counts=Counter({1: 184, 2: 120})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-1.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-2\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33385\n",
      "  Avg trajectory length: 16.69\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.69\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=34.45% win[2]=65.35%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=352 win[1]=27.27% counts=Counter({2: 255, 1: 96, None: 1})\n",
      "  a=2: n=403 win[1]=36.48% counts=Counter({2: 253, 1: 147, None: 3})\n",
      "  a=3: n=213 win[1]=32.39% counts=Counter({2: 144, 1: 69})\n",
      "  a=4: n=117 win[1]=49.57% counts=Counter({2: 59, 1: 58})\n",
      "  a=5: n=327 win[1]=40.06% counts=Counter({2: 196, 1: 131})\n",
      "  a=6: n=335 win[1]=33.73% counts=Counter({2: 222, 1: 113})\n",
      "  a=7: n=253 win[1]=29.64% counts=Counter({2: 178, 1: 75})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-2.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-3\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 34638\n",
      "  Avg trajectory length: 17.32\n",
      "  Trajectory length - min: 7, max: 42, mean: 17.32\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=70.40% win[2]=28.85%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=192 win[1]=60.94% counts=Counter({1: 117, 2: 74, None: 1})\n",
      "  a=2: n=168 win[1]=66.07% counts=Counter({1: 111, 2: 55, None: 2})\n",
      "  a=3: n=193 win[1]=77.20% counts=Counter({1: 149, 2: 43, None: 1})\n",
      "  a=4: n= 55 win[1]=81.82% counts=Counter({1: 45, 2: 10})\n",
      "  a=5: n=279 win[1]=77.06% counts=Counter({1: 215, 2: 58, None: 6})\n",
      "  a=6: n=549 win[1]=71.58% counts=Counter({1: 393, 2: 154, None: 2})\n",
      "  a=7: n=564 win[1]=67.02% counts=Counter({1: 378, 2: 183, None: 3})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-3.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-4\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 30542\n",
      "  Avg trajectory length: 15.27\n",
      "  Trajectory length - min: 7, max: 42, mean: 15.27\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=81.60% win[2]=18.20%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=127 win[1]=77.95% counts=Counter({1: 99, 2: 28})\n",
      "  a=2: n=171 win[1]=75.44% counts=Counter({1: 129, 2: 42})\n",
      "  a=3: n= 96 win[1]=77.08% counts=Counter({1: 74, 2: 21, None: 1})\n",
      "  a=4: n=164 win[1]=92.07% counts=Counter({1: 151, 2: 13})\n",
      "  a=5: n=289 win[1]=89.27% counts=Counter({1: 258, 2: 31})\n",
      "  a=6: n=309 win[1]=78.32% counts=Counter({1: 242, 2: 66, None: 1})\n",
      "  a=7: n=844 win[1]=80.45% counts=Counter({1: 679, 2: 163, None: 2})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-4.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-5\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 32700\n",
      "  Avg trajectory length: 16.35\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.35\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=79.20% win[2]=20.55%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=211 win[1]=67.77% counts=Counter({1: 143, 2: 67, None: 1})\n",
      "  a=2: n=281 win[1]=77.58% counts=Counter({1: 218, 2: 62, None: 1})\n",
      "  a=3: n=136 win[1]=79.41% counts=Counter({1: 108, 2: 28})\n",
      "  a=5: n=379 win[1]=84.96% counts=Counter({1: 322, 2: 57})\n",
      "  a=6: n=385 win[1]=80.78% counts=Counter({1: 311, 2: 72, None: 2})\n",
      "  a=7: n=608 win[1]=79.28% counts=Counter({1: 482, 2: 125, None: 1})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-5.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 6 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-6\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33445\n",
      "  Avg trajectory length: 16.72\n",
      "  Trajectory length - min: 7, max: 40, mean: 16.72\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=74.45% win[2]=25.55%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=248 win[1]=68.15% counts=Counter({1: 169, 2: 79})\n",
      "  a=2: n=277 win[1]=74.37% counts=Counter({1: 206, 2: 71})\n",
      "  a=3: n=194 win[1]=77.32% counts=Counter({1: 150, 2: 44})\n",
      "  a=4: n=137 win[1]=83.94% counts=Counter({1: 115, 2: 22})\n",
      "  a=5: n=398 win[1]=79.90% counts=Counter({1: 318, 2: 80})\n",
      "  a=6: n=320 win[1]=71.88% counts=Counter({1: 230, 2: 90})\n",
      "  a=7: n=426 win[1]=70.66% counts=Counter({1: 301, 2: 125})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-6.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 7 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-7\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 32954\n",
      "  Avg trajectory length: 16.48\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.48\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=71.80% win[2]=28.10%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=273 win[1]=65.57% counts=Counter({1: 179, 2: 94})\n",
      "  a=2: n=262 win[1]=70.23% counts=Counter({1: 184, 2: 78})\n",
      "  a=3: n=151 win[1]=74.83% counts=Counter({1: 113, 2: 38})\n",
      "  a=4: n=194 win[1]=85.57% counts=Counter({1: 166, 2: 28})\n",
      "  a=5: n=424 win[1]=72.64% counts=Counter({1: 308, 2: 116})\n",
      "  a=6: n=315 win[1]=73.33% counts=Counter({1: 231, 2: 83, None: 1})\n",
      "  a=7: n=381 win[1]=66.93% counts=Counter({1: 255, 2: 125, None: 1})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-7.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 8 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-8\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 34211\n",
      "  Avg trajectory length: 17.11\n",
      "  Trajectory length - min: 7, max: 42, mean: 17.11\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=69.65% win[2]=30.15%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=322 win[1]=62.11% counts=Counter({1: 200, 2: 120, None: 2})\n",
      "  a=2: n=306 win[1]=71.90% counts=Counter({1: 220, 2: 85, None: 1})\n",
      "  a=3: n=219 win[1]=68.49% counts=Counter({1: 150, 2: 69})\n",
      "  a=4: n=160 win[1]=85.62% counts=Counter({1: 137, 2: 23})\n",
      "  a=5: n=451 win[1]=71.18% counts=Counter({1: 321, 2: 129, None: 1})\n",
      "  a=6: n=327 win[1]=69.42% counts=Counter({1: 227, 2: 100})\n",
      "  a=7: n=215 win[1]=64.19% counts=Counter({1: 138, 2: 77})\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-8.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 9 for config_alias=trajectory_sims-200_games-2000_size-small_train-10000_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-2000_size-small_train-10000_x1/gen-9\n",
      "Dataset Stats:\n",
      "  Trajectories: 2000\n",
      "  Total actions: 33852\n",
      "  Avg trajectory length: 16.93\n",
      "  Trajectory length - min: 7, max: 42, mean: 16.93\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=53.30% win[2]=46.60%, n=2000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=341 win[1]=48.09% counts=Counter({2: 176, 1: 164, None: 1})\n",
      "  a=2: n=334 win[1]=50.90% counts=Counter({1: 170, 2: 163, None: 1})\n",
      "  a=3: n=268 win[1]=55.60% counts=Counter({1: 149, 2: 119})\n",
      "  a=4: n= 92 win[1]=92.39% counts=Counter({1: 85, 2: 7})\n",
      "  a=5: n=447 win[1]=54.81% counts=Counter({1: 245, 2: 202})\n",
      "  a=6: n=238 win[1]=55.04% counts=Counter({1: 131, 2: 107})\n",
      "  a=7: n=280 win[1]=43.57% counts=Counter({2: 158, 1: 122})\n",
      "Training model on gen-9\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "Training epoch 0 of 500\n",
      "step 0: train loss 0.4033, val loss 0.4032\n",
      "iter 0: loss 0.8847, time 1288.24ms\n",
      "Training epoch 1 of 500\n",
      "Training epoch 2 of 500\n",
      "Training epoch 3 of 500\n",
      "Training epoch 4 of 500\n",
      "Training epoch 5 of 500\n",
      "Training epoch 6 of 500\n",
      "Training epoch 7 of 500\n",
      "Training epoch 8 of 500\n",
      "Training epoch 9 of 500\n",
      "Training epoch 10 of 500\n",
      "Training epoch 11 of 500\n",
      "iter 100: loss 0.8669, time 97.39ms\n",
      "Training epoch 12 of 500\n",
      "Training epoch 13 of 500\n",
      "Training epoch 14 of 500\n",
      "Training epoch 15 of 500\n",
      "Training epoch 16 of 500\n",
      "Training epoch 17 of 500\n",
      "Training epoch 18 of 500\n",
      "Training epoch 19 of 500\n",
      "Training epoch 20 of 500\n",
      "Training epoch 21 of 500\n",
      "Training epoch 22 of 500\n",
      "iter 200: loss 0.8612, time 94.58ms\n",
      "Training epoch 23 of 500\n",
      "Training epoch 24 of 500\n",
      "Training epoch 25 of 500\n",
      "Training epoch 26 of 500\n",
      "Training epoch 27 of 500\n",
      "Training epoch 28 of 500\n",
      "Training epoch 29 of 500\n",
      "Training epoch 30 of 500\n",
      "Training epoch 31 of 500\n",
      "Training epoch 32 of 500\n",
      "Training epoch 33 of 500\n",
      "iter 300: loss 0.8606, time 94.06ms\n",
      "Training epoch 34 of 500\n",
      "Training epoch 35 of 500\n",
      "Training epoch 36 of 500\n",
      "Training epoch 37 of 500\n",
      "Training epoch 38 of 500\n",
      "Training epoch 39 of 500\n",
      "Training epoch 40 of 500\n",
      "Training epoch 41 of 500\n",
      "Training epoch 42 of 500\n",
      "Training epoch 43 of 500\n",
      "Training epoch 44 of 500\n",
      "iter 400: loss 0.8535, time 93.65ms\n",
      "Training epoch 45 of 500\n",
      "Training epoch 46 of 500\n",
      "Training epoch 47 of 500\n",
      "Training epoch 48 of 500\n",
      "Training epoch 49 of 500\n",
      "Training epoch 50 of 500\n",
      "Training epoch 51 of 500\n",
      "Training epoch 52 of 500\n",
      "Training epoch 53 of 500\n",
      "Training epoch 54 of 500\n",
      "Training epoch 55 of 500\n",
      "iter 500: loss 0.8632, time 92.63ms\n",
      "Training epoch 56 of 500\n",
      "Training epoch 57 of 500\n",
      "Training epoch 58 of 500\n",
      "Training epoch 59 of 500\n",
      "Training epoch 60 of 500\n",
      "Training epoch 61 of 500\n",
      "Training epoch 62 of 500\n",
      "Training epoch 63 of 500\n",
      "Training epoch 64 of 500\n",
      "Training epoch 65 of 500\n",
      "Training epoch 66 of 500\n",
      "iter 600: loss 0.8425, time 92.93ms\n",
      "Training epoch 67 of 500\n",
      "Training epoch 68 of 500\n",
      "Training epoch 69 of 500\n",
      "Training epoch 70 of 500\n",
      "Training epoch 71 of 500\n",
      "Training epoch 72 of 500\n",
      "Training epoch 73 of 500\n",
      "Training epoch 74 of 500\n",
      "Training epoch 75 of 500\n",
      "Training epoch 76 of 500\n",
      "Training epoch 77 of 500\n",
      "iter 700: loss 0.8515, time 93.94ms\n",
      "Training epoch 78 of 500\n",
      "Training epoch 79 of 500\n",
      "Training epoch 80 of 500\n",
      "Training epoch 81 of 500\n",
      "Training epoch 82 of 500\n",
      "Training epoch 83 of 500\n",
      "Training epoch 84 of 500\n",
      "Training epoch 85 of 500\n",
      "Training epoch 86 of 500\n",
      "Training epoch 87 of 500\n",
      "Training epoch 88 of 500\n",
      "iter 800: loss 0.8414, time 95.25ms\n",
      "Training epoch 89 of 500\n",
      "Training epoch 90 of 500\n",
      "Training epoch 91 of 500\n",
      "Training epoch 92 of 500\n",
      "Training epoch 93 of 500\n",
      "Training epoch 94 of 500\n",
      "Training epoch 95 of 500\n",
      "Training epoch 96 of 500\n",
      "Training epoch 97 of 500\n",
      "Training epoch 98 of 500\n",
      "Training epoch 99 of 500\n",
      "Training epoch 100 of 500\n",
      "iter 900: loss 0.8422, time 75.53ms\n",
      "Training epoch 101 of 500\n",
      "Training epoch 102 of 500\n",
      "Training epoch 103 of 500\n",
      "Training epoch 104 of 500\n",
      "Training epoch 105 of 500\n",
      "Training epoch 106 of 500\n",
      "Training epoch 107 of 500\n",
      "Training epoch 108 of 500\n",
      "Training epoch 109 of 500\n",
      "Training epoch 110 of 500\n",
      "Training epoch 111 of 500\n",
      "step 1000: train loss 0.3757, val loss 0.3758\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 1000: loss 0.8374, time 1234.26ms\n",
      "Training epoch 112 of 500\n",
      "Training epoch 113 of 500\n",
      "Training epoch 114 of 500\n",
      "Training epoch 115 of 500\n",
      "Training epoch 116 of 500\n",
      "Training epoch 117 of 500\n",
      "Training epoch 118 of 500\n",
      "Training epoch 119 of 500\n",
      "Training epoch 120 of 500\n",
      "Training epoch 121 of 500\n",
      "Training epoch 122 of 500\n",
      "iter 1100: loss 0.8370, time 93.95ms\n",
      "Training epoch 123 of 500\n",
      "Training epoch 124 of 500\n",
      "Training epoch 125 of 500\n",
      "Training epoch 126 of 500\n",
      "Training epoch 127 of 500\n",
      "Training epoch 128 of 500\n",
      "Training epoch 129 of 500\n",
      "Training epoch 130 of 500\n",
      "Training epoch 131 of 500\n",
      "Training epoch 132 of 500\n",
      "Training epoch 133 of 500\n",
      "iter 1200: loss 0.8259, time 94.65ms\n",
      "Training epoch 134 of 500\n",
      "Training epoch 135 of 500\n",
      "Training epoch 136 of 500\n",
      "Training epoch 137 of 500\n",
      "Training epoch 138 of 500\n",
      "Training epoch 139 of 500\n",
      "Training epoch 140 of 500\n",
      "Training epoch 141 of 500\n",
      "Training epoch 142 of 500\n",
      "Training epoch 143 of 500\n",
      "Training epoch 144 of 500\n",
      "iter 1300: loss 0.8230, time 127.34ms\n",
      "Training epoch 145 of 500\n",
      "Training epoch 146 of 500\n",
      "Training epoch 147 of 500\n",
      "Training epoch 148 of 500\n",
      "Training epoch 149 of 500\n",
      "Training epoch 150 of 500\n",
      "Training epoch 151 of 500\n",
      "Training epoch 152 of 500\n",
      "Training epoch 153 of 500\n",
      "Training epoch 154 of 500\n",
      "Training epoch 155 of 500\n",
      "iter 1400: loss 0.8100, time 99.11ms\n",
      "Training epoch 156 of 500\n",
      "Training epoch 157 of 500\n",
      "Training epoch 158 of 500\n",
      "Training epoch 159 of 500\n",
      "Training epoch 160 of 500\n",
      "Training epoch 161 of 500\n",
      "Training epoch 162 of 500\n",
      "Training epoch 163 of 500\n",
      "Training epoch 164 of 500\n",
      "Training epoch 165 of 500\n",
      "Training epoch 166 of 500\n",
      "iter 1500: loss 0.8256, time 93.58ms\n",
      "Training epoch 167 of 500\n",
      "Training epoch 168 of 500\n",
      "Training epoch 169 of 500\n",
      "Training epoch 170 of 500\n",
      "Training epoch 171 of 500\n",
      "Training epoch 172 of 500\n",
      "Training epoch 173 of 500\n",
      "Training epoch 174 of 500\n",
      "Training epoch 175 of 500\n",
      "Training epoch 176 of 500\n",
      "Training epoch 177 of 500\n",
      "iter 1600: loss 0.8212, time 92.38ms\n",
      "Training epoch 178 of 500\n",
      "Training epoch 179 of 500\n",
      "Training epoch 180 of 500\n",
      "Training epoch 181 of 500\n",
      "Training epoch 182 of 500\n",
      "Training epoch 183 of 500\n",
      "Training epoch 184 of 500\n",
      "Training epoch 185 of 500\n",
      "Training epoch 186 of 500\n",
      "Training epoch 187 of 500\n",
      "Training epoch 188 of 500\n",
      "iter 1700: loss 0.8125, time 94.93ms\n",
      "Training epoch 189 of 500\n",
      "Training epoch 190 of 500\n",
      "Training epoch 191 of 500\n",
      "Training epoch 192 of 500\n",
      "Training epoch 193 of 500\n",
      "Training epoch 194 of 500\n",
      "Training epoch 195 of 500\n",
      "Training epoch 196 of 500\n",
      "Training epoch 197 of 500\n",
      "Training epoch 198 of 500\n",
      "Training epoch 199 of 500\n",
      "Training epoch 200 of 500\n",
      "iter 1800: loss 0.8027, time 76.00ms\n",
      "Training epoch 201 of 500\n",
      "Training epoch 202 of 500\n",
      "Training epoch 203 of 500\n",
      "Training epoch 204 of 500\n",
      "Training epoch 205 of 500\n",
      "Training epoch 206 of 500\n",
      "Training epoch 207 of 500\n",
      "Training epoch 208 of 500\n",
      "Training epoch 209 of 500\n",
      "Training epoch 210 of 500\n",
      "Training epoch 211 of 500\n",
      "iter 1900: loss 0.8019, time 93.17ms\n",
      "Training epoch 212 of 500\n",
      "Training epoch 213 of 500\n",
      "Training epoch 214 of 500\n",
      "Training epoch 215 of 500\n",
      "Training epoch 216 of 500\n",
      "Training epoch 217 of 500\n",
      "Training epoch 218 of 500\n",
      "Training epoch 219 of 500\n",
      "Training epoch 220 of 500\n",
      "Training epoch 221 of 500\n",
      "Training epoch 222 of 500\n",
      "step 2000: train loss 0.3623, val loss 0.3623\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 2000: loss 0.8016, time 1152.20ms\n",
      "Training epoch 223 of 500\n",
      "Training epoch 224 of 500\n",
      "Training epoch 225 of 500\n",
      "Training epoch 226 of 500\n",
      "Training epoch 227 of 500\n",
      "Training epoch 228 of 500\n",
      "Training epoch 229 of 500\n",
      "Training epoch 230 of 500\n",
      "Training epoch 231 of 500\n",
      "Training epoch 232 of 500\n",
      "Training epoch 233 of 500\n",
      "iter 2100: loss 0.8092, time 128.82ms\n",
      "Training epoch 234 of 500\n",
      "Training epoch 235 of 500\n",
      "Training epoch 236 of 500\n",
      "Training epoch 237 of 500\n",
      "Training epoch 238 of 500\n",
      "Training epoch 239 of 500\n",
      "Training epoch 240 of 500\n",
      "Training epoch 241 of 500\n",
      "Training epoch 242 of 500\n",
      "Training epoch 243 of 500\n",
      "Training epoch 244 of 500\n",
      "iter 2200: loss 0.8043, time 97.37ms\n",
      "Training epoch 245 of 500\n",
      "Training epoch 246 of 500\n",
      "Training epoch 247 of 500\n",
      "Training epoch 248 of 500\n",
      "Training epoch 249 of 500\n",
      "Training epoch 250 of 500\n",
      "Training epoch 251 of 500\n",
      "Training epoch 252 of 500\n",
      "Training epoch 253 of 500\n",
      "Training epoch 254 of 500\n",
      "Training epoch 255 of 500\n",
      "iter 2300: loss 0.8035, time 99.49ms\n",
      "Training epoch 256 of 500\n",
      "Training epoch 257 of 500\n",
      "Training epoch 258 of 500\n",
      "Training epoch 259 of 500\n",
      "Training epoch 260 of 500\n",
      "Training epoch 261 of 500\n",
      "Training epoch 262 of 500\n",
      "Training epoch 263 of 500\n",
      "Training epoch 264 of 500\n",
      "Training epoch 265 of 500\n",
      "Training epoch 266 of 500\n",
      "iter 2400: loss 0.8089, time 93.11ms\n",
      "Training epoch 267 of 500\n",
      "Training epoch 268 of 500\n",
      "Training epoch 269 of 500\n",
      "Training epoch 270 of 500\n",
      "Training epoch 271 of 500\n",
      "Training epoch 272 of 500\n",
      "Training epoch 273 of 500\n",
      "Training epoch 274 of 500\n",
      "Training epoch 275 of 500\n",
      "Training epoch 276 of 500\n",
      "Training epoch 277 of 500\n",
      "iter 2500: loss 0.7951, time 94.50ms\n",
      "Training epoch 278 of 500\n",
      "Training epoch 279 of 500\n",
      "Training epoch 280 of 500\n",
      "Training epoch 281 of 500\n",
      "Training epoch 282 of 500\n",
      "Training epoch 283 of 500\n",
      "Training epoch 284 of 500\n",
      "Training epoch 285 of 500\n",
      "Training epoch 286 of 500\n",
      "Training epoch 287 of 500\n",
      "Training epoch 288 of 500\n",
      "iter 2600: loss 0.7879, time 93.69ms\n",
      "Training epoch 289 of 500\n",
      "Training epoch 290 of 500\n",
      "Training epoch 291 of 500\n",
      "Training epoch 292 of 500\n",
      "Training epoch 293 of 500\n",
      "Training epoch 294 of 500\n",
      "Training epoch 295 of 500\n",
      "Training epoch 296 of 500\n",
      "Training epoch 297 of 500\n",
      "Training epoch 298 of 500\n",
      "Training epoch 299 of 500\n",
      "Training epoch 300 of 500\n",
      "iter 2700: loss 0.7974, time 77.08ms\n",
      "Training epoch 301 of 500\n",
      "Training epoch 302 of 500\n",
      "Training epoch 303 of 500\n",
      "Training epoch 304 of 500\n",
      "Training epoch 305 of 500\n",
      "Training epoch 306 of 500\n",
      "Training epoch 307 of 500\n",
      "Training epoch 308 of 500\n",
      "Training epoch 309 of 500\n",
      "Training epoch 310 of 500\n",
      "Training epoch 311 of 500\n",
      "iter 2800: loss 0.8071, time 92.10ms\n",
      "Training epoch 312 of 500\n",
      "Training epoch 313 of 500\n",
      "Training epoch 314 of 500\n",
      "Training epoch 315 of 500\n",
      "Training epoch 316 of 500\n",
      "Training epoch 317 of 500\n",
      "Training epoch 318 of 500\n",
      "Training epoch 319 of 500\n",
      "Training epoch 320 of 500\n",
      "Training epoch 321 of 500\n",
      "Training epoch 322 of 500\n",
      "iter 2900: loss 0.7949, time 93.23ms\n",
      "Training epoch 323 of 500\n",
      "Training epoch 324 of 500\n",
      "Training epoch 325 of 500\n",
      "Training epoch 326 of 500\n",
      "Training epoch 327 of 500\n",
      "Training epoch 328 of 500\n",
      "Training epoch 329 of 500\n",
      "Training epoch 330 of 500\n",
      "Training epoch 331 of 500\n",
      "Training epoch 332 of 500\n",
      "Training epoch 333 of 500\n",
      "step 3000: train loss 0.3551, val loss 0.3549\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 3000: loss 0.7899, time 1190.40ms\n",
      "Training epoch 334 of 500\n",
      "Training epoch 335 of 500\n",
      "Training epoch 336 of 500\n",
      "Training epoch 337 of 500\n",
      "Training epoch 338 of 500\n",
      "Training epoch 339 of 500\n",
      "Training epoch 340 of 500\n",
      "Training epoch 341 of 500\n",
      "Training epoch 342 of 500\n",
      "Training epoch 343 of 500\n",
      "Training epoch 344 of 500\n",
      "iter 3100: loss 0.7834, time 94.34ms\n",
      "Training epoch 345 of 500\n",
      "Training epoch 346 of 500\n",
      "Training epoch 347 of 500\n",
      "Training epoch 348 of 500\n",
      "Training epoch 349 of 500\n",
      "Training epoch 350 of 500\n",
      "Training epoch 351 of 500\n",
      "Training epoch 352 of 500\n",
      "Training epoch 353 of 500\n",
      "Training epoch 354 of 500\n",
      "Training epoch 355 of 500\n",
      "iter 3200: loss 0.7916, time 91.98ms\n",
      "Training epoch 356 of 500\n",
      "Training epoch 357 of 500\n",
      "Training epoch 358 of 500\n",
      "Training epoch 359 of 500\n",
      "Training epoch 360 of 500\n",
      "Training epoch 361 of 500\n",
      "Training epoch 362 of 500\n",
      "Training epoch 363 of 500\n",
      "Training epoch 364 of 500\n",
      "Training epoch 365 of 500\n",
      "Training epoch 366 of 500\n",
      "iter 3300: loss 0.7939, time 94.06ms\n",
      "Training epoch 367 of 500\n",
      "Training epoch 368 of 500\n",
      "Training epoch 369 of 500\n",
      "Training epoch 370 of 500\n",
      "Training epoch 371 of 500\n",
      "Training epoch 372 of 500\n",
      "Training epoch 373 of 500\n",
      "Training epoch 374 of 500\n",
      "Training epoch 375 of 500\n",
      "Training epoch 376 of 500\n",
      "Training epoch 377 of 500\n",
      "iter 3400: loss 0.7871, time 95.77ms\n",
      "Training epoch 378 of 500\n",
      "Training epoch 379 of 500\n",
      "Training epoch 380 of 500\n",
      "Training epoch 381 of 500\n",
      "Training epoch 382 of 500\n",
      "Training epoch 383 of 500\n",
      "Training epoch 384 of 500\n",
      "Training epoch 385 of 500\n",
      "Training epoch 386 of 500\n",
      "Training epoch 387 of 500\n",
      "Training epoch 388 of 500\n",
      "iter 3500: loss 0.7831, time 105.34ms\n",
      "Training epoch 389 of 500\n",
      "Training epoch 390 of 500\n",
      "Training epoch 391 of 500\n",
      "Training epoch 392 of 500\n",
      "Training epoch 393 of 500\n",
      "Training epoch 394 of 500\n",
      "Training epoch 395 of 500\n",
      "Training epoch 396 of 500\n",
      "Training epoch 397 of 500\n",
      "Training epoch 398 of 500\n",
      "Training epoch 399 of 500\n",
      "Training epoch 400 of 500\n",
      "iter 3600: loss 0.7707, time 74.51ms\n",
      "Training epoch 401 of 500\n",
      "Training epoch 402 of 500\n",
      "Training epoch 403 of 500\n",
      "Training epoch 404 of 500\n",
      "Training epoch 405 of 500\n",
      "Training epoch 406 of 500\n",
      "Training epoch 407 of 500\n",
      "Training epoch 408 of 500\n",
      "Training epoch 409 of 500\n",
      "Training epoch 410 of 500\n",
      "Training epoch 411 of 500\n",
      "iter 3700: loss 0.7933, time 93.85ms\n",
      "Training epoch 412 of 500\n",
      "Training epoch 413 of 500\n",
      "Training epoch 414 of 500\n",
      "Training epoch 415 of 500\n",
      "Training epoch 416 of 500\n",
      "Training epoch 417 of 500\n",
      "Training epoch 418 of 500\n",
      "Training epoch 419 of 500\n",
      "Training epoch 420 of 500\n",
      "Training epoch 421 of 500\n",
      "Training epoch 422 of 500\n",
      "iter 3800: loss 0.7735, time 92.26ms\n",
      "Training epoch 423 of 500\n",
      "Training epoch 424 of 500\n",
      "Training epoch 425 of 500\n",
      "Training epoch 426 of 500\n",
      "Training epoch 427 of 500\n",
      "Training epoch 428 of 500\n",
      "Training epoch 429 of 500\n",
      "Training epoch 430 of 500\n",
      "Training epoch 431 of 500\n",
      "Training epoch 432 of 500\n",
      "Training epoch 433 of 500\n",
      "iter 3900: loss 0.7705, time 93.65ms\n",
      "Training epoch 434 of 500\n",
      "Training epoch 435 of 500\n",
      "Training epoch 436 of 500\n",
      "Training epoch 437 of 500\n",
      "Training epoch 438 of 500\n",
      "Training epoch 439 of 500\n",
      "Training epoch 440 of 500\n",
      "Training epoch 441 of 500\n",
      "Training epoch 442 of 500\n",
      "Training epoch 443 of 500\n",
      "Training epoch 444 of 500\n",
      "step 4000: train loss 0.3488, val loss 0.3489\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 4000: loss 0.7800, time 1143.38ms\n",
      "Training epoch 445 of 500\n",
      "Training epoch 446 of 500\n",
      "Training epoch 447 of 500\n",
      "Training epoch 448 of 500\n",
      "Training epoch 449 of 500\n",
      "Training epoch 450 of 500\n",
      "Training epoch 451 of 500\n",
      "Training epoch 452 of 500\n",
      "Training epoch 453 of 500\n",
      "Training epoch 454 of 500\n",
      "Training epoch 455 of 500\n",
      "iter 4100: loss 0.7727, time 93.92ms\n",
      "Training epoch 456 of 500\n",
      "Training epoch 457 of 500\n",
      "Training epoch 458 of 500\n",
      "Training epoch 459 of 500\n",
      "Training epoch 460 of 500\n",
      "Training epoch 461 of 500\n",
      "Training epoch 462 of 500\n",
      "Training epoch 463 of 500\n",
      "Training epoch 464 of 500\n",
      "Training epoch 465 of 500\n",
      "Training epoch 466 of 500\n",
      "iter 4200: loss 0.7819, time 94.27ms\n",
      "Training epoch 467 of 500\n",
      "Training epoch 468 of 500\n",
      "Training epoch 469 of 500\n",
      "Training epoch 470 of 500\n",
      "Training epoch 471 of 500\n",
      "Training epoch 472 of 500\n",
      "Training epoch 473 of 500\n",
      "Training epoch 474 of 500\n",
      "Training epoch 475 of 500\n",
      "Training epoch 476 of 500\n",
      "Training epoch 477 of 500\n",
      "iter 4300: loss 0.7684, time 92.67ms\n",
      "Training epoch 478 of 500\n",
      "Training epoch 479 of 500\n",
      "Training epoch 480 of 500\n",
      "Training epoch 481 of 500\n",
      "Training epoch 482 of 500\n",
      "Training epoch 483 of 500\n",
      "Training epoch 484 of 500\n",
      "Training epoch 485 of 500\n",
      "Training epoch 486 of 500\n",
      "Training epoch 487 of 500\n",
      "Training epoch 488 of 500\n",
      "iter 4400: loss 0.7730, time 91.39ms\n",
      "Training epoch 489 of 500\n",
      "Training epoch 490 of 500\n",
      "Training epoch 491 of 500\n",
      "Training epoch 492 of 500\n",
      "Training epoch 493 of 500\n",
      "Training epoch 494 of 500\n",
      "Training epoch 495 of 500\n",
      "Training epoch 496 of 500\n",
      "Training epoch 497 of 500\n",
      "Training epoch 498 of 500\n",
      "Training epoch 499 of 500\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "# 10 hours!\n",
    "for generation_id in range(1, 10):\n",
    "    current_model = model_dict[generation_id-1]\n",
    "    results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "    results_dict[generation_id] = results_i\n",
    "    trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "    model_dict[generation_id] = model_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dd) = 2644\n"
     ]
    }
   ],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    # for g in ['*', gen]:\n",
    "    for g in ['*']:\n",
    "        dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=()\n",
      "gen=*: tensor([12197.,  5803.]), win_pct=67.76%, sum=18000.0\n",
      "legal_policy=[0.09077198 0.10554948 0.15931028 0.25835538 0.27734098 0.07338901 0.03528288]\n",
      "player_values=[-0.9040589  0.9040588]\n",
      "player_probs=[0.04797056 0.9520294 ]\n",
      "\n",
      "prefix=(4,)\n",
      "gen=*: tensor([1009.,  206.]), win_pct=83.05%, sum=1215.0\n",
      "legal_policy=[0.09611177 0.09573263 0.09513989 0.42159563 0.07386575 0.13182265 0.08573171]\n",
      "player_values=[-0.9998971  0.999897 ]\n",
      "player_probs=[5.143881e-05 9.999485e-01]\n",
      "\n",
      "prefix=(4, 1)\n",
      "gen=*: tensor([139.,  21.]), win_pct=86.88%, sum=160.0\n",
      "legal_policy=[0.2322435  0.12637495 0.27581427 0.05514194 0.1012088  0.12752587 0.08169063]\n",
      "player_values=[-0.9999521  0.9999521]\n",
      "player_probs=[2.3961067e-05 9.9997604e-01]\n",
      "\n",
      "prefix=(4, 2)\n",
      "gen=*: tensor([135.,  23.]), win_pct=85.44%, sum=158.0\n",
      "legal_policy=[0.10657366 0.2969809  0.14868882 0.0774617  0.15467586 0.15689282 0.05872629]\n",
      "player_values=[-0.99990606  0.99990606]\n",
      "player_probs=[4.6968460e-05 9.9995303e-01]\n",
      "\n",
      "prefix=(4, 3)\n",
      "gen=*: tensor([74., 39.]), win_pct=65.49%, sum=113.0\n",
      "legal_policy=[0.15590389 0.11586201 0.29838294 0.08332977 0.14775994 0.12914483 0.06961666]\n",
      "player_values=[-0.9999462  0.9999461]\n",
      "player_probs=[2.6911497e-05 9.9997306e-01]\n",
      "\n",
      "prefix=(4, 4)\n",
      "gen=*: tensor([285.,  52.]), win_pct=84.57%, sum=337.0\n",
      "legal_policy=[0.10499605 0.10178175 0.09391157 0.37119222 0.08474551 0.15614668 0.08722629]\n",
      "player_values=[-0.99993145  0.99993134]\n",
      "player_probs=[3.4272671e-05 9.9996567e-01]\n",
      "\n",
      "prefix=(4, 5)\n",
      "gen=*: tensor([114.,  28.]), win_pct=80.28%, sum=142.0\n",
      "legal_policy=[0.06906158 0.08798946 0.09147097 0.03440101 0.534704   0.09264545 0.08972755]\n",
      "player_values=[-0.9996317   0.99963176]\n",
      "player_probs=[1.8414855e-04 9.9981588e-01]\n",
      "\n",
      "prefix=(4, 6)\n",
      "gen=*: tensor([95., 16.]), win_pct=85.59%, sum=111.0\n",
      "legal_policy=[0.16814223 0.14150798 0.13913035 0.06591284 0.12271354 0.29854375 0.06404922]\n",
      "player_values=[-0.9999211  0.9999211]\n",
      "player_probs=[3.9458275e-05 9.9996054e-01]\n",
      "\n",
      "prefix=(4, 7)\n",
      "gen=*: tensor([167.,  27.]), win_pct=86.08%, sum=194.0\n",
      "legal_policy=[0.17173842 0.20531759 0.1985837  0.06651854 0.08936282 0.12276546 0.1457135 ]\n",
      "player_values=[-0.99992    0.9999201]\n",
      "player_probs=[3.9994717e-05 9.9996006e-01]\n"
     ]
    }
   ],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "prefix_list = [(), (4,), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7)]\n",
    "for prefix in prefix_list:\n",
    "    print(f\"\\nprefix={prefix}\")\n",
    "    for gen, counts in dd[prefix].items():\n",
    "        print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "    eval_result = eval_prefix(current_model, game, prefix)\n",
    "    print(f'legal_policy={eval_result.legal_policy}')\n",
    "    print(f'player_values={eval_result.player_values}')\n",
    "    print(f'player_probs={(eval_result.player_values+1)/2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=()\n",
      "  gen=*: tensor([12197.,  5803.]), win_pct=67.76%, sum=18000.0\n",
      "  Training data values (transformed): mean=[ 0.35522223 -0.35522223], std=[0.93375266 0.93375266]\n",
      "  Found 18000 examples\n",
      "  Player 1 win rate from stored values: 67.66%\n",
      "  MODEL: values=[-0.9040589  0.9040588], legal_policy=[0.09077198 0.10554948 0.15931028 0.25835538 0.27734098 0.07338901 0.03528288]\n",
      "\n",
      "prefix=(4,)\n",
      "  gen=*: tensor([1009.,  206.]), win_pct=83.05%, sum=1215.0\n",
      "  Training data values (transformed): mean=[ 0.66090536 -0.66090536], std=[0.75047135 0.75047135]\n",
      "  Found 1215 examples\n",
      "  Player 1 win rate from stored values: 83.05%\n",
      "  MODEL: values=[-0.9998971  0.999897 ], legal_policy=[0.09611177 0.09573263 0.09513989 0.42159563 0.07386575 0.13182265 0.08573171]\n",
      "\n",
      "prefix=(4, 1)\n",
      "  gen=*: tensor([139.,  21.]), win_pct=86.88%, sum=160.0\n",
      "  Training data values (transformed): mean=[ 0.7375 -0.7375], std=[0.67534685 0.67534685]\n",
      "  Found 160 examples\n",
      "  Player 1 win rate from stored values: 86.88%\n",
      "  MODEL: values=[-0.9999521  0.9999521], legal_policy=[0.2322435  0.12637495 0.27581427 0.05514194 0.1012088  0.12752587 0.08169063]\n",
      "\n",
      "prefix=(4, 2)\n",
      "  gen=*: tensor([135.,  23.]), win_pct=85.44%, sum=158.0\n",
      "  Training data values (transformed): mean=[ 0.70886075 -0.70886075], std=[0.7053486 0.7053486]\n",
      "  Found 158 examples\n",
      "  Player 1 win rate from stored values: 85.44%\n",
      "  MODEL: values=[-0.99990606  0.99990606], legal_policy=[0.10657366 0.2969809  0.14868882 0.0774617  0.15467586 0.15689282 0.05872629]\n",
      "\n",
      "prefix=(4, 3)\n",
      "  gen=*: tensor([74., 39.]), win_pct=65.49%, sum=113.0\n",
      "  Training data values (transformed): mean=[ 0.30973452 -0.30973452], std=[0.95082283 0.95082283]\n",
      "  Found 113 examples\n",
      "  Player 1 win rate from stored values: 65.49%\n",
      "  MODEL: values=[-0.9999462  0.9999461], legal_policy=[0.15590389 0.11586201 0.29838294 0.08332977 0.14775994 0.12914483 0.06961666]\n",
      "\n",
      "prefix=(4, 4)\n",
      "  gen=*: tensor([285.,  52.]), win_pct=84.57%, sum=337.0\n",
      "  Training data values (transformed): mean=[ 0.6913947 -0.6913947], std=[0.7224759 0.7224759]\n",
      "  Found 337 examples\n",
      "  Player 1 win rate from stored values: 84.57%\n",
      "  MODEL: values=[-0.99993145  0.99993134], legal_policy=[0.10499605 0.10178175 0.09391157 0.37119222 0.08474551 0.15614668 0.08722629]\n",
      "\n",
      "prefix=(4, 5)\n",
      "  gen=*: tensor([114.,  28.]), win_pct=80.28%, sum=142.0\n",
      "  Training data values (transformed): mean=[ 0.6056338 -0.6056338], std=[0.7957436 0.7957436]\n",
      "  Found 142 examples\n",
      "  Player 1 win rate from stored values: 80.28%\n",
      "  MODEL: values=[-0.9996317   0.99963176], legal_policy=[0.06906158 0.08798946 0.09147097 0.03440101 0.534704   0.09264545 0.08972755]\n",
      "\n",
      "prefix=(4, 6)\n",
      "  gen=*: tensor([95., 16.]), win_pct=85.59%, sum=111.0\n",
      "  Training data values (transformed): mean=[ 0.7117117 -0.7117117], std=[0.7024719 0.7024719]\n",
      "  Found 111 examples\n",
      "  Player 1 win rate from stored values: 85.59%\n",
      "  MODEL: values=[-0.9999211  0.9999211], legal_policy=[0.16814223 0.14150798 0.13913035 0.06591284 0.12271354 0.29854375 0.06404922]\n",
      "\n",
      "prefix=(4, 7)\n",
      "  gen=*: tensor([167.,  27.]), win_pct=86.08%, sum=194.0\n",
      "  Training data values (transformed): mean=[ 0.72164947 -0.72164947], std=[0.6922582 0.6922582]\n",
      "  Found 194 examples\n",
      "  Player 1 win rate from stored values: 86.08%\n",
      "  MODEL: values=[-0.99992    0.9999201], legal_policy=[0.17173842 0.20531759 0.1985837  0.06651854 0.08936282 0.12276546 0.1457135 ]\n"
     ]
    }
   ],
   "source": [
    "# Check what values are stored in the training data for these prefixes\n",
    "def check_training_values(prefix):\n",
    "    \"\"\"See what values are actually stored in training data for this prefix.\"\"\"\n",
    "    matching_values = []\n",
    "    \n",
    "    for gen, td in enumerate(td_array, 1):\n",
    "        for trajectory in td:\n",
    "            actions = trajectory.action.numpy()\n",
    "            if len(prefix) == 0 or np.array_equal(actions[:len(prefix)], prefix):\n",
    "                value_at_position = trajectory.value[len(prefix)].numpy()\n",
    "                # Transform to [-1, 1] range to match model output\n",
    "                transformed_value = value_at_position * 2 - 1\n",
    "                matching_values.append((gen, transformed_value))\n",
    "    \n",
    "    if matching_values:\n",
    "        values_array = np.array([v for _, v in matching_values])\n",
    "        print(f\"  Training data values (transformed): mean={values_array.mean(axis=0)}, std={values_array.std(axis=0)}\")\n",
    "        print(f\"  Found {len(matching_values)} examples\")\n",
    "        \n",
    "        # Also show empirical win rate from these values\n",
    "        player1_wins = (values_array[:, 0] > values_array[:, 1]).sum()\n",
    "        win_pct = 100 * player1_wins / len(values_array)\n",
    "        print(f\"  Player 1 win rate from stored values: {win_pct:.2f}%\")\n",
    "\n",
    "# Add this to your debug loop\n",
    "prefix_list = [(), (4,), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7)]\n",
    "\n",
    "for prefix in prefix_list:\n",
    "    print(f\"\\nprefix={prefix}\")\n",
    "    \n",
    "    # Empirical win rates from games\n",
    "    for gen, counts in dd[prefix].items():\n",
    "        print(f\"  gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "    \n",
    "    # What's actually stored in training data\n",
    "    check_training_values(prefix)\n",
    "    \n",
    "    # Model prediction\n",
    "    result = eval_prefix(current_model, game, prefix)\n",
    "    print(f\"  MODEL: values={result.player_values}, legal_policy={result.legal_policy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
