{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"tiny\"  # \"tiny\" or \"small\" or \"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "RUN_GENERATIONS = True\n",
    "RUN_TOURNAMENT = False\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 10_000\n",
    "MAX_TRAINING_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 2048\n",
    "MAX_TRAINING_ITERS = 1_000_000 // TRAIN_BATCH_SIZE\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 20\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MODEL_SIZE = \"small\"\n",
    "MAX_TRAINING_ITERS = 100_000_000 // TRAIN_BATCH_SIZE\n",
    "MAX_TRAINING_EPOCHS = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        win2_pct = 100 * counts[2] / total if total > 0 else 0\n",
    "        draw_pct = 100 * counts[None] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}, win[2]={win2_pct:.2f}% draw={draw_pct:.2f}%\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "from rgi.rgizero.models.tuner import create_random_model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "# TODO: Use MODEL_SIZE!\n",
    "# model_config = model_config_dict[\"small\"] # Override to see if we can fit better.\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer], max_concurrent_games: int = 1000):\n",
    "    sem = asyncio.Semaphore(max_concurrent_games)\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        async with sem:\n",
    "            t0 = time.time()\n",
    "            player = player_factory()\n",
    "            game_result = await play_game_async(game, [player, player])\n",
    "            t1 = time.time()\n",
    "            game_result['time'] = t1 - t0\n",
    "            return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS, max_concurrent_games=1024):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=max_concurrent_games, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory, max_concurrent_games=max_concurrent_games)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}, draw={100*winner_stats[None]/sum(winner_stats.values()):.2f}%\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}, win[2]={100*counts[2]/sum(counts.values()):.2f}% draw={100*counts[None]/sum(counts.values()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = MAX_TRAINING_EPOCHS,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "\n",
    "    learning_rate = LEARNING_RATE,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = MAX_TRAINING_ITERS,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    from rgi.rgizero.models.tuner import train_model as tuner_train_model\n",
    "    model, trainer = tuner_train_model(model, training_splits, train_config, device, n_max_context, DATA_DIR, num_workers)\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        # TODO: We're continuing training on a previosu model here ... should we train a new model from scratch?\n",
    "        print(train_config)\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, NUM_GENERATIONS+1):\n",
    "        current_model = model_dict[generation_id-1]\n",
    "        results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "        results_dict[generation_id] = results_i\n",
    "        trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "        model_dict[generation_id] = model_i\n",
    "\n",
    "## refactor, learning_rate = 0.05, warmup_iters=0\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/488: loss 2.7801, time 611.02ms\n",
    "# iter 100/105/488: loss 2.5840, time 63.73ms\n",
    "# iter 200/205/488: loss 2.5958, time 62.98ms\n",
    "# iter 300/305/488: loss 2.5835, time 60.15ms\n",
    "# iter 400/405/488: loss 2.5793, time 63.62ms\n",
    "\n",
    "# ## model = small\n",
    "# step 0: train loss 2.7741, val loss 2.7741\n",
    "# iter 0/5/488: loss 2.7743, time 1624.89ms\n",
    "# iter 100/105/488: loss 2.6157, time 141.39ms\n",
    "# iter 200/205/488: loss 2.6120, time 161.22ms\n",
    "# iter 300/305/488: loss 2.5983, time 203.82ms\n",
    "\n",
    "\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/4882: loss 2.7801, time 1422.53ms\n",
    "# iter 100/105/4882: loss 2.5970, time 110.71ms\n",
    "# iter 200/205/4882: loss 2.5962, time 116.96ms\n",
    "# iter 300/305/4882: loss 2.5917, time 160.95ms\n",
    "# iter 400/405/4882: loss 2.5885, time 63.37ms\n",
    "# iter 500/505/4882: loss 2.5912, time 65.25ms\n",
    "# iter 600/605/4882: loss 2.6000, time 67.49ms\n",
    "# iter 700/705/4882: loss 2.5780, time 61.43ms\n",
    "# iter 800/805/4882: loss 2.5864, time 265.56ms\n",
    "# iter 900/905/4882: loss 2.5857, time 263.09ms\n",
    "# step 1000: train loss 2.5849, val loss 2.5847\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 1000/1005/4882: loss 2.5844, time 1812.58ms\n",
    "# iter 1100/1105/4882: loss 2.5832, time 62.89ms\n",
    "# iter 1200/1205/4882: loss 2.5743, time 95.25ms\n",
    "# iter 1300/1305/4882: loss 2.5720, time 324.18ms\n",
    "# iter 1400/1405/4882: loss 2.5880, time 73.66ms\n",
    "# iter 1500/1505/4882: loss 2.5745, time 295.39ms\n",
    "# iter 1600/1605/4882: loss 2.5726, time 76.05ms\n",
    "# iter 1700/1705/4882: loss 2.5670, time 63.20ms\n",
    "# iter 1800/1805/4882: loss 2.5720, time 62.66ms\n",
    "# iter 1900/1905/4882: loss 2.5694, time 449.06ms\n",
    "# step 2000: train loss 2.5806, val loss 2.5806\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 2000/2005/4882: loss 2.5893, time 920.12ms\n",
    "# iter 2100/2105/4882: loss 2.5686, time 430.53ms\n",
    "# iter 2200/2205/4882: loss 2.5741, time 63.05ms\n",
    "# iter 2300/2305/4882: loss 2.5679, time 60.90ms\n",
    "# iter 2400/2405/4882: loss 2.5754, time 69.07ms\n",
    "# iter 2500/2505/4882: loss 2.5673, time 68.33ms\n",
    "# iter 2600/2605/4882: loss 2.5648, time 66.26ms\n",
    "# iter 2700/2705/4882: loss 2.5622, time 69.76ms\n",
    "# iter 2800/2805/4882: loss 2.5541, time 143.65ms\n",
    "# iter 2900/2905/4882: loss 2.5634, time 66.40ms\n",
    "# step 3000: train loss 2.5550, val loss 2.5547\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 3000/3005/4882: loss 2.5545, time 975.15ms\n",
    "# iter 3100/3105/4882: loss 2.5594, time 63.55ms\n",
    "# iter 3200/3205/4882: loss 2.5499, time 64.17ms\n",
    "# iter 3300/3305/4882: loss 2.5481, time 70.28ms\n",
    "# iter 3400/3405/4882: loss 2.5565, time 73.58ms\n",
    "# iter 3500/3505/4882: loss 2.5602, time 72.22ms\n",
    "# iter 3600/3605/4882: loss 2.5429, time 88.68ms\n",
    "# iter 3700/3705/4882: loss 2.5259, time 63.15ms\n",
    "# iter 3800/3805/4882: loss 2.5346, time 66.07ms\n",
    "# iter 3900/3905/4882: loss 2.5386, time 73.50ms\n",
    "# step 4000: train loss 2.5350, val loss 2.5345\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 4000/4005/4882: loss 2.5424, time 1217.41ms\n",
    "# iter 4100/4105/4882: loss 2.5290, time 101.01ms\n",
    "# iter 4200/4205/4882: loss 2.5323, time 61.94ms\n",
    "# iter 4300/4305/4882: loss 2.5250, time 72.57ms\n",
    "# iter 4400/4405/4882: loss 2.5243, time 68.38ms\n",
    "# iter 4500/4505/4882: loss 2.5331, time 73.33ms\n",
    "# iter 4600/4605/4882: loss 2.5246, time 101.00ms\n",
    "# iter 4700/4705/4882: loss 2.5336, time 67.27ms\n",
    "# iter 4800/4805/4882: loss 2.5170, time 79.40ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        # dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:5].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:6].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:7].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:8].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:9].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:10].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):    \n",
    "    list(dd.items())[10][1]['*'].sum() > 100\n",
    "    top_k = sorted(dd.items(), key=lambda kv: kv[1]['*'].sum(), reverse=True)[:20]\n",
    "    top_k_keys = sorted(k for k, v in top_k)\n",
    "    \n",
    "    prefix_list = top_k_keys\n",
    "\n",
    "    # prefix_list = [\n",
    "    #     (0,), \n",
    "    #     (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "    #     (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "    #     (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7),\n",
    "    # ]\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tournament to calcualte ELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from contextlib import asynccontextmanager\n",
    "from rgi.rgizero.tournament import Tournament\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_player_factory(model, simulations, game, device, block_size, action_vocab, max_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a shared evaluator and returns a factory function that produces \n",
    "    new AlphazeroPlayer instances using that shared evaluator.\n",
    "    \"\"\"\n",
    "    # 1. Setup the shared evaluator\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(\n",
    "        model, \n",
    "        device=device, \n",
    "        block_size=block_size, \n",
    "        vocab=action_vocab\n",
    "    )\n",
    "    async_evaluator = AsyncNetworkEvaluator(\n",
    "        base_evaluator=serial_evaluator, \n",
    "        max_batch_size=max_batch_size, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 2. Start the evaluator background task\n",
    "    await async_evaluator.start()\n",
    "    \n",
    "    try:\n",
    "        # 3. Define the factory. This is called by Tournament for every game.\n",
    "        # It creates a NEW player instance but uses the SHARED async_evaluator.\n",
    "        def player_factory():\n",
    "            # Create a fresh RNG for each game/player instance\n",
    "            rng = np.random.default_rng(np.random.randint(0, 2**31))\n",
    "            return AlphazeroPlayer(\n",
    "                game, \n",
    "                async_evaluator, \n",
    "                rng=rng, \n",
    "                add_noise=True, \n",
    "                simulations=simulations\n",
    "            )\n",
    "            \n",
    "        yield player_factory\n",
    "        \n",
    "    finally:\n",
    "        # 4. Cleanup\n",
    "        await async_evaluator.stop()\n",
    "\n",
    "async def run_tournament_async():\n",
    "    # Use async with to manage the lifecycle of the evaluators\n",
    "    async with (\n",
    "        # create_player_factory(model_dict[0], 100, game, device, block_size, action_vocab, 10) as factory_gen0_100,\n",
    "        # create_player_factory(model_dict[1], 100, game, device, block_size, action_vocab, 10) as factory_gen1_100,\n",
    "        # create_player_factory(model_dict[2], 100, game, device, block_size, action_vocab, 10) as factory_gen2_100,\n",
    "        # create_player_factory(model_dict[3], 100, game, device, block_size, action_vocab, 10) as factory_gen3_100,\n",
    "        # create_player_factory(model_dict[4], 100, game, device, block_size, action_vocab, 10) as factory_gen4_100,\n",
    "        # create_player_factory(model_dict[5], 100, game, device, block_size, action_vocab, 10) as factory_gen5_100,\n",
    "        # create_player_factory(model_dict[10], 100, game, device, block_size, action_vocab, 10) as factory_gen6_100,\n",
    "        # create_player_factory(model_dict[15], 100, game, device, block_size, action_vocab, 10) as factory_gen7_100,\n",
    "        # create_player_factory(model_dict[20], 100, game, device, block_size, action_vocab, 10) as factory_gen8_100,\n",
    "\n",
    "        create_player_factory(model_dict[0], 200, game, device, block_size, action_vocab, 10) as factory_gen0_200,\n",
    "        create_player_factory(model_dict[1], 200, game, device, block_size, action_vocab, 10) as factory_gen1_200,\n",
    "        create_player_factory(model_dict[2], 200, game, device, block_size, action_vocab, 10) as factory_gen2_200,\n",
    "        create_player_factory(model_dict[3], 200, game, device, block_size, action_vocab, 10) as factory_gen3_200,\n",
    "        create_player_factory(model_dict[4], 200, game, device, block_size, action_vocab, 10) as factory_gen4_200,\n",
    "        create_player_factory(model_dict[5], 200, game, device, block_size, action_vocab, 10) as factory_gen5_200,\n",
    "        create_player_factory(model_dict[10], 200, game, device, block_size, action_vocab, 10) as factory_gen10_200,\n",
    "        create_player_factory(model_dict[15], 200, game, device, block_size, action_vocab, 10) as factory_gen15_200,\n",
    "        create_player_factory(model_dict[20], 200, game, device, block_size, action_vocab, 10) as factory_gen20_200,\n",
    "        ):\n",
    "        \n",
    "        # The dictionary now maps names to FACTORIES (Callables), not Player instances\n",
    "        player_factories = {\n",
    "            # \"factory_gen0_100\": factory_gen0_100,\n",
    "            # \"factory_gen1_100\": factory_gen1_100,\n",
    "            # \"factory_gen2_100\": factory_gen2_100,\n",
    "            # \"factory_gen3_100\": factory_gen3_100,\n",
    "            # \"factory_gen4_100\": factory_gen4_100,\n",
    "            # \"factory_gen5_100\": factory_gen5_100,\n",
    "            # \"factory_gen6_100\": factory_gen6_100,\n",
    "            # \"factory_gen7_100\": factory_gen7_100,\n",
    "\n",
    "            \"factory_gen0_200\": factory_gen0_200,\n",
    "            \"factory_gen1_200\": factory_gen1_200,\n",
    "            \"factory_gen2_200\": factory_gen2_200,\n",
    "            #\"factory_gen3_200\": factory_gen3_200,\n",
    "            #\"factory_gen4_200\": factory_gen4_200,\n",
    "            \"factory_gen5_200\": factory_gen5_200,\n",
    "            \"factory_gen10_200\": factory_gen10_200,\n",
    "            #\"factory_gen15_200\": factory_gen15_200,\n",
    "            \"factory_gen20_200\": factory_gen20_200,\n",
    "        }\n",
    "        \n",
    "        tournament = Tournament(game, player_factories, initial_elo=1000)\n",
    "        \n",
    "        print(\"Running tournament...\")\n",
    "        await tournament.run(num_games=1_000, concurrent_games=2000)\n",
    "        tournament.print_standings()\n",
    "\n",
    "if RUN_TOURNAMENT:\n",
    "    await run_tournament_async()\n",
    "\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 10000/10000 [1:25:59<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen6_200     1140.5     1247     827-419-1      \n",
    "# 2     factory_gen2_200     1100.1     1251     693-554-4      \n",
    "# 3     factory_gen5_100     1074.4     1251     598-652-1      \n",
    "# 4     factory_gen3_200     1029.1     1252     674-573-5      \n",
    "# 5     factory_gen4_200     1027.0     1248     711-536-1      \n",
    "# 6     factory_gen0_200     1020.0     1254     444-810-0      \n",
    "# 7     factory_gen5_200     990.2      1248     742-502-4      \n",
    "# 8     factory_gen7_100     987.5      1250     650-597-3      \n",
    "# 9     factory_gen7_200     979.2      1248     768-476-4      \n",
    "# 10    factory_gen2_100     974.0      1249     522-723-4      \n",
    "# 11    factory_gen6_100     966.6      1248     684-564-0      \n",
    "# 12    factory_gen4_100     964.2      1251     557-693-1      \n",
    "# 13    factory_gen1_100     962.5      1252     547-705-0      \n",
    "# 14    factory_gen3_100     947.0      1251     528-723-0      \n",
    "# 15    factory_gen1_200     941.1      1252     630-620-2      \n",
    "# 16    factory_gen0_100     896.5      1248     410-838-0     \n",
    "\n",
    "\n",
    "## 20 generations.\n",
    "# Running tournament...\n",
    "# Tournament Progress: 100%|██████████| 1000/1000 [08:35<00:00,  1.94it/s]\n",
    "\n",
    "# Tournament Standings:\n",
    "# Rank  Player               ELO        Games    W-L-D          \n",
    "# -----------------------------------------------------------------\n",
    "# 1     factory_gen10_200    1114.2     333      212-120-1      \n",
    "# 2     factory_gen2_200     1032.6     333      190-141-2      \n",
    "# 3     factory_gen1_200     1003.9     334      159-175-0      \n",
    "# 4     factory_gen20_200    1000.9     335      171-164-0      \n",
    "# 5     factory_gen5_200     974.6      331      183-146-2      \n",
    "# 6     factory_gen0_200     873.8      334      82-251-1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.tuner import clear_failures_from_cache_file\n",
    "clear_failures_from_cache_file('result_cache-v0.0.2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "# Parameters which will never be used for tuning.\n",
    "fixed_params = dict(\n",
    "    model_name='c4-tuning',\n",
    "    model_version='0.1',\n",
    "    num_players = game.num_players(state_0),\n",
    "    vocab_size = action_vocab.vocab_size,\n",
    "    num_genrations = NUM_GENERATIONS,\n",
    "    data_dir = DATA_DIR,\n",
    "\n",
    "    eval_iters = 200,\n",
    "    log_interval = 1000,\n",
    "    eval_interval = 10_000,\n",
    "\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "initial_params = dict(\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_embd=8,  # tiny model\n",
    "\n",
    "    n_max_context=n_max_context,\n",
    "    batch_size = 32,\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    max_iters=100,\n",
    "    max_epochs=1_000_000, # Make max_epoch high, rely on max_iters to stop.\n",
    "        \n",
    "    learning_rate = LEARNING_RATE,    \n",
    "    decay_lr = True,  # whether to decay the learning rate\n",
    "    lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    "\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    grad_clip = 1.0,  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = \"float16\",\n",
    "\n",
    "    dropout = 0.0,\n",
    "    bias = False,  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    ")\n",
    "\n",
    "tune_options = dict(\n",
    "    n_layer = [2, 3, 4, 5, 6, 8, 10, 12, 16, 32],\n",
    "    # n_head = [1, 2, 4, 8, 16, 32],   # Needs to be calcualted to ensure n_embed % n_head == 0\n",
    "    n_embd = [8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "\n",
    "    n_max_context = [initial_params['n_max_context']],\n",
    "    batch_size = [16, 32, 64, 128, 256, 512, 1024],\n",
    "    gradient_accumulation_steps = [1],  # TODO: We only support 1 for now. This fails is we don't have an exact multiple of the batch size per epoch.\n",
    "\n",
    "    max_iters = [100, 300, 1_000, 3_000, 10_000, 30_000, 100_000, 300_000],\n",
    "    max_epochs = [1_000_000], # Make max_epoch high, rely on max_iters to stop.\n",
    "    \n",
    "    learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "    decay_lr = [False, True],\n",
    "\n",
    "    beta1 = [0.9],\n",
    "    beta2 = [0.95, 0.99],\n",
    "\n",
    "    weight_decay = [0.01, 0.1],\n",
    "    grad_clip = [0,0, 1.0],  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = [\"bfloat16\", \"float16\"],\n",
    "    dropout = [0.0, 0.01, 0.05, 0.1],\n",
    "    bias = [True, False],    \n",
    ")\n",
    "\n",
    "_n_head_options = [1, 2, 4, 8, 16, 32]\n",
    "computed_tune_options = dict(\n",
    "    min_lr = lambda opt: [opt['learning_rate'] / 10],\n",
    "    lr_decay_iters = lambda opt: [opt['max_iters']],\n",
    "    warmup_iters = lambda opt: [x for x in [0, 100, 1000] if x < opt['lr_decay_iters']] if opt['decay_lr'] else [0],\n",
    "    n_head = lambda opt: [n for n in _n_head_options if opt['n_embd'] % n == 0],\n",
    ")\n",
    "\n",
    "TUNER_VERSION = \"0.0.3\"\n",
    "\n",
    "from rgi.rgizero.models.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=1.00)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.1)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.01)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0001)\n",
    "tuner.autotune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.0)\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "tuner.autotune(num_generations=0)\n",
    "print(f'tuner.best_loss={tuner.best_loss}')\n",
    "print(f'tuner.best_loss_elapsed={int(tuner.best_loss_elapsed)//60}m{tuner.best_loss_elapsed%60:.0f}s')\n",
    "pprint(tuner.best_params)\n",
    "# best_params = tuner.initial_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print tuner stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "tuner = Tuner(\n",
    "    tune_options=tune_options.copy(), \n",
    "    initial_params=initial_params.copy(),\n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "# print stats based on cached results.\n",
    "tuner_stats = tuner.print_hparam_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(k,v['mean_val_delta']) for (k,v) in sorted(tuner_stats.items(), key=lambda x: x[1]['mean_val_delta'], reverse=True)]\n",
    "\n",
    "for x in  sorted([(v['mean_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True): print(x)\n",
    "# sorted([(v['mean_val_delta'], k) for (k,v) in tuner_stats.items() if not np.isnan(v['mean_val_delta'])], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted([(v['std_val_delta'], k, v['mean_val_1'], v['mean_val_2']) for (k,v) in tuner_stats.items() if not np.isnan(v['std_val_delta'])], reverse=True): print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx STOP HERE xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42, device=device)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
