{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.games.connect4 import Connect4Game\n",
    "from rgi.rgizero.players.alphazero import AlphazeroPlayer\n",
    "from rgi.rgizero.players.alphazero import play_game\n",
    "\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "assert device in ('cuda', 'mps'), f\"No accelerator available, device={device}\"\n",
    "\n",
    "# Allow asyncio to work with jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True     # Set options to make debugger work properly. Single worker, etc.\n",
    "LOAD_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "MODEL_SIZE = \"tiny\"  # \"tiny\" or \"small\" or \"large\" or \"xl\"\n",
    "NUM_SIMULATIONS = 200\n",
    "RUN_GENERATIONS = True\n",
    "\n",
    "# If False, we still load previous games from disk.\n",
    "NUM_GAMES = 10_000\n",
    "MAX_TRAINING_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 2048\n",
    "MAX_TRAINING_ITERS = 1_000_000 // TRAIN_BATCH_SIZE\n",
    "CONFIG_ALIAS = f'trajectory_sims-{NUM_SIMULATIONS}_games-{NUM_GAMES}_size-{MODEL_SIZE}_train-{MAX_TRAINING_ITERS}_x1'\n",
    "NUM_GENERATIONS = 5\n",
    "\n",
    "# DEBUG: Update batch_size after config_alias\n",
    "MODEL_SIZE = \"small\"\n",
    "MAX_TRAINING_ITERS = 100_000_000 // TRAIN_BATCH_SIZE\n",
    "MAX_TRAINING_EPOCHS = 10_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up history-wrapped game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using HistoryTrackingGame from module\n",
      "Game: Connect4Game, Players: 2, Actions: [1, 2, 3, 4, 5, 6, 7]\n",
      "Creating data dir:  /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Creating model dir:  /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1\n"
     ]
    }
   ],
   "source": [
    "from rgi.rgizero.games.history_wrapper import HistoryTrackingGame\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "base_game, max_game_length = Connect4Game(connect_length=4), 7*6\n",
    "\n",
    "game = HistoryTrackingGame(base_game)\n",
    "state_0 = game.initial_state()\n",
    "block_size = max_game_length + 2\n",
    "all_actions = game.all_actions()\n",
    "action_vocab = Vocab(itos=[TOKENS.START_OF_GAME] + list(all_actions))\n",
    "n_max_context = max_game_length + 2\n",
    "game_name = base_game.__class__.__name__\n",
    "\n",
    "print(\"✅ Using HistoryTrackingGame from module\")\n",
    "print(f\"Game: {game_name}, Players: {game.num_players(state_0)}, Actions: {list(game.all_actions())}\")\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating data dir: \", DATA_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path.cwd().parent / \"models\" / \"rgizero-e2e\" / game_name / CONFIG_ALIAS\n",
    "print(\"Creating model dir: \", MODEL_DIR)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create random generation_0 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(dataset_path, split_name):\n",
    "    \"\"\"Print statistics about a loaded trajectory dataset.\"\"\"\n",
    "    td = TrajectoryDataset(dataset_path.parent, split_name, block_size=n_max_context)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    num_trajectories = len(td)\n",
    "    total_actions = td._num_actions\n",
    "    avg_trajectory_length = total_actions / num_trajectories if num_trajectories > 0 else 0\n",
    "    \n",
    "    # Get trajectory lengths, winners, and first moves\n",
    "    trajectory_lengths = []\n",
    "    winners = []\n",
    "    first_moves = []\n",
    "    \n",
    "    for i in range(num_trajectories):\n",
    "        start_idx = td.boundaries[i]\n",
    "        end_idx = td.boundaries[i + 1]\n",
    "        traj_length = end_idx - start_idx\n",
    "        trajectory_lengths.append(traj_length)\n",
    "        \n",
    "        # Get winner from final values (values are the same throughout trajectory)\n",
    "        # Values are in range [-1, 1] where positive means player 1 advantage\n",
    "        final_values = td.value_data[start_idx]  # shape: (num_players,)\n",
    "        if final_values[0] > final_values[1]:\n",
    "            winners.append(1)\n",
    "        elif final_values[1] > final_values[0]:\n",
    "            winners.append(2)\n",
    "        else:\n",
    "            winners.append(None)  # Draw\n",
    "        \n",
    "        # Get first move (decode from vocab)\n",
    "        first_action_encoded = td.action_data[start_idx]\n",
    "        first_action = action_vocab.decode([first_action_encoded])[0]\n",
    "        first_moves.append(first_action)\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Dataset Stats:\")\n",
    "    print(f\"  Trajectories: {num_trajectories}\")\n",
    "    print(f\"  Total actions: {total_actions}\")\n",
    "    print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "    print(f\"  Trajectory length - min: {min(trajectory_lengths)}, max: {max(trajectory_lengths)}, mean: {np.mean(trajectory_lengths):.2f}\")\n",
    "    \n",
    "    # Print winner stats (similar to print_game_stats)\n",
    "    print(f\"Winner Stats:\")\n",
    "    winner_stats = Counter(winners)\n",
    "    total_games = num_trajectories\n",
    "    win1_pct = 100 * winner_stats[1] / total_games if total_games > 0 else 0\n",
    "    win2_pct = 100 * winner_stats[2] / total_games if total_games > 0 else 0\n",
    "    print(f\"  Winner counts: win[1]={win1_pct:.2f}% win[2]={win2_pct:.2f}%, n={total_games}\")\n",
    "    \n",
    "    # Print stats by initial move\n",
    "    print(f\"Winner Stats by initial move:\")\n",
    "    move_stats = defaultdict(Counter)\n",
    "    for first_move, winner in zip(first_moves, winners):\n",
    "        move_stats[first_move][winner] += 1\n",
    "    \n",
    "    for action in sorted(move_stats.keys()):\n",
    "        counts = move_stats[action]\n",
    "        total = sum(counts.values())\n",
    "        win1_pct = 100 * counts[1] / total if total > 0 else 0\n",
    "        win2_pct = 100 * counts[2] / total if total > 0 else 0\n",
    "        draw_pct = 100 * counts[None] / total if total > 0 else 0\n",
    "        print(f\"  a={action}: n={total:3} win[1]={win1_pct:.2f}% counts={counts}, win[2]={win2_pct:.2f}% draw={draw_pct:.2f}%\")\n",
    "\n",
    "def print_model_stats(model, config_alias=\"\"):\n",
    "    \"\"\"Print statistics about a model.\"\"\"\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Stats:\")\n",
    "    print(f\"  Config: {model.config}\")\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "    if config_alias:\n",
    "        print(f\"  Config alias: {config_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "from rgi.rgizero.models.action_history_transformer import ActionHistoryTransformer\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "\n",
    "model_config_dict = {\n",
    "    \"tiny\": TransformerConfig(n_max_context=n_max_context, n_layer=2, n_head=2, n_embd=8),\n",
    "    \"small\": TransformerConfig(n_max_context=n_max_context, n_layer=4, n_head=4, n_embd=32),\n",
    "    \"large\": TransformerConfig(n_max_context=n_max_context, n_layer=8, n_head=8, n_embd=128),\n",
    "    \"xl\": TransformerConfig(n_max_context=n_max_context, n_layer=16, n_head=16, n_embd=256),\n",
    "}\n",
    "\n",
    "\n",
    "def create_random_model(config: TransformerConfig, action_vocab_size, num_players,  seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) # Ensure numpy operations are also seeded\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    model = ActionHistoryTransformer(config=config, action_vocab_size=action_vocab_size, num_players=num_players)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Make model initialization deterministic\n",
    "model_config = model_config_dict[MODEL_SIZE]\n",
    "# TODO: Use MODEL_SIZE!\n",
    "# model_config = model_config_dict[\"small\"] # Override to see if we can fit better.\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define play & generation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.models.action_history_transformer import AsyncNetworkEvaluator, ActionHistoryTransformerEvaluator\n",
    "from rgi.rgizero.players.alphazero import play_game_async\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def play_games_async(num_games: int, player_factory: Callable[[], AlphazeroPlayer], max_concurrent_games: int = 1000):\n",
    "    sem = asyncio.Semaphore(max_concurrent_games)\n",
    "    tasks = []\n",
    "    async def create_player_and_create_game():\n",
    "        async with sem:\n",
    "            t0 = time.time()\n",
    "            player = player_factory()\n",
    "            game_result = await play_game_async(game, [player, player])\n",
    "            t1 = time.time()\n",
    "            game_result['time'] = t1 - t0\n",
    "            return game_result\n",
    "\n",
    "    tasks = [create_player_and_create_game() for _ in range(num_games)]\n",
    "    results = await tqdm.gather(*tasks)   # same as asyncio.gather, but with a progress bar\n",
    "    return results\n",
    "\n",
    "async def play_generation_async(model, num_games, simulations=NUM_SIMULATIONS, max_concurrent_games=1024):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    async_evaluator = AsyncNetworkEvaluator(base_evaluator=serial_evaluator, max_batch_size=max_concurrent_games, verbose=False)\n",
    "\n",
    "    master_rng = np.random.default_rng(42)\n",
    "    async_player_factory = lambda: AlphazeroPlayer(game, async_evaluator, rng=np.random.default_rng(master_rng.integers(0, 2**31)), add_noise=False, simulations=simulations)\n",
    "\n",
    "    await async_evaluator.start()\n",
    "    results = await play_games_async(num_games=num_games, player_factory=async_player_factory, max_concurrent_games=max_concurrent_games)\n",
    "    await async_evaluator.stop()\n",
    "    return results\n",
    "\n",
    "def print_game_stats(results):\n",
    "    print(\"Winner Stats:\")\n",
    "    winner_stats = Counter(result['winner'] for result in results)\n",
    "    print(f\"Winner counts: win[1]={100*winner_stats[1]/sum(winner_stats.values()):.2f}% win[2]={100*winner_stats[2]/sum(winner_stats.values()):.2f}%, n={sum(winner_stats.values())}, draw={100*winner_stats[None]/sum(winner_stats.values()):.2f}%\")\n",
    "    game_lengths = [len(result['action_history']) for result in results]\n",
    "    print(f\"Game Length min: {min(game_lengths)}, max: {max(game_lengths)}, mean: {np.mean(game_lengths):.2f}\")\n",
    "    print(\"Winner Stats by initial move:\")\n",
    "    dd = defaultdict(Counter)\n",
    "    for result in results:\n",
    "        dd[result['action_history'][0]][result['winner']] += 1\n",
    "    for action, counts in sorted(dd.items()):\n",
    "        print(f\"  a={action}: n={sum(counts.values()):3} win[1]={100*counts[1]/sum(counts.values()):.2f}% counts={counts}, win[2]={100*counts[2]/sum(counts.values()):.2f}% draw={100*counts[None]/sum(counts.values()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confirm we can read & write to trajectory_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import TrajectoryDatasetBuilder, TrajectoryDataset, build_trajectory_loader\n",
    "reload_local_modules(verbose=False)\n",
    "\n",
    "def add_trajectory(game_result, vocab, td_builder):\n",
    "    action_history = game_result['action_history']\n",
    "    trajectory_length = len(action_history)\n",
    "    legal_policies = game_result['legal_policies']\n",
    "    legal_action_idx = game_result['legal_action_idx']\n",
    "    rewards = game_result['rewards']\n",
    "\n",
    "    # Translation key for converting legal_action_ids to vocab_action_idx.\n",
    "    action_idx_to_vocab_idx = vocab.encode(all_actions)\n",
    "\n",
    "    fixed_width_policies = np.zeros((trajectory_length, vocab.vocab_size))\n",
    "    for i in range(trajectory_length):\n",
    "        vocab_action_idx = action_idx_to_vocab_idx[legal_action_idx[i]]\n",
    "        fixed_width_policies[i, vocab_action_idx] = legal_policies[i]\n",
    "\n",
    "    encoded_action_history = vocab.encode(action_history)\n",
    "    tiled_rewards = np.tile(rewards, (trajectory_length, 1))  # shape (num_players,) -> (num_moves, num_players)\n",
    "    \n",
    "    td_builder.add_trajectory(actions=encoded_action_history, fixed_width_policies=fixed_width_policies, values=tiled_rewards)\n",
    "\n",
    "def write_trajectory_dataset(results, action_vocab, generation_id):\n",
    "    td_builder = TrajectoryDatasetBuilder(action_vocab)\n",
    "    for game_result in results:\n",
    "        add_trajectory(game_result, action_vocab, td_builder)\n",
    "\n",
    "    trajectory_path = td_builder.save(DATA_DIR, f\"gen-{generation_id}\")\n",
    "    return trajectory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    model_name=\"connect4-e2e\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 1000,  # keep frequent because we'll overfit\n",
    "    eval_iters = 20,\n",
    "    log_interval = 100,  # don't print too too often\n",
    "    max_epochs = MAX_TRAINING_EPOCHS,\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "\n",
    "    learning_rate = LEARNING_RATE,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = MAX_TRAINING_ITERS,\n",
    "    lr_decay_iters = MAX_TRAINING_ITERS,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "def train_model(model, training_splits, train_config):\n",
    "    # Load dataset\n",
    "    num_workers = 0 if DEBUG_MODE else 4\n",
    "\n",
    "    trajectory_loader = build_trajectory_loader(\n",
    "        DATA_DIR, training_splits, block_size=n_max_context, batch_size=train_config.batch_size,\n",
    "        device=device, workers=num_workers, shuffle=True)\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_config=train_config,\n",
    "        train_loader=trajectory_loader,\n",
    "        val_loader=trajectory_loader,  # TODO: Create separate validation loader\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "def get_model_path(generation_id):\n",
    "    return MODEL_DIR / f\"gen-{generation_id}.pt\"\n",
    "\n",
    "def save_model(model, trainer, generation_id):\n",
    "    # Save model\n",
    "    model_path = get_model_path(generation_id)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_config': dataclasses.asdict(model.config),\n",
    "        'vocab': action_vocab.to_dict(),\n",
    "        'iter_num': trainer.iter_num,\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'num_players': game.num_players(state_0),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    return model_path\n",
    "\n",
    "def load_model(generation_id):\n",
    "    model_path = get_model_path(generation_id)\n",
    "    loaded_checkpoint = torch.load(model_path)\n",
    "    loaded_model = ActionHistoryTransformer(\n",
    "        config=TransformerConfig(**loaded_checkpoint['model_config']),\n",
    "        action_vocab_size=loaded_checkpoint['vocab']['vocab_size'],\n",
    "        num_players=loaded_checkpoint['num_players']\n",
    "    )\n",
    "    loaded_model.load_state_dict(loaded_checkpoint['model']) \n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a single generation of play & train\n",
    "async def run_generation(model, num_games, num_simulations, generation_id):\n",
    "    print(f\"\\n\\n## Running generation {generation_id} for config_alias={CONFIG_ALIAS}\")\n",
    "    split_name = f\"gen-{generation_id}\"\n",
    "    expected_trajectory_path = DATA_DIR / split_name\n",
    "    if not expected_trajectory_path.exists():\n",
    "        print(f\"Playing {num_games} games, simulations={num_simulations}, model_size={MODEL_SIZE}\")\n",
    "        results = await play_generation_async(model, num_games=NUM_GAMES, simulations=NUM_SIMULATIONS)\n",
    "        print_game_stats(results)\n",
    "        trajectory_path = write_trajectory_dataset(results, action_vocab, generation_id)\n",
    "        assert trajectory_path == expected_trajectory_path\n",
    "    else:\n",
    "        print(f\"Loading trajectory from {expected_trajectory_path}\")\n",
    "        print_dataset_stats(expected_trajectory_path, split_name)\n",
    "        trajectory_path = expected_trajectory_path\n",
    "        results = None\n",
    "\n",
    "    model_path = get_model_path(generation_id)\n",
    "    if not model_path.exists():\n",
    "        print(f\"Training model on {split_name}\")\n",
    "        training_splits = [f\"gen-{i}\" for i in range(1, generation_id+1)]\n",
    "        # TODO: We're continuing training on a previosu model here ... should we train a new model from scratch?\n",
    "        print(train_config)\n",
    "        updated_model, trainer = train_model(model, training_splits, train_config)\n",
    "        save_model(updated_model, trainer, generation_id)\n",
    "    else:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        updated_model = load_model(generation_id)\n",
    "        print_model_stats(updated_model, config_alias=MODEL_SIZE)\n",
    "\n",
    "    return results, trajectory_path, updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Running generation 1 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Loading trajectory from /Users/rodo/src/rgi3/data/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1\n",
      "Dataset Stats:\n",
      "  Trajectories: 10000\n",
      "  Total actions: 140493\n",
      "  Avg trajectory length: 14.05\n",
      "  Trajectory length - min: 7, max: 42, mean: 14.05\n",
      "Winner Stats:\n",
      "  Winner counts: win[1]=61.93% win[2]=38.06%, n=10000\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=1564 win[1]=51.92% counts=Counter({1: 812, 2: 752}), win[2]=48.08% draw=0.00%\n",
      "  a=2: n=1344 win[1]=57.37% counts=Counter({1: 771, 2: 573}), win[2]=42.63% draw=0.00%\n",
      "  a=3: n=1297 win[1]=65.23% counts=Counter({1: 846, 2: 451}), win[2]=34.77% draw=0.00%\n",
      "  a=4: n=1493 win[1]=76.36% counts=Counter({1: 1140, 2: 353}), win[2]=23.64% draw=0.00%\n",
      "  a=5: n=1525 win[1]=66.49% counts=Counter({1: 1014, 2: 511}), win[2]=33.51% draw=0.00%\n",
      "  a=6: n=1401 win[1]=62.03% counts=Counter({1: 869, 2: 532}), win[2]=37.97% draw=0.00%\n",
      "  a=7: n=1376 win[1]=53.85% counts=Counter({1: 741, 2: 634, None: 1}), win[2]=46.08% draw=0.07%\n",
      "Loading model from /Users/rodo/src/rgi3/models/rgizero-e2e/Connect4Game/trajectory_sims-200_games-10000_size-tiny_train-488_x1/gen-1.pt\n",
      "Model Stats:\n",
      "  Config: TransformerConfig(n_max_context=44, n_layer=4, n_head=4, n_embd=32, dropout=0.0, bias=False)\n",
      "  Total parameters: 51,178\n",
      "  Trainable parameters: 51,178\n",
      "  Config alias: small\n",
      "\n",
      "\n",
      "## Running generation 2 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [18:54<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=46.45% win[2]=53.55%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 38, mean: 13.62\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=933 win[1]=34.19% counts=Counter({2: 614, 1: 319}), win[2]=65.81% draw=0.00%\n",
      "  a=2: n=328 win[1]=20.12% counts=Counter({2: 262, 1: 66}), win[2]=79.88% draw=0.00%\n",
      "  a=3: n=1647 win[1]=43.90% counts=Counter({2: 924, 1: 723}), win[2]=56.10% draw=0.00%\n",
      "  a=4: n=4260 win[1]=57.54% counts=Counter({1: 2451, 2: 1809}), win[2]=42.46% draw=0.00%\n",
      "  a=5: n=1508 win[1]=41.71% counts=Counter({2: 879, 1: 629}), win[2]=58.29% draw=0.00%\n",
      "  a=6: n=330 win[1]=26.06% counts=Counter({2: 244, 1: 86}), win[2]=73.94% draw=0.00%\n",
      "  a=7: n=994 win[1]=37.32% counts=Counter({2: 623, 1: 371}), win[2]=62.68% draw=0.00%\n",
      "Training model on gen-2\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.6150, val loss 2.6149\n",
      "iter 0/10/48828: loss 2.6150, time 3071.97ms\n",
      "iter 100/110/48828: loss 2.5960, time 147.43ms\n",
      "iter 200/210/48828: loss 2.6007, time 171.00ms\n",
      "iter 300/310/48828: loss 2.5932, time 215.12ms\n",
      "iter 400/410/48828: loss 2.5897, time 107.57ms\n",
      "iter 500/510/48828: loss 2.5850, time 106.12ms\n",
      "iter 600/610/48828: loss 2.6012, time 212.78ms\n",
      "iter 700/710/48828: loss 2.6098, time 289.95ms\n",
      "iter 800/810/48828: loss 2.6083, time 102.11ms\n",
      "iter 900/910/48828: loss 2.6002, time 100.36ms\n",
      "step 1000: train loss 2.6047, val loss 2.6047\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 1000/1010/48828: loss 2.6059, time 2575.02ms\n",
      "iter 1100/1110/48828: loss 2.6030, time 107.70ms\n",
      "iter 1200/1210/48828: loss 2.6074, time 104.83ms\n",
      "iter 1300/1310/48828: loss 2.5967, time 348.56ms\n",
      "iter 1400/1410/48828: loss 2.6011, time 392.84ms\n",
      "iter 1500/1510/48828: loss 2.5902, time 105.97ms\n",
      "iter 1600/1610/48828: loss 2.5796, time 104.35ms\n",
      "iter 1700/1710/48828: loss 2.5931, time 108.19ms\n",
      "iter 1800/1810/48828: loss 2.5927, time 110.90ms\n",
      "iter 1900/1910/48828: loss 2.6073, time 109.42ms\n",
      "step 2000: train loss 2.5830, val loss 2.5830\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 2000/2010/48828: loss 2.5842, time 2151.08ms\n",
      "iter 2100/2110/48828: loss 2.6053, time 331.18ms\n",
      "iter 2200/2210/48828: loss 2.5992, time 106.40ms\n",
      "iter 2300/2310/48828: loss 2.5906, time 111.29ms\n",
      "iter 2400/2410/48828: loss 2.5865, time 113.75ms\n",
      "iter 2500/2510/48828: loss 2.5944, time 279.97ms\n",
      "iter 2600/2610/48828: loss 2.5963, time 99.81ms\n",
      "iter 2700/2710/48828: loss 2.6050, time 118.97ms\n",
      "iter 2800/2810/48828: loss 2.6075, time 99.71ms\n",
      "iter 2900/2910/48828: loss 2.5964, time 102.09ms\n",
      "step 3000: train loss 2.5828, val loss 2.5828\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 3000/3010/48828: loss 2.5780, time 2508.30ms\n",
      "iter 3100/3110/48828: loss 2.5869, time 112.29ms\n",
      "iter 3200/3210/48828: loss 2.5978, time 99.04ms\n",
      "iter 3300/3310/48828: loss 2.6025, time 116.75ms\n",
      "iter 3400/3410/48828: loss 2.5967, time 110.75ms\n",
      "iter 3500/3510/48828: loss 2.6052, time 100.52ms\n",
      "iter 3600/3610/48828: loss 2.5940, time 108.09ms\n",
      "iter 3700/3710/48828: loss 2.6001, time 107.33ms\n",
      "iter 3800/3810/48828: loss 2.6122, time 110.33ms\n",
      "iter 3900/3910/48828: loss 2.6039, time 98.90ms\n",
      "step 4000: train loss 2.6008, val loss 2.6009\n",
      "iter 4000/4010/48828: loss 2.6019, time 3055.17ms\n",
      "iter 4100/4110/48828: loss 2.6027, time 111.29ms\n",
      "iter 4200/4210/48828: loss 2.5945, time 114.19ms\n",
      "iter 4300/4310/48828: loss 2.6065, time 110.41ms\n",
      "iter 4400/4410/48828: loss 2.5950, time 102.38ms\n",
      "iter 4500/4510/48828: loss 2.6036, time 98.77ms\n",
      "iter 4600/4610/48828: loss 2.5896, time 106.59ms\n",
      "iter 4700/4710/48828: loss 2.5873, time 112.11ms\n",
      "iter 4800/4810/48828: loss 2.5901, time 113.90ms\n",
      "iter 4900/4910/48828: loss 2.5878, time 97.98ms\n",
      "step 5000: train loss 2.5994, val loss 2.5995\n",
      "iter 5000/5010/48828: loss 2.5963, time 2443.84ms\n",
      "iter 5100/5110/48828: loss 2.5954, time 105.00ms\n",
      "iter 5200/5210/48828: loss 2.6083, time 112.40ms\n",
      "iter 5300/5310/48828: loss 2.5928, time 100.69ms\n",
      "iter 5400/5410/48828: loss 2.6012, time 110.18ms\n",
      "iter 5500/5510/48828: loss 2.5836, time 98.20ms\n",
      "iter 5600/5610/48828: loss 2.5933, time 108.05ms\n",
      "iter 5700/5710/48828: loss 2.6031, time 96.84ms\n",
      "iter 5800/5810/48828: loss 2.6018, time 104.41ms\n",
      "iter 5900/5910/48828: loss 2.5883, time 112.65ms\n",
      "step 6000: train loss 2.5783, val loss 2.5784\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 6000/6010/48828: loss 2.5753, time 2491.27ms\n",
      "iter 6100/6110/48828: loss 2.5895, time 114.45ms\n",
      "iter 6200/6210/48828: loss 2.5862, time 115.58ms\n",
      "iter 6300/6310/48828: loss 2.5912, time 108.98ms\n",
      "iter 6400/6410/48828: loss 2.6014, time 563.54ms\n",
      "iter 6500/6510/48828: loss 2.6139, time 105.26ms\n",
      "iter 6600/6610/48828: loss 2.5978, time 106.17ms\n",
      "iter 6700/6710/48828: loss 2.5975, time 107.52ms\n",
      "iter 6800/6810/48828: loss 2.5910, time 99.11ms\n",
      "iter 6900/6910/48828: loss 2.5783, time 109.65ms\n",
      "step 7000: train loss 2.5997, val loss 2.5996\n",
      "iter 7000/7010/48828: loss 2.6014, time 2486.94ms\n",
      "iter 7100/7110/48828: loss 2.5944, time 109.59ms\n",
      "iter 7200/7210/48828: loss 2.5762, time 108.82ms\n",
      "iter 7300/7310/48828: loss 2.6022, time 102.20ms\n",
      "iter 7400/7410/48828: loss 2.5922, time 111.41ms\n",
      "iter 7500/7510/48828: loss 2.5962, time 101.59ms\n",
      "iter 7600/7610/48828: loss 2.5989, time 106.19ms\n",
      "iter 7700/7710/48828: loss 2.6036, time 109.29ms\n",
      "iter 7800/7810/48828: loss 2.6069, time 111.18ms\n",
      "iter 7900/7910/48828: loss 2.5965, time 101.31ms\n",
      "step 8000: train loss 2.5900, val loss 2.5898\n",
      "iter 8000/8010/48828: loss 2.5923, time 2979.57ms\n",
      "iter 8100/8110/48828: loss 2.5870, time 105.46ms\n",
      "iter 8200/8210/48828: loss 2.6002, time 102.84ms\n",
      "iter 8300/8310/48828: loss 2.5942, time 100.01ms\n",
      "iter 8400/8410/48828: loss 2.5845, time 108.18ms\n",
      "iter 8500/8510/48828: loss 2.6062, time 143.77ms\n",
      "iter 8600/8610/48828: loss 2.5862, time 102.64ms\n",
      "iter 8700/8710/48828: loss 2.5880, time 97.99ms\n",
      "iter 8800/8810/48828: loss 2.6025, time 112.31ms\n",
      "iter 8900/8910/48828: loss 2.5886, time 115.89ms\n",
      "step 9000: train loss 2.5979, val loss 2.5980\n",
      "iter 9000/9010/48828: loss 2.6006, time 2489.98ms\n",
      "iter 9100/9110/48828: loss 2.6050, time 107.77ms\n",
      "iter 9200/9210/48828: loss 2.5981, time 101.91ms\n",
      "iter 9300/9310/48828: loss 2.6114, time 97.52ms\n",
      "iter 9400/9410/48828: loss 2.5989, time 98.68ms\n",
      "iter 9500/9510/48828: loss 2.6020, time 108.33ms\n",
      "iter 9600/9610/48828: loss 2.5878, time 100.03ms\n",
      "iter 9700/9710/48828: loss 2.5989, time 99.05ms\n",
      "iter 9800/9810/48828: loss 2.6032, time 108.56ms\n",
      "iter 9900/9910/48828: loss 2.6035, time 97.99ms\n",
      "step 10000: train loss 2.5836, val loss 2.5838\n",
      "iter 10000/10010/48828: loss 2.5827, time 3244.51ms\n",
      "iter 10100/10110/48828: loss 2.5932, time 105.76ms\n",
      "iter 10200/10210/48828: loss 2.5919, time 100.08ms\n",
      "iter 10300/10310/48828: loss 2.5941, time 98.65ms\n",
      "iter 10400/10410/48828: loss 2.5858, time 108.10ms\n",
      "iter 10500/10510/48828: loss 2.5996, time 97.13ms\n",
      "iter 10600/10610/48828: loss 2.5849, time 106.51ms\n",
      "iter 10700/10710/48828: loss 2.5995, time 111.87ms\n",
      "iter 10800/10810/48828: loss 2.5825, time 112.11ms\n",
      "iter 10900/10910/48828: loss 2.5991, time 118.03ms\n",
      "step 11000: train loss 2.5951, val loss 2.5951\n",
      "iter 11000/11010/48828: loss 2.5967, time 3467.39ms\n",
      "iter 11100/11110/48828: loss 2.5957, time 107.49ms\n",
      "iter 11200/11210/48828: loss 2.5998, time 604.38ms\n",
      "iter 11300/11310/48828: loss 2.5921, time 113.64ms\n",
      "iter 11400/11410/48828: loss 2.5985, time 99.00ms\n",
      "iter 11500/11510/48828: loss 2.5937, time 107.82ms\n",
      "iter 11600/11610/48828: loss 2.5943, time 119.37ms\n",
      "iter 11700/11710/48828: loss 2.5954, time 97.91ms\n",
      "iter 11800/11810/48828: loss 2.5889, time 106.48ms\n",
      "iter 11900/11910/48828: loss 2.5835, time 106.24ms\n",
      "step 12000: train loss 2.5920, val loss 2.5919\n",
      "iter 12000/12010/48828: loss 2.5929, time 1967.25ms\n",
      "iter 12100/12110/48828: loss 2.5814, time 109.68ms\n",
      "iter 12200/12210/48828: loss 2.5796, time 106.94ms\n",
      "iter 12300/12310/48828: loss 2.5823, time 135.49ms\n",
      "iter 12400/12410/48828: loss 2.5874, time 107.65ms\n",
      "iter 12500/12510/48828: loss 2.5954, time 99.23ms\n",
      "iter 12600/12610/48828: loss 2.5884, time 98.16ms\n",
      "iter 12700/12710/48828: loss 2.5856, time 113.21ms\n",
      "iter 12800/12810/48828: loss 2.5792, time 99.49ms\n",
      "iter 12900/12910/48828: loss 2.5939, time 136.55ms\n",
      "step 13000: train loss 2.5763, val loss 2.5763\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 13000/13010/48828: loss 2.5774, time 3185.92ms\n",
      "iter 13100/13110/48828: loss 2.5825, time 100.84ms\n",
      "iter 13200/13210/48828: loss 2.5713, time 158.08ms\n",
      "iter 13300/13310/48828: loss 2.5978, time 107.95ms\n",
      "iter 13400/13410/48828: loss 2.5766, time 1841.95ms\n",
      "iter 13500/13510/48828: loss 2.5878, time 102.45ms\n",
      "iter 13600/13610/48828: loss 2.5897, time 112.44ms\n",
      "iter 13700/13710/48828: loss 2.5815, time 107.45ms\n",
      "iter 13800/13810/48828: loss 2.5784, time 110.62ms\n",
      "iter 13900/13910/48828: loss 2.5923, time 104.86ms\n",
      "step 14000: train loss 2.5812, val loss 2.5810\n",
      "iter 14000/14010/48828: loss 2.5782, time 2381.50ms\n",
      "iter 14100/14110/48828: loss 2.5807, time 108.38ms\n",
      "iter 14200/14210/48828: loss 2.5943, time 111.10ms\n",
      "iter 14300/14310/48828: loss 2.5871, time 123.52ms\n",
      "iter 14400/14410/48828: loss 2.6003, time 99.54ms\n",
      "iter 14500/14510/48828: loss 2.6011, time 109.10ms\n",
      "iter 14600/14610/48828: loss 2.6043, time 99.19ms\n",
      "iter 14700/14710/48828: loss 2.5880, time 106.49ms\n",
      "iter 14800/14810/48828: loss 2.5882, time 109.71ms\n",
      "iter 14900/14910/48828: loss 2.6070, time 105.70ms\n",
      "step 15000: train loss 2.5947, val loss 2.5946\n",
      "iter 15000/15010/48828: loss 2.5928, time 2086.75ms\n",
      "iter 15100/15110/48828: loss 2.5873, time 108.75ms\n",
      "iter 15200/15210/48828: loss 2.5908, time 113.01ms\n",
      "iter 15300/15310/48828: loss 2.5829, time 131.70ms\n",
      "iter 15400/15410/48828: loss 2.5834, time 100.50ms\n",
      "iter 15500/15510/48828: loss 2.5929, time 107.97ms\n",
      "iter 15600/15610/48828: loss 2.5900, time 101.63ms\n",
      "iter 15700/15710/48828: loss 2.5971, time 117.94ms\n",
      "iter 15800/15810/48828: loss 2.6127, time 108.85ms\n",
      "iter 15900/15910/48828: loss 2.5964, time 101.73ms\n",
      "step 16000: train loss 2.5950, val loss 2.5951\n",
      "iter 16000/16010/48828: loss 2.5957, time 1898.48ms\n",
      "iter 16100/16110/48828: loss 2.5923, time 109.32ms\n",
      "iter 16200/16210/48828: loss 2.5920, time 102.72ms\n",
      "iter 16300/16310/48828: loss 2.5965, time 106.41ms\n",
      "iter 16400/16410/48828: loss 2.5832, time 108.17ms\n",
      "iter 16500/16510/48828: loss 2.5947, time 104.13ms\n",
      "iter 16600/16610/48828: loss 2.5978, time 124.73ms\n",
      "iter 16700/16710/48828: loss 2.5955, time 110.25ms\n",
      "iter 16800/16810/48828: loss 2.6001, time 113.32ms\n",
      "iter 16900/16910/48828: loss 2.5981, time 105.02ms\n",
      "step 17000: train loss 2.5878, val loss 2.5877\n",
      "iter 17000/17010/48828: loss 2.5878, time 1932.13ms\n",
      "iter 17100/17110/48828: loss 2.5912, time 113.26ms\n",
      "iter 17200/17210/48828: loss 2.5908, time 112.26ms\n",
      "iter 17300/17310/48828: loss 2.5955, time 102.87ms\n",
      "iter 17400/17410/48828: loss 2.6038, time 108.62ms\n",
      "iter 17500/17510/48828: loss 2.5900, time 109.57ms\n",
      "iter 17600/17610/48828: loss 2.5988, time 105.04ms\n",
      "iter 17700/17710/48828: loss 2.5965, time 99.01ms\n",
      "iter 17800/17810/48828: loss 2.5941, time 108.41ms\n",
      "iter 17900/17910/48828: loss 2.6012, time 111.92ms\n",
      "step 18000: train loss 2.6187, val loss 2.6187\n",
      "iter 18000/18010/48828: loss 2.6201, time 1881.96ms\n",
      "iter 18100/18110/48828: loss 2.6010, time 101.62ms\n",
      "iter 18200/18210/48828: loss 2.5936, time 102.91ms\n",
      "iter 18300/18310/48828: loss 2.5861, time 100.24ms\n",
      "iter 18400/18410/48828: loss 2.6035, time 151.01ms\n",
      "iter 18500/18510/48828: loss 2.5916, time 106.48ms\n",
      "iter 18600/18610/48828: loss 2.5952, time 108.21ms\n",
      "iter 18700/18710/48828: loss 2.5904, time 109.46ms\n",
      "iter 18800/18810/48828: loss 2.6038, time 105.55ms\n",
      "iter 18900/18910/48828: loss 2.5992, time 104.91ms\n",
      "step 19000: train loss 2.5871, val loss 2.5870\n",
      "iter 19000/19010/48828: loss 2.5848, time 1837.22ms\n",
      "iter 19100/19110/48828: loss 2.5986, time 100.07ms\n",
      "iter 19200/19210/48828: loss 2.5973, time 108.60ms\n",
      "iter 19300/19310/48828: loss 2.5917, time 105.73ms\n",
      "iter 19400/19410/48828: loss 2.5974, time 104.87ms\n",
      "iter 19500/19510/48828: loss 2.5982, time 99.67ms\n",
      "iter 19600/19610/48828: loss 2.5798, time 112.48ms\n",
      "iter 19700/19710/48828: loss 2.6014, time 111.32ms\n",
      "iter 19800/19810/48828: loss 2.6072, time 114.17ms\n",
      "iter 19900/19910/48828: loss 2.5963, time 105.70ms\n",
      "step 20000: train loss 2.5961, val loss 2.5960\n",
      "iter 20000/20010/48828: loss 2.5963, time 1882.15ms\n",
      "iter 20100/20110/48828: loss 2.5784, time 109.09ms\n",
      "iter 20200/20210/48828: loss 2.5740, time 106.99ms\n",
      "iter 20300/20310/48828: loss 2.5836, time 137.30ms\n",
      "iter 20400/20410/48828: loss 2.5975, time 114.25ms\n",
      "iter 20500/20510/48828: loss 2.5824, time 98.52ms\n",
      "iter 20600/20610/48828: loss 2.5923, time 104.70ms\n",
      "iter 20700/20710/48828: loss 2.5858, time 109.33ms\n",
      "iter 20800/20810/48828: loss 2.5810, time 103.17ms\n",
      "iter 20900/20910/48828: loss 2.5983, time 102.68ms\n",
      "step 21000: train loss 2.5964, val loss 2.5965\n",
      "iter 21000/21010/48828: loss 2.5940, time 2874.42ms\n",
      "iter 21100/21110/48828: loss 2.5759, time 102.05ms\n",
      "iter 21200/21210/48828: loss 2.5747, time 104.67ms\n",
      "iter 21300/21310/48828: loss 2.5959, time 110.39ms\n",
      "iter 21400/21410/48828: loss 2.5947, time 107.30ms\n",
      "iter 21500/21510/48828: loss 2.6037, time 109.11ms\n",
      "iter 21600/21610/48828: loss 2.5997, time 108.47ms\n",
      "iter 21700/21710/48828: loss 2.6009, time 117.65ms\n",
      "iter 21800/21810/48828: loss 2.5952, time 103.07ms\n",
      "iter 21900/21910/48828: loss 2.5923, time 104.98ms\n",
      "step 22000: train loss 2.5816, val loss 2.5815\n",
      "iter 22000/22010/48828: loss 2.5858, time 1944.22ms\n",
      "iter 22100/22110/48828: loss 2.5906, time 100.88ms\n",
      "iter 22200/22210/48828: loss 2.5890, time 224.61ms\n",
      "iter 22300/22310/48828: loss 2.5861, time 98.94ms\n",
      "iter 22400/22410/48828: loss 2.5937, time 108.60ms\n",
      "iter 22500/22510/48828: loss 2.5963, time 109.66ms\n",
      "iter 22600/22610/48828: loss 2.5886, time 99.93ms\n",
      "iter 22700/22710/48828: loss 2.5891, time 97.50ms\n",
      "iter 22800/22810/48828: loss 2.5900, time 107.61ms\n",
      "iter 22900/22910/48828: loss 2.5843, time 109.18ms\n",
      "step 23000: train loss 2.5872, val loss 2.5872\n",
      "iter 23000/23010/48828: loss 2.5930, time 3063.25ms\n",
      "iter 23100/23110/48828: loss 2.5774, time 98.35ms\n",
      "iter 23200/23210/48828: loss 2.5815, time 98.51ms\n",
      "iter 23300/23310/48828: loss 2.5814, time 105.25ms\n",
      "iter 23400/23410/48828: loss 2.5869, time 125.75ms\n",
      "iter 23500/23510/48828: loss 2.5843, time 108.21ms\n",
      "iter 23600/23610/48828: loss 2.5805, time 221.24ms\n",
      "iter 23700/23710/48828: loss 2.5822, time 106.49ms\n",
      "iter 23800/23810/48828: loss 2.5858, time 113.05ms\n",
      "iter 23900/23910/48828: loss 2.5941, time 110.25ms\n",
      "step 24000: train loss 2.5907, val loss 2.5907\n",
      "iter 24000/24010/48828: loss 2.5899, time 2823.77ms\n",
      "iter 24100/24110/48828: loss 2.5897, time 97.10ms\n",
      "iter 24200/24210/48828: loss 2.5793, time 105.71ms\n",
      "iter 24300/24310/48828: loss 2.6061, time 109.60ms\n",
      "iter 24400/24410/48828: loss 2.5860, time 97.66ms\n",
      "iter 24500/24510/48828: loss 2.5814, time 105.21ms\n",
      "iter 24600/24610/48828: loss 2.5905, time 108.46ms\n",
      "iter 24700/24710/48828: loss 2.5843, time 113.54ms\n",
      "iter 24800/24810/48828: loss 2.5884, time 100.49ms\n",
      "iter 24900/24910/48828: loss 2.5809, time 110.48ms\n",
      "step 25000: train loss 2.6005, val loss 2.6005\n",
      "iter 25000/25010/48828: loss 2.6021, time 1776.17ms\n",
      "iter 25100/25110/48828: loss 2.5930, time 110.29ms\n",
      "iter 25200/25210/48828: loss 2.5904, time 107.44ms\n",
      "iter 25300/25310/48828: loss 2.5861, time 100.66ms\n",
      "iter 25400/25410/48828: loss 2.5971, time 101.24ms\n",
      "iter 25500/25510/48828: loss 2.5893, time 100.08ms\n",
      "iter 25600/25610/48828: loss 2.5855, time 108.32ms\n",
      "iter 25700/25710/48828: loss 2.5897, time 99.46ms\n",
      "iter 25800/25810/48828: loss 2.5913, time 112.68ms\n",
      "iter 25900/25910/48828: loss 2.5804, time 107.21ms\n",
      "step 26000: train loss 2.5913, val loss 2.5913\n",
      "iter 26000/26010/48828: loss 2.5940, time 2872.46ms\n",
      "iter 26100/26110/48828: loss 2.5730, time 102.43ms\n",
      "iter 26200/26210/48828: loss 2.5930, time 99.37ms\n",
      "iter 26300/26310/48828: loss 2.6030, time 113.09ms\n",
      "iter 26400/26410/48828: loss 2.5767, time 107.37ms\n",
      "iter 26500/26510/48828: loss 2.5646, time 114.83ms\n",
      "iter 26600/26610/48828: loss 2.5781, time 110.65ms\n",
      "iter 26700/26710/48828: loss 2.5907, time 106.00ms\n",
      "iter 26800/26810/48828: loss 2.5841, time 100.69ms\n",
      "iter 26900/26910/48828: loss 2.5808, time 94.71ms\n",
      "step 27000: train loss 2.6009, val loss 2.6009\n",
      "iter 27000/27010/48828: loss 2.5998, time 1808.93ms\n",
      "iter 27100/27110/48828: loss 2.5995, time 108.74ms\n",
      "iter 27200/27210/48828: loss 2.5848, time 110.64ms\n",
      "iter 27300/27310/48828: loss 2.5826, time 100.26ms\n",
      "iter 27400/27410/48828: loss 2.5983, time 107.60ms\n",
      "iter 27500/27510/48828: loss 2.5955, time 105.37ms\n",
      "iter 27600/27610/48828: loss 2.5947, time 114.45ms\n",
      "iter 27700/27710/48828: loss 2.5939, time 115.96ms\n",
      "iter 27800/27810/48828: loss 2.5875, time 100.53ms\n",
      "iter 27900/27910/48828: loss 2.5920, time 108.77ms\n",
      "step 28000: train loss 2.5941, val loss 2.5942\n",
      "iter 28000/28010/48828: loss 2.5954, time 1990.26ms\n",
      "iter 28100/28110/48828: loss 2.5911, time 149.68ms\n",
      "iter 28200/28210/48828: loss 2.5936, time 111.07ms\n",
      "iter 28300/28310/48828: loss 2.5877, time 103.08ms\n",
      "iter 28400/28410/48828: loss 2.5825, time 101.37ms\n",
      "iter 28500/28510/48828: loss 2.5827, time 111.48ms\n",
      "iter 28600/28610/48828: loss 2.5855, time 101.60ms\n",
      "iter 28700/28710/48828: loss 2.5847, time 110.17ms\n",
      "iter 28800/28810/48828: loss 2.5877, time 104.76ms\n",
      "iter 28900/28910/48828: loss 2.5732, time 154.20ms\n",
      "step 29000: train loss 2.5713, val loss 2.5714\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 29000/29010/48828: loss 2.5693, time 3335.61ms\n",
      "iter 29100/29110/48828: loss 2.5839, time 99.71ms\n",
      "iter 29200/29210/48828: loss 2.6072, time 107.03ms\n",
      "iter 29300/29310/48828: loss 2.5921, time 112.61ms\n",
      "iter 29400/29410/48828: loss 2.5919, time 109.76ms\n",
      "iter 29500/29510/48828: loss 2.5832, time 105.32ms\n",
      "iter 29600/29610/48828: loss 2.5873, time 112.68ms\n",
      "iter 29700/29710/48828: loss 2.5777, time 118.90ms\n",
      "iter 29800/29810/48828: loss 2.5896, time 110.49ms\n",
      "iter 29900/29910/48828: loss 2.5866, time 112.97ms\n",
      "step 30000: train loss 2.5792, val loss 2.5794\n",
      "iter 30000/30010/48828: loss 2.5795, time 1750.80ms\n",
      "iter 30100/30110/48828: loss 2.5962, time 104.54ms\n",
      "iter 30200/30210/48828: loss 2.5774, time 127.19ms\n",
      "iter 30300/30310/48828: loss 2.5740, time 105.33ms\n",
      "iter 30400/30410/48828: loss 2.5673, time 110.51ms\n",
      "iter 30500/30510/48828: loss 2.5728, time 110.77ms\n",
      "iter 30600/30610/48828: loss 2.5917, time 100.72ms\n",
      "iter 30700/30710/48828: loss 2.5788, time 104.87ms\n",
      "iter 30800/30810/48828: loss 2.5781, time 107.82ms\n",
      "iter 30900/30910/48828: loss 2.5811, time 101.52ms\n",
      "step 31000: train loss 2.5807, val loss 2.5807\n",
      "iter 31000/31010/48828: loss 2.5765, time 1970.39ms\n",
      "iter 31100/31110/48828: loss 2.6065, time 112.42ms\n",
      "iter 31200/31210/48828: loss 2.5756, time 102.38ms\n",
      "iter 31300/31310/48828: loss 2.5857, time 102.32ms\n",
      "iter 31400/31410/48828: loss 2.5720, time 100.22ms\n",
      "iter 31500/31510/48828: loss 2.5633, time 107.56ms\n",
      "iter 31600/31610/48828: loss 2.5762, time 107.47ms\n",
      "iter 31700/31710/48828: loss 2.5790, time 110.93ms\n",
      "iter 31800/31810/48828: loss 2.5756, time 103.78ms\n",
      "iter 31900/31910/48828: loss 2.5687, time 101.03ms\n",
      "step 32000: train loss 2.5684, val loss 2.5683\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 32000/32010/48828: loss 2.5678, time 1788.46ms\n",
      "iter 32100/32110/48828: loss 2.5787, time 111.40ms\n",
      "iter 32200/32210/48828: loss 2.5783, time 98.36ms\n",
      "iter 32300/32310/48828: loss 2.5737, time 133.17ms\n",
      "iter 32400/32410/48828: loss 2.5731, time 102.04ms\n",
      "iter 32500/32510/48828: loss 2.5863, time 97.91ms\n",
      "iter 32600/32610/48828: loss 2.5993, time 97.55ms\n",
      "iter 32700/32710/48828: loss 2.5733, time 98.11ms\n",
      "iter 32800/32810/48828: loss 2.5685, time 100.44ms\n",
      "iter 32900/32910/48828: loss 2.5826, time 127.19ms\n",
      "step 33000: train loss 2.5749, val loss 2.5748\n",
      "iter 33000/33010/48828: loss 2.5784, time 1811.85ms\n",
      "iter 33100/33110/48828: loss 2.5533, time 109.40ms\n",
      "iter 33200/33210/48828: loss 2.5975, time 106.49ms\n",
      "iter 33300/33310/48828: loss 2.5764, time 133.30ms\n",
      "iter 33400/33410/48828: loss 2.5593, time 105.77ms\n",
      "iter 33500/33510/48828: loss 2.5602, time 98.83ms\n",
      "iter 33600/33610/48828: loss 2.5970, time 108.51ms\n",
      "iter 33700/33710/48828: loss 2.5743, time 107.76ms\n",
      "iter 33800/33810/48828: loss 2.5671, time 111.70ms\n",
      "iter 33900/33910/48828: loss 2.5782, time 107.00ms\n",
      "step 34000: train loss 2.5738, val loss 2.5737\n",
      "iter 34000/34010/48828: loss 2.5786, time 2958.78ms\n",
      "iter 34100/34110/48828: loss 2.5737, time 98.72ms\n",
      "iter 34200/34210/48828: loss 2.5627, time 134.92ms\n",
      "iter 34300/34310/48828: loss 2.5691, time 108.16ms\n",
      "iter 34400/34410/48828: loss 2.5773, time 131.23ms\n",
      "iter 34500/34510/48828: loss 2.5640, time 110.28ms\n",
      "iter 34600/34610/48828: loss 2.5756, time 99.00ms\n",
      "iter 34700/34710/48828: loss 2.5718, time 115.80ms\n",
      "iter 34800/34810/48828: loss 2.5754, time 106.43ms\n",
      "iter 34900/34910/48828: loss 2.5557, time 110.07ms\n",
      "step 35000: train loss 2.5629, val loss 2.5630\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 35000/35010/48828: loss 2.5682, time 1838.70ms\n",
      "iter 35100/35110/48828: loss 2.5552, time 137.45ms\n",
      "iter 35200/35210/48828: loss 2.5701, time 104.87ms\n",
      "iter 35300/35310/48828: loss 2.5569, time 102.69ms\n",
      "iter 35400/35410/48828: loss 2.5618, time 101.74ms\n",
      "iter 35500/35510/48828: loss 2.5562, time 110.11ms\n",
      "iter 35600/35610/48828: loss 2.5544, time 101.33ms\n",
      "iter 35700/35710/48828: loss 2.5601, time 102.20ms\n",
      "iter 35800/35810/48828: loss 2.5649, time 96.45ms\n",
      "iter 35900/35910/48828: loss 2.5625, time 101.12ms\n",
      "step 36000: train loss 2.5509, val loss 2.5510\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 36000/36010/48828: loss 2.5492, time 1820.39ms\n",
      "iter 36100/36110/48828: loss 2.5509, time 107.14ms\n",
      "iter 36200/36210/48828: loss 2.5627, time 109.78ms\n",
      "iter 36300/36310/48828: loss 2.5652, time 101.26ms\n",
      "iter 36400/36410/48828: loss 2.5596, time 106.83ms\n",
      "iter 36500/36510/48828: loss 2.5711, time 110.46ms\n",
      "iter 36600/36610/48828: loss 2.5609, time 112.99ms\n",
      "iter 36700/36710/48828: loss 2.5646, time 99.17ms\n",
      "iter 36800/36810/48828: loss 2.5714, time 103.10ms\n",
      "iter 36900/36910/48828: loss 2.5848, time 105.13ms\n",
      "step 37000: train loss 2.5675, val loss 2.5673\n",
      "iter 37000/37010/48828: loss 2.5615, time 2106.18ms\n",
      "iter 37100/37110/48828: loss 2.5615, time 111.18ms\n",
      "iter 37200/37210/48828: loss 2.5792, time 100.95ms\n",
      "iter 37300/37310/48828: loss 2.5579, time 101.46ms\n",
      "iter 37400/37410/48828: loss 2.5530, time 111.58ms\n",
      "iter 37500/37510/48828: loss 2.5503, time 110.25ms\n",
      "iter 37600/37610/48828: loss 2.5635, time 110.30ms\n",
      "iter 37700/37710/48828: loss 2.5542, time 128.10ms\n",
      "iter 37800/37810/48828: loss 2.5527, time 105.52ms\n",
      "iter 37900/37910/48828: loss 2.5589, time 98.80ms\n",
      "step 38000: train loss 2.5510, val loss 2.5511\n",
      "iter 38000/38010/48828: loss 2.5486, time 1852.38ms\n",
      "iter 38100/38110/48828: loss 2.5504, time 108.30ms\n",
      "iter 38200/38210/48828: loss 2.5633, time 102.32ms\n",
      "iter 38300/38310/48828: loss 2.5456, time 102.35ms\n",
      "iter 38400/38410/48828: loss 2.5586, time 100.44ms\n",
      "iter 38500/38510/48828: loss 2.5491, time 107.86ms\n",
      "iter 38600/38610/48828: loss 2.5424, time 108.78ms\n",
      "iter 38700/38710/48828: loss 2.5473, time 108.16ms\n",
      "iter 38800/38810/48828: loss 2.5453, time 103.41ms\n",
      "iter 38900/38910/48828: loss 2.5320, time 100.71ms\n",
      "step 39000: train loss 2.5388, val loss 2.5387\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 39000/39010/48828: loss 2.5378, time 1830.23ms\n",
      "iter 39100/39110/48828: loss 2.5355, time 112.44ms\n",
      "iter 39200/39210/48828: loss 2.5487, time 106.73ms\n",
      "iter 39300/39310/48828: loss 2.5316, time 110.28ms\n",
      "iter 39400/39410/48828: loss 2.5416, time 100.42ms\n",
      "iter 39500/39510/48828: loss 2.5361, time 99.06ms\n",
      "iter 39600/39610/48828: loss 2.5309, time 100.67ms\n",
      "iter 39700/39710/48828: loss 2.5435, time 109.78ms\n",
      "iter 39800/39810/48828: loss 2.5478, time 112.87ms\n",
      "iter 39900/39910/48828: loss 2.5538, time 109.06ms\n",
      "step 40000: train loss 2.5352, val loss 2.5354\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 40000/40010/48828: loss 2.5317, time 1762.64ms\n",
      "iter 40100/40110/48828: loss 2.5339, time 107.00ms\n",
      "iter 40200/40210/48828: loss 2.5340, time 125.86ms\n",
      "iter 40300/40310/48828: loss 2.5459, time 106.73ms\n",
      "iter 40400/40410/48828: loss 2.5403, time 129.49ms\n",
      "iter 40500/40510/48828: loss 2.5348, time 109.18ms\n",
      "iter 40600/40610/48828: loss 2.5387, time 101.47ms\n",
      "iter 40700/40710/48828: loss 2.5329, time 99.36ms\n",
      "iter 40800/40810/48828: loss 2.5363, time 101.22ms\n",
      "iter 40900/40910/48828: loss 2.5455, time 107.22ms\n",
      "step 41000: train loss 2.5542, val loss 2.5544\n",
      "iter 41000/41010/48828: loss 2.5517, time 1965.74ms\n",
      "iter 41100/41110/48828: loss 2.5432, time 109.18ms\n",
      "iter 41200/41210/48828: loss 2.5581, time 113.79ms\n",
      "iter 41300/41310/48828: loss 2.5346, time 100.86ms\n",
      "iter 41400/41410/48828: loss 2.5305, time 101.04ms\n",
      "iter 41500/41510/48828: loss 2.5377, time 101.05ms\n",
      "iter 41600/41610/48828: loss 2.5277, time 109.33ms\n",
      "iter 41700/41710/48828: loss 2.5223, time 113.09ms\n",
      "iter 41800/41810/48828: loss 2.5248, time 111.36ms\n",
      "iter 41900/41910/48828: loss 2.5351, time 125.72ms\n",
      "step 42000: train loss 2.5301, val loss 2.5304\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 42000/42010/48828: loss 2.5288, time 1865.96ms\n",
      "iter 42100/42110/48828: loss 2.5339, time 112.42ms\n",
      "iter 42200/42210/48828: loss 2.5284, time 109.25ms\n",
      "iter 42300/42310/48828: loss 2.5278, time 111.01ms\n",
      "iter 42400/42410/48828: loss 2.5151, time 102.48ms\n",
      "iter 42500/42510/48828: loss 2.5249, time 100.96ms\n",
      "iter 42600/42610/48828: loss 2.5243, time 100.73ms\n",
      "iter 42700/42710/48828: loss 2.5103, time 110.57ms\n",
      "iter 42800/42810/48828: loss 2.5253, time 105.21ms\n",
      "iter 42900/42910/48828: loss 2.5227, time 98.98ms\n",
      "step 43000: train loss 2.5194, val loss 2.5193\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 43000/43010/48828: loss 2.5240, time 2133.82ms\n",
      "iter 43100/43110/48828: loss 2.5175, time 101.72ms\n",
      "iter 43200/43210/48828: loss 2.5106, time 101.03ms\n",
      "iter 43300/43310/48828: loss 2.5159, time 107.23ms\n",
      "iter 43400/43410/48828: loss 2.4993, time 97.29ms\n",
      "iter 43500/43510/48828: loss 2.5105, time 99.29ms\n",
      "iter 43600/43610/48828: loss 2.5131, time 109.76ms\n",
      "iter 43700/43710/48828: loss 2.5230, time 106.77ms\n",
      "iter 43800/43810/48828: loss 2.5069, time 103.07ms\n",
      "iter 43900/43910/48828: loss 2.5129, time 111.37ms\n",
      "step 44000: train loss 2.5041, val loss 2.5041\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 44000/44010/48828: loss 2.5066, time 3464.03ms\n",
      "iter 44100/44110/48828: loss 2.5145, time 109.71ms\n",
      "iter 44200/44210/48828: loss 2.4916, time 102.79ms\n",
      "iter 44300/44310/48828: loss 2.4965, time 113.53ms\n",
      "iter 44400/44410/48828: loss 2.5078, time 104.18ms\n",
      "iter 44500/44510/48828: loss 2.4970, time 98.83ms\n",
      "iter 44600/44610/48828: loss 2.4991, time 101.21ms\n",
      "iter 44700/44710/48828: loss 2.4967, time 107.60ms\n",
      "iter 44800/44810/48828: loss 2.4923, time 104.68ms\n",
      "iter 44900/44910/48828: loss 2.4814, time 113.61ms\n",
      "step 45000: train loss 2.4957, val loss 2.4952\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 45000/45010/48828: loss 2.5016, time 1885.37ms\n",
      "iter 45100/45110/48828: loss 2.4906, time 104.90ms\n",
      "iter 45200/45210/48828: loss 2.4743, time 116.33ms\n",
      "iter 45300/45310/48828: loss 2.4803, time 111.03ms\n",
      "iter 45400/45410/48828: loss 2.4875, time 107.41ms\n",
      "iter 45500/45510/48828: loss 2.4705, time 109.95ms\n",
      "iter 45600/45610/48828: loss 2.4768, time 101.82ms\n",
      "iter 45700/45710/48828: loss 2.4454, time 103.39ms\n",
      "iter 45800/45810/48828: loss 2.4207, time 104.18ms\n",
      "iter 45900/45910/48828: loss 2.4182, time 110.18ms\n",
      "step 46000: train loss 2.4104, val loss 2.4103\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 46000/46010/48828: loss 2.4068, time 4000.15ms\n",
      "iter 46100/46110/48828: loss 2.3851, time 109.50ms\n",
      "iter 46200/46210/48828: loss 2.3877, time 110.93ms\n",
      "iter 46300/46310/48828: loss 2.3628, time 106.06ms\n",
      "iter 46400/46410/48828: loss 2.3789, time 110.38ms\n",
      "iter 46500/46510/48828: loss 2.3729, time 109.60ms\n",
      "iter 46600/46610/48828: loss 2.3386, time 112.09ms\n",
      "iter 46700/46710/48828: loss 2.3829, time 108.35ms\n",
      "iter 46800/46810/48828: loss 2.3565, time 101.18ms\n",
      "iter 46900/46910/48828: loss 2.3579, time 102.89ms\n",
      "step 47000: train loss 2.3305, val loss 2.3305\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 47000/47010/48828: loss 2.3383, time 1872.78ms\n",
      "iter 47100/47110/48828: loss 2.3206, time 131.35ms\n",
      "iter 47200/47210/48828: loss 2.3106, time 111.39ms\n",
      "iter 47300/47310/48828: loss 2.3072, time 107.65ms\n",
      "iter 47400/47410/48828: loss 2.3123, time 100.35ms\n",
      "iter 47500/47510/48828: loss 2.3028, time 108.11ms\n",
      "iter 47600/47610/48828: loss 2.3038, time 106.42ms\n",
      "iter 47700/47710/48828: loss 2.3005, time 113.95ms\n",
      "iter 47800/47810/48828: loss 2.3017, time 110.53ms\n",
      "iter 47900/47910/48828: loss 2.3028, time 108.42ms\n",
      "step 48000: train loss 2.2881, val loss 2.2877\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 48000/48010/48828: loss 2.2830, time 2691.48ms\n",
      "iter 48100/48110/48828: loss 2.2923, time 98.59ms\n",
      "iter 48200/48210/48828: loss 2.2920, time 97.45ms\n",
      "iter 48300/48310/48828: loss 2.2915, time 119.85ms\n",
      "iter 48400/48410/48828: loss 2.2977, time 109.14ms\n",
      "iter 48500/48510/48828: loss 2.2913, time 99.82ms\n",
      "iter 48600/48610/48828: loss 2.2691, time 106.07ms\n",
      "iter 48700/48710/48828: loss 2.2876, time 104.73ms\n",
      "iter 48800/48810/48828: loss 2.2768, time 153.61ms\n",
      "\n",
      "\n",
      "## Running generation 3 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [15:32<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=52.01% win[2]=47.96%, n=10000, draw=0.03%\n",
      "Game Length min: 7, max: 42, mean: 14.15\n",
      "Winner Stats by initial move:\n",
      "  a=1: n= 94 win[1]=43.62% counts=Counter({2: 53, 1: 41}), win[2]=56.38% draw=0.00%\n",
      "  a=2: n=109 win[1]=32.11% counts=Counter({2: 74, 1: 35}), win[2]=67.89% draw=0.00%\n",
      "  a=3: n=103 win[1]=22.33% counts=Counter({2: 80, 1: 23}), win[2]=77.67% draw=0.00%\n",
      "  a=4: n=9218 win[1]=53.38% counts=Counter({1: 4921, 2: 4295, None: 2}), win[2]=46.59% draw=0.02%\n",
      "  a=5: n=132 win[1]=50.00% counts=Counter({1: 66, 2: 65, None: 1}), win[2]=49.24% draw=0.76%\n",
      "  a=6: n=148 win[1]=34.46% counts=Counter({2: 97, 1: 51}), win[2]=65.54% draw=0.00%\n",
      "  a=7: n=196 win[1]=32.65% counts=Counter({2: 132, 1: 64}), win[2]=67.35% draw=0.00%\n",
      "Training model on gen-3\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.4029, val loss 2.4029\n",
      "iter 0/15/48828: loss 2.3943, time 6364.63ms\n",
      "iter 100/105/48828: loss 2.5193, time 113.92ms\n",
      "iter 200/210/48828: loss 2.5428, time 111.86ms\n",
      "iter 300/315/48828: loss 2.5474, time 119.98ms\n",
      "iter 400/405/48828: loss 2.5639, time 903.50ms\n",
      "iter 500/510/48828: loss 2.5833, time 109.73ms\n",
      "iter 600/615/48828: loss 2.5483, time 108.54ms\n",
      "iter 700/705/48828: loss 2.5488, time 366.78ms\n",
      "iter 800/810/48828: loss 2.5373, time 125.00ms\n",
      "iter 900/915/48828: loss 2.5329, time 124.03ms\n",
      "step 1000: train loss 2.5357, val loss 2.5359\n",
      "iter 1000/1005/48828: loss 2.5316, time 6153.87ms\n",
      "iter 1100/1110/48828: loss 2.5396, time 112.78ms\n",
      "iter 1200/1215/48828: loss 2.5736, time 107.00ms\n",
      "iter 1300/1305/48828: loss 2.5309, time 108.89ms\n",
      "iter 1400/1410/48828: loss 2.5558, time 411.58ms\n",
      "iter 1500/1515/48828: loss 2.5598, time 177.75ms\n",
      "iter 1600/1605/48828: loss 2.5486, time 111.42ms\n",
      "iter 1700/1710/48828: loss 2.5394, time 105.16ms\n",
      "iter 1800/1815/48828: loss 2.5406, time 127.36ms\n",
      "iter 1900/1905/48828: loss 2.5685, time 109.92ms\n",
      "step 2000: train loss 2.5544, val loss 2.5542\n",
      "iter 2000/2010/48828: loss 2.5545, time 5094.91ms\n",
      "iter 2100/2115/48828: loss 2.5762, time 107.27ms\n",
      "iter 2200/2205/48828: loss 2.5587, time 105.90ms\n",
      "iter 2300/2310/48828: loss 2.5660, time 111.41ms\n",
      "iter 2400/2415/48828: loss 2.5312, time 107.00ms\n",
      "iter 2500/2505/48828: loss 2.5312, time 105.06ms\n",
      "iter 2600/2610/48828: loss 2.5335, time 129.24ms\n",
      "iter 2700/2715/48828: loss 2.5231, time 105.93ms\n",
      "iter 2800/2805/48828: loss 2.5241, time 300.29ms\n",
      "iter 2900/2910/48828: loss 2.5536, time 105.11ms\n",
      "step 3000: train loss 2.5499, val loss 2.5501\n",
      "iter 3000/3015/48828: loss 2.5522, time 4293.04ms\n",
      "iter 3100/3105/48828: loss 2.5711, time 119.49ms\n",
      "iter 3200/3210/48828: loss 2.5418, time 120.01ms\n",
      "iter 3300/3315/48828: loss 2.5373, time 107.08ms\n",
      "iter 3400/3405/48828: loss 2.5320, time 110.23ms\n",
      "iter 3500/3510/48828: loss 2.5446, time 104.21ms\n",
      "iter 3600/3615/48828: loss 2.5344, time 108.18ms\n",
      "iter 3700/3705/48828: loss 2.5113, time 104.44ms\n",
      "iter 3800/3810/48828: loss 2.5319, time 119.65ms\n",
      "iter 3900/3915/48828: loss 2.5343, time 296.88ms\n",
      "step 4000: train loss 2.5438, val loss 2.5439\n",
      "iter 4000/4005/48828: loss 2.5434, time 3721.38ms\n",
      "iter 4100/4110/48828: loss 2.5437, time 179.05ms\n",
      "iter 4200/4215/48828: loss 2.5601, time 98.46ms\n",
      "iter 4300/4305/48828: loss 2.5469, time 106.02ms\n",
      "iter 4400/4410/48828: loss 2.5460, time 113.81ms\n",
      "iter 4500/4515/48828: loss 2.5589, time 110.94ms\n",
      "iter 4600/4605/48828: loss 2.5171, time 112.91ms\n",
      "iter 4700/4710/48828: loss 2.5386, time 105.13ms\n",
      "iter 4800/4815/48828: loss 2.5352, time 104.64ms\n",
      "iter 4900/4905/48828: loss 2.5259, time 285.70ms\n",
      "step 5000: train loss 2.5595, val loss 2.5596\n",
      "iter 5000/5010/48828: loss 2.5593, time 3002.09ms\n",
      "iter 5100/5115/48828: loss 2.5412, time 109.91ms\n",
      "iter 5200/5205/48828: loss 2.5523, time 1090.04ms\n",
      "iter 5300/5310/48828: loss 2.5496, time 112.13ms\n",
      "iter 5400/5415/48828: loss 2.5289, time 129.28ms\n",
      "iter 5500/5505/48828: loss 2.5435, time 121.02ms\n",
      "iter 5600/5610/48828: loss 2.5462, time 120.32ms\n",
      "iter 5700/5715/48828: loss 2.5475, time 107.48ms\n",
      "iter 5800/5805/48828: loss 2.5499, time 359.21ms\n",
      "iter 5900/5910/48828: loss 2.5291, time 104.23ms\n",
      "step 6000: train loss 2.5495, val loss 2.5494\n",
      "iter 6000/6015/48828: loss 2.5515, time 5226.18ms\n",
      "iter 6100/6105/48828: loss 2.5270, time 108.02ms\n",
      "iter 6200/6210/48828: loss 2.5530, time 116.97ms\n",
      "iter 6300/6315/48828: loss 2.5513, time 112.69ms\n",
      "iter 6400/6405/48828: loss 2.6109, time 139.44ms\n",
      "iter 6500/6510/48828: loss 2.5669, time 104.71ms\n",
      "iter 6600/6615/48828: loss 2.5509, time 106.04ms\n",
      "iter 6700/6705/48828: loss 2.5528, time 109.14ms\n",
      "iter 6800/6810/48828: loss 2.5546, time 109.64ms\n",
      "iter 6900/6915/48828: loss 2.5538, time 125.20ms\n",
      "step 7000: train loss 2.5275, val loss 2.5274\n",
      "iter 7000/7005/48828: loss 2.5249, time 3898.64ms\n",
      "iter 7100/7110/48828: loss 2.5261, time 105.53ms\n",
      "iter 7200/7215/48828: loss 2.5531, time 99.75ms\n",
      "iter 7300/7305/48828: loss 2.5204, time 103.97ms\n",
      "iter 7400/7410/48828: loss 2.5290, time 113.25ms\n",
      "iter 7500/7515/48828: loss 2.5297, time 106.64ms\n",
      "iter 7600/7605/48828: loss 2.5329, time 104.92ms\n",
      "iter 7700/7710/48828: loss 2.5464, time 110.52ms\n",
      "iter 7800/7815/48828: loss 2.5450, time 99.97ms\n",
      "iter 7900/7905/48828: loss 2.5441, time 112.70ms\n",
      "step 8000: train loss 2.5520, val loss 2.5521\n",
      "iter 8000/8010/48828: loss 2.5552, time 3796.24ms\n",
      "iter 8100/8115/48828: loss 2.5433, time 238.67ms\n",
      "iter 8200/8205/48828: loss 2.5533, time 111.64ms\n",
      "iter 8300/8310/48828: loss 2.5503, time 132.28ms\n",
      "iter 8400/8415/48828: loss 2.5384, time 102.26ms\n",
      "iter 8500/8505/48828: loss 2.5464, time 110.18ms\n",
      "iter 8600/8610/48828: loss 2.5523, time 107.54ms\n",
      "iter 8700/8715/48828: loss 2.5441, time 105.17ms\n",
      "iter 8800/8805/48828: loss 2.5535, time 104.98ms\n",
      "iter 8900/8910/48828: loss 2.5384, time 103.54ms\n",
      "step 9000: train loss 2.5402, val loss 2.5403\n",
      "iter 9000/9015/48828: loss 2.5389, time 5029.92ms\n",
      "iter 9100/9105/48828: loss 2.5422, time 104.14ms\n",
      "iter 9200/9210/48828: loss 2.5447, time 106.37ms\n",
      "iter 9300/9315/48828: loss 2.5496, time 105.47ms\n",
      "iter 9400/9405/48828: loss 2.5546, time 114.17ms\n",
      "iter 9500/9510/48828: loss 2.5610, time 108.26ms\n",
      "iter 9600/9615/48828: loss 2.5533, time 99.98ms\n",
      "iter 9700/9705/48828: loss 2.5398, time 109.46ms\n",
      "iter 9800/9810/48828: loss 2.5369, time 113.77ms\n",
      "iter 9900/9915/48828: loss 2.5686, time 113.70ms\n",
      "step 10000: train loss 2.5714, val loss 2.5716\n",
      "iter 10000/10005/48828: loss 2.5742, time 2697.48ms\n",
      "iter 10100/10110/48828: loss 2.5500, time 107.15ms\n",
      "iter 10200/10215/48828: loss 2.5467, time 107.87ms\n",
      "iter 10300/10305/48828: loss 2.5528, time 124.99ms\n",
      "iter 10400/10410/48828: loss 2.5412, time 108.72ms\n",
      "iter 10500/10515/48828: loss 2.5374, time 105.23ms\n",
      "iter 10600/10605/48828: loss 2.5289, time 121.12ms\n",
      "iter 10700/10710/48828: loss 2.5391, time 108.42ms\n",
      "iter 10800/10815/48828: loss 2.5357, time 108.17ms\n",
      "iter 10900/10905/48828: loss 2.5509, time 110.91ms\n",
      "step 11000: train loss 2.5489, val loss 2.5489\n",
      "iter 11000/11010/48828: loss 2.5451, time 5191.85ms\n",
      "iter 11100/11115/48828: loss 2.5348, time 110.64ms\n",
      "iter 11200/11205/48828: loss 2.5331, time 104.03ms\n",
      "iter 11300/11310/48828: loss 2.5144, time 103.86ms\n",
      "iter 11400/11415/48828: loss 2.5482, time 130.14ms\n",
      "iter 11500/11505/48828: loss 2.5474, time 104.67ms\n",
      "iter 11600/11610/48828: loss 2.5319, time 104.14ms\n",
      "iter 11700/11715/48828: loss 2.5935, time 113.49ms\n",
      "iter 11800/11805/48828: loss 2.5611, time 112.95ms\n",
      "iter 11900/11910/48828: loss 2.5508, time 105.30ms\n",
      "step 12000: train loss 2.5440, val loss 2.5439\n",
      "iter 12000/12015/48828: loss 2.5446, time 4501.53ms\n",
      "iter 12100/12105/48828: loss 2.5287, time 304.04ms\n",
      "iter 12200/12210/48828: loss 2.5587, time 131.74ms\n",
      "iter 12300/12315/48828: loss 2.5410, time 330.03ms\n",
      "iter 12400/12405/48828: loss 2.5261, time 116.68ms\n",
      "iter 12500/12510/48828: loss 2.5507, time 114.39ms\n",
      "iter 12600/12615/48828: loss 2.5616, time 113.73ms\n",
      "iter 12700/12705/48828: loss 2.5259, time 105.74ms\n",
      "iter 12800/12810/48828: loss 2.5393, time 114.16ms\n",
      "iter 12900/12915/48828: loss 2.5572, time 114.51ms\n",
      "step 13000: train loss 2.5302, val loss 2.5301\n",
      "iter 13000/13005/48828: loss 2.5284, time 4605.91ms\n",
      "iter 13100/13110/48828: loss 2.5309, time 114.54ms\n",
      "iter 13200/13215/48828: loss 2.5347, time 129.05ms\n",
      "iter 13300/13305/48828: loss 2.5359, time 107.03ms\n",
      "iter 13400/13410/48828: loss 2.5482, time 111.38ms\n",
      "iter 13500/13515/48828: loss 2.5223, time 99.67ms\n",
      "iter 13600/13605/48828: loss 2.5410, time 109.97ms\n",
      "iter 13700/13710/48828: loss 2.5409, time 106.55ms\n",
      "iter 13800/13815/48828: loss 2.5368, time 101.82ms\n",
      "iter 13900/13905/48828: loss 2.5418, time 104.88ms\n",
      "step 14000: train loss 2.5402, val loss 2.5402\n",
      "iter 14000/14010/48828: loss 2.5391, time 3934.36ms\n",
      "iter 14100/14115/48828: loss 2.5463, time 121.26ms\n",
      "iter 14200/14205/48828: loss 2.5525, time 392.93ms\n",
      "iter 14300/14310/48828: loss 2.5337, time 108.05ms\n",
      "iter 14400/14415/48828: loss 2.5393, time 107.90ms\n",
      "iter 14500/14505/48828: loss 2.5570, time 117.46ms\n",
      "iter 14600/14610/48828: loss 2.5718, time 108.18ms\n",
      "iter 14700/14715/48828: loss 2.5574, time 101.73ms\n",
      "iter 14800/14805/48828: loss 2.5736, time 111.86ms\n",
      "iter 14900/14910/48828: loss 2.5374, time 111.48ms\n",
      "step 15000: train loss 2.5434, val loss 2.5433\n",
      "iter 15000/15015/48828: loss 2.5453, time 3537.31ms\n",
      "iter 15100/15105/48828: loss 2.5556, time 112.58ms\n",
      "iter 15200/15210/48828: loss 2.5460, time 104.71ms\n",
      "iter 15300/15315/48828: loss 2.5529, time 107.93ms\n",
      "iter 15400/15405/48828: loss 2.5512, time 110.64ms\n",
      "iter 15500/15510/48828: loss 2.5314, time 111.61ms\n",
      "iter 15600/15615/48828: loss 2.5540, time 101.86ms\n",
      "iter 15700/15705/48828: loss 2.5397, time 110.22ms\n",
      "iter 15800/15810/48828: loss 2.5525, time 114.74ms\n",
      "iter 15900/15915/48828: loss 2.5476, time 103.57ms\n",
      "step 16000: train loss 2.5414, val loss 2.5414\n",
      "iter 16000/16005/48828: loss 2.5473, time 2574.37ms\n",
      "iter 16100/16110/48828: loss 2.5400, time 112.03ms\n",
      "iter 16200/16215/48828: loss 2.5207, time 114.33ms\n",
      "iter 16300/16305/48828: loss 2.5686, time 359.90ms\n",
      "iter 16400/16410/48828: loss 2.5340, time 119.90ms\n",
      "iter 16500/16515/48828: loss 2.5408, time 120.25ms\n",
      "iter 16600/16605/48828: loss 2.5332, time 104.34ms\n",
      "iter 16700/16710/48828: loss 2.5403, time 106.98ms\n",
      "iter 16800/16815/48828: loss 2.5057, time 174.23ms\n",
      "iter 16900/16905/48828: loss 2.5060, time 120.06ms\n",
      "step 17000: train loss 2.5363, val loss 2.5361\n",
      "iter 17000/17010/48828: loss 2.5339, time 2857.80ms\n",
      "iter 17100/17115/48828: loss 2.5351, time 112.68ms\n",
      "iter 17200/17205/48828: loss 2.5338, time 114.27ms\n",
      "iter 17300/17310/48828: loss 2.5451, time 112.93ms\n",
      "iter 17400/17415/48828: loss 2.5378, time 100.08ms\n",
      "iter 17500/17505/48828: loss 2.5401, time 178.93ms\n",
      "iter 17600/17610/48828: loss 2.5294, time 113.15ms\n",
      "iter 17700/17715/48828: loss 2.5452, time 105.67ms\n",
      "iter 17800/17805/48828: loss 2.5484, time 111.11ms\n",
      "iter 17900/17910/48828: loss 2.5325, time 110.04ms\n",
      "step 18000: train loss 2.5531, val loss 2.5530\n",
      "iter 18000/18015/48828: loss 2.5530, time 4204.20ms\n",
      "iter 18100/18105/48828: loss 2.5337, time 105.60ms\n",
      "iter 18200/18210/48828: loss 2.5374, time 105.59ms\n",
      "iter 18300/18315/48828: loss 2.5330, time 98.67ms\n",
      "iter 18400/18405/48828: loss 2.5188, time 578.17ms\n",
      "iter 18500/18510/48828: loss 2.5307, time 114.83ms\n",
      "iter 18600/18615/48828: loss 2.5323, time 119.08ms\n",
      "iter 18700/18705/48828: loss 2.5608, time 114.09ms\n",
      "iter 18800/18810/48828: loss 2.5582, time 112.82ms\n",
      "iter 18900/18915/48828: loss 2.5252, time 107.77ms\n",
      "step 19000: train loss 2.5406, val loss 2.5405\n",
      "iter 19000/19005/48828: loss 2.5446, time 2679.70ms\n",
      "iter 19100/19110/48828: loss 2.5701, time 111.32ms\n",
      "iter 19200/19215/48828: loss 2.5351, time 109.63ms\n",
      "iter 19300/19305/48828: loss 2.5465, time 133.28ms\n",
      "iter 19400/19410/48828: loss 2.5410, time 117.23ms\n",
      "iter 19500/19515/48828: loss 2.5505, time 102.46ms\n",
      "iter 19600/19605/48828: loss 2.5389, time 111.70ms\n",
      "iter 19700/19710/48828: loss 2.5506, time 114.32ms\n",
      "iter 19800/19815/48828: loss 2.5268, time 124.38ms\n",
      "iter 19900/19905/48828: loss 2.5269, time 115.26ms\n",
      "step 20000: train loss 2.5669, val loss 2.5669\n",
      "iter 20000/20010/48828: loss 2.5668, time 5552.52ms\n",
      "iter 20100/20115/48828: loss 2.5396, time 105.76ms\n",
      "iter 20200/20205/48828: loss 2.5432, time 105.46ms\n",
      "iter 20300/20310/48828: loss 2.5333, time 110.89ms\n",
      "iter 20400/20415/48828: loss 2.5323, time 111.37ms\n",
      "iter 20500/20505/48828: loss 2.5053, time 331.36ms\n",
      "iter 20600/20610/48828: loss 2.5133, time 113.52ms\n",
      "iter 20700/20715/48828: loss 2.5517, time 108.86ms\n",
      "iter 20800/20805/48828: loss 2.5511, time 105.68ms\n",
      "iter 20900/20910/48828: loss 2.5249, time 110.36ms\n",
      "step 21000: train loss 2.5499, val loss 2.5498\n",
      "iter 21000/21015/48828: loss 2.5460, time 2818.42ms\n",
      "iter 21100/21105/48828: loss 2.5328, time 110.69ms\n",
      "iter 21200/21210/48828: loss 2.5262, time 110.12ms\n",
      "iter 21300/21315/48828: loss 2.5639, time 99.53ms\n",
      "iter 21400/21405/48828: loss 2.5445, time 113.04ms\n",
      "iter 21500/21510/48828: loss 2.5497, time 107.08ms\n",
      "iter 21600/21615/48828: loss 2.5300, time 122.82ms\n",
      "iter 21700/21705/48828: loss 2.5381, time 122.32ms\n",
      "iter 21800/21810/48828: loss 2.5334, time 111.72ms\n",
      "iter 21900/21915/48828: loss 2.5601, time 159.58ms\n",
      "step 22000: train loss 2.5180, val loss 2.5178\n",
      "iter 22000/22005/48828: loss 2.5138, time 2448.33ms\n",
      "iter 22100/22110/48828: loss 2.5444, time 110.35ms\n",
      "iter 22200/22215/48828: loss 2.6561, time 429.43ms\n",
      "iter 22300/22305/48828: loss 2.5501, time 112.27ms\n",
      "iter 22400/22410/48828: loss 2.5260, time 129.07ms\n",
      "iter 22500/22515/48828: loss 2.5295, time 108.50ms\n",
      "iter 22600/22605/48828: loss 2.5376, time 302.82ms\n",
      "iter 22700/22710/48828: loss 2.5472, time 119.63ms\n",
      "iter 22800/22815/48828: loss 2.5468, time 124.19ms\n",
      "iter 22900/22905/48828: loss 2.5326, time 105.61ms\n",
      "step 23000: train loss 2.5618, val loss 2.5618\n",
      "iter 23000/23010/48828: loss 2.5610, time 2658.82ms\n",
      "iter 23100/23115/48828: loss 2.5409, time 101.05ms\n",
      "iter 23200/23205/48828: loss 2.5450, time 123.14ms\n",
      "iter 23300/23310/48828: loss 2.5470, time 107.22ms\n",
      "iter 23400/23415/48828: loss 2.5145, time 94.27ms\n",
      "iter 23500/23505/48828: loss 2.5196, time 111.74ms\n",
      "iter 23600/23610/48828: loss 2.5242, time 109.15ms\n",
      "iter 23700/23715/48828: loss 2.5225, time 101.90ms\n",
      "iter 23800/23805/48828: loss 2.5479, time 112.02ms\n",
      "iter 23900/23910/48828: loss 2.5188, time 104.11ms\n",
      "step 24000: train loss 2.5518, val loss 2.5517\n",
      "iter 24000/24015/48828: loss 2.5568, time 6188.14ms\n",
      "iter 24100/24105/48828: loss 2.5119, time 113.35ms\n",
      "iter 24200/24210/48828: loss 2.5475, time 278.38ms\n",
      "iter 24300/24315/48828: loss 2.5604, time 196.66ms\n",
      "iter 24400/24405/48828: loss 2.5407, time 120.44ms\n",
      "iter 24500/24510/48828: loss 2.5374, time 109.37ms\n",
      "iter 24600/24615/48828: loss 2.5565, time 101.91ms\n",
      "iter 24700/24705/48828: loss 2.5481, time 302.11ms\n",
      "iter 24800/24810/48828: loss 2.5435, time 113.94ms\n",
      "iter 24900/24915/48828: loss 2.5484, time 99.29ms\n",
      "step 25000: train loss 2.5365, val loss 2.5366\n",
      "iter 25000/25005/48828: loss 2.5356, time 2683.03ms\n",
      "iter 25100/25110/48828: loss 2.5458, time 103.97ms\n",
      "iter 25200/25215/48828: loss 2.5446, time 118.55ms\n",
      "iter 25300/25305/48828: loss 2.5474, time 105.34ms\n",
      "iter 25400/25410/48828: loss 2.5603, time 105.50ms\n",
      "iter 25500/25515/48828: loss 2.5336, time 100.87ms\n",
      "iter 25600/25605/48828: loss 2.5167, time 106.67ms\n",
      "iter 25700/25710/48828: loss 2.5317, time 114.33ms\n",
      "iter 25800/25815/48828: loss 2.5193, time 108.59ms\n",
      "iter 25900/25905/48828: loss 2.5408, time 105.87ms\n",
      "step 26000: train loss 2.5330, val loss 2.5327\n",
      "iter 26000/26010/48828: loss 2.5311, time 2894.23ms\n",
      "iter 26100/26115/48828: loss 2.5711, time 110.25ms\n",
      "iter 26200/26205/48828: loss 2.5200, time 121.05ms\n",
      "iter 26300/26310/48828: loss 2.5251, time 113.96ms\n",
      "iter 26400/26415/48828: loss 2.5187, time 110.17ms\n",
      "iter 26500/26505/48828: loss 2.5271, time 137.60ms\n",
      "iter 26600/26610/48828: loss 2.5220, time 109.30ms\n",
      "iter 26700/26715/48828: loss 2.5453, time 111.35ms\n",
      "iter 26800/26805/48828: loss 2.5595, time 366.02ms\n",
      "iter 26900/26910/48828: loss 2.5298, time 115.49ms\n",
      "step 27000: train loss 2.5463, val loss 2.5462\n",
      "iter 27000/27015/48828: loss 2.5543, time 2949.78ms\n",
      "iter 27100/27105/48828: loss 2.5223, time 111.23ms\n",
      "iter 27200/27210/48828: loss 2.5360, time 111.61ms\n",
      "iter 27300/27315/48828: loss 2.5181, time 109.13ms\n",
      "iter 27400/27405/48828: loss 2.5238, time 102.86ms\n",
      "iter 27500/27510/48828: loss 2.5029, time 109.24ms\n",
      "iter 27600/27615/48828: loss 2.5234, time 109.11ms\n",
      "iter 27700/27705/48828: loss 2.5213, time 109.09ms\n",
      "iter 27800/27810/48828: loss 2.5022, time 105.76ms\n",
      "iter 27900/27915/48828: loss 2.4912, time 106.57ms\n",
      "step 28000: train loss 2.5328, val loss 2.5327\n",
      "iter 28000/28005/48828: loss 2.5385, time 2925.37ms\n",
      "iter 28100/28110/48828: loss 2.5189, time 109.99ms\n",
      "iter 28200/28215/48828: loss 2.5161, time 100.35ms\n",
      "iter 28300/28305/48828: loss 2.5325, time 105.54ms\n",
      "iter 28400/28410/48828: loss 2.5137, time 109.37ms\n",
      "iter 28500/28515/48828: loss 2.5286, time 113.14ms\n",
      "iter 28600/28605/48828: loss 2.5045, time 113.77ms\n",
      "iter 28700/28710/48828: loss 2.5422, time 114.53ms\n",
      "iter 28800/28815/48828: loss 2.5048, time 105.04ms\n",
      "iter 28900/28905/48828: loss 2.5328, time 529.91ms\n",
      "step 29000: train loss 2.5198, val loss 2.5197\n",
      "iter 29000/29010/48828: loss 2.5223, time 3073.82ms\n",
      "iter 29100/29115/48828: loss 2.5155, time 107.73ms\n",
      "iter 29200/29205/48828: loss 2.5228, time 106.37ms\n",
      "iter 29300/29310/48828: loss 2.5012, time 104.89ms\n",
      "iter 29400/29415/48828: loss 2.5326, time 109.78ms\n",
      "iter 29500/29505/48828: loss 2.5160, time 106.03ms\n",
      "iter 29600/29610/48828: loss 2.5347, time 112.10ms\n",
      "iter 29700/29715/48828: loss 2.5199, time 106.72ms\n",
      "iter 29800/29805/48828: loss 2.5289, time 112.39ms\n",
      "iter 29900/29910/48828: loss 2.5192, time 111.75ms\n",
      "step 30000: train loss 2.5296, val loss 2.5295\n",
      "iter 30000/30015/48828: loss 2.5286, time 2929.68ms\n",
      "iter 30100/30105/48828: loss 2.5388, time 103.08ms\n",
      "iter 30200/30210/48828: loss 2.5313, time 112.54ms\n",
      "iter 30300/30315/48828: loss 2.5169, time 117.35ms\n",
      "iter 30400/30405/48828: loss 2.5383, time 116.60ms\n",
      "iter 30500/30510/48828: loss 2.5414, time 113.61ms\n",
      "iter 30600/30615/48828: loss 2.5146, time 108.41ms\n",
      "iter 30700/30705/48828: loss 2.5184, time 108.21ms\n",
      "iter 30800/30810/48828: loss 2.5016, time 105.72ms\n",
      "iter 30900/30915/48828: loss 2.4879, time 110.97ms\n",
      "step 31000: train loss 2.5099, val loss 2.5099\n",
      "iter 31000/31005/48828: loss 2.5121, time 4329.18ms\n",
      "iter 31100/31110/48828: loss 2.5183, time 111.68ms\n",
      "iter 31200/31215/48828: loss 2.5040, time 444.86ms\n",
      "iter 31300/31305/48828: loss 2.5214, time 118.65ms\n",
      "iter 31400/31410/48828: loss 2.5265, time 106.57ms\n",
      "iter 31500/31515/48828: loss 2.5217, time 99.43ms\n",
      "iter 31600/31605/48828: loss 2.5108, time 113.11ms\n",
      "iter 31700/31710/48828: loss 2.5043, time 113.61ms\n",
      "iter 31800/31815/48828: loss 2.5074, time 107.63ms\n",
      "iter 31900/31905/48828: loss 2.5013, time 114.17ms\n",
      "step 32000: train loss 2.5049, val loss 2.5048\n",
      "iter 32000/32010/48828: loss 2.5039, time 4651.37ms\n",
      "iter 32100/32115/48828: loss 2.5027, time 104.25ms\n",
      "iter 32200/32205/48828: loss 2.4979, time 113.30ms\n",
      "iter 32300/32310/48828: loss 2.4862, time 117.31ms\n",
      "iter 32400/32415/48828: loss 2.4923, time 100.68ms\n",
      "iter 32500/32505/48828: loss 2.5370, time 105.50ms\n",
      "iter 32600/32610/48828: loss 2.5247, time 113.76ms\n",
      "iter 32700/32715/48828: loss 2.4739, time 109.08ms\n",
      "iter 32800/32805/48828: loss 2.5279, time 106.66ms\n",
      "iter 32900/32910/48828: loss 2.4746, time 104.73ms\n",
      "step 33000: train loss 2.4938, val loss 2.4939\n",
      "iter 33000/33015/48828: loss 2.5026, time 2580.89ms\n",
      "iter 33100/33105/48828: loss 2.4907, time 114.41ms\n",
      "iter 33200/33210/48828: loss 2.4959, time 108.08ms\n",
      "iter 33300/33315/48828: loss 2.4812, time 110.61ms\n",
      "iter 33400/33405/48828: loss 2.4855, time 112.30ms\n",
      "iter 33500/33510/48828: loss 2.5173, time 105.89ms\n",
      "iter 33600/33615/48828: loss 2.5131, time 108.47ms\n",
      "iter 33700/33705/48828: loss 2.4816, time 104.94ms\n",
      "iter 33800/33810/48828: loss 2.4734, time 111.85ms\n",
      "iter 33900/33915/48828: loss 2.4827, time 109.17ms\n",
      "step 34000: train loss 2.4548, val loss 2.4544\n",
      "iter 34000/34005/48828: loss 2.4575, time 2592.73ms\n",
      "iter 34100/34110/48828: loss 2.4594, time 107.19ms\n",
      "iter 34200/34215/48828: loss 2.4875, time 110.99ms\n",
      "iter 34300/34305/48828: loss 2.5014, time 103.86ms\n",
      "iter 34400/34410/48828: loss 2.5042, time 111.93ms\n",
      "iter 34500/34515/48828: loss 2.5025, time 110.75ms\n",
      "iter 34600/34605/48828: loss 2.5030, time 116.10ms\n",
      "iter 34700/34710/48828: loss 2.4848, time 120.72ms\n",
      "iter 34800/34815/48828: loss 2.4607, time 108.72ms\n",
      "iter 34900/34905/48828: loss 2.5005, time 105.44ms\n",
      "step 35000: train loss 2.4844, val loss 2.4842\n",
      "iter 35000/35010/48828: loss 2.4852, time 5857.95ms\n",
      "iter 35100/35115/48828: loss 2.4783, time 111.33ms\n",
      "iter 35200/35205/48828: loss 2.4782, time 105.17ms\n",
      "iter 35300/35310/48828: loss 2.4727, time 105.99ms\n",
      "iter 35400/35415/48828: loss 2.4811, time 106.05ms\n",
      "iter 35500/35505/48828: loss 2.4758, time 108.07ms\n",
      "iter 35600/35610/48828: loss 2.4636, time 111.93ms\n",
      "iter 35700/35715/48828: loss 2.4672, time 105.42ms\n",
      "iter 35800/35805/48828: loss 2.4595, time 115.45ms\n",
      "iter 35900/35910/48828: loss 2.4838, time 109.98ms\n",
      "step 36000: train loss 2.5083, val loss 2.5084\n",
      "iter 36000/36015/48828: loss 2.5050, time 2830.65ms\n",
      "iter 36100/36105/48828: loss 2.4877, time 332.95ms\n",
      "iter 36200/36210/48828: loss 2.4890, time 110.49ms\n",
      "iter 36300/36315/48828: loss 2.4723, time 111.31ms\n",
      "iter 36400/36405/48828: loss 2.4729, time 111.16ms\n",
      "iter 36500/36510/48828: loss 2.4580, time 107.84ms\n",
      "iter 36600/36615/48828: loss 2.5013, time 111.38ms\n",
      "iter 36700/36705/48828: loss 2.4989, time 104.56ms\n",
      "iter 36800/36810/48828: loss 2.4829, time 103.89ms\n",
      "iter 36900/36915/48828: loss 2.4755, time 97.48ms\n",
      "step 37000: train loss 2.4736, val loss 2.4736\n",
      "iter 37000/37005/48828: loss 2.4702, time 6734.06ms\n",
      "iter 37100/37110/48828: loss 2.4500, time 108.78ms\n",
      "iter 37200/37215/48828: loss 2.4868, time 106.64ms\n",
      "iter 37300/37305/48828: loss 2.4719, time 108.85ms\n",
      "iter 37400/37410/48828: loss 2.4599, time 109.40ms\n",
      "iter 37500/37515/48828: loss 2.4905, time 107.19ms\n",
      "iter 37600/37605/48828: loss 2.4592, time 105.95ms\n",
      "iter 37700/37710/48828: loss 2.4579, time 117.53ms\n",
      "iter 37800/37815/48828: loss 2.4382, time 108.44ms\n",
      "iter 37900/37905/48828: loss 2.4646, time 110.90ms\n",
      "step 38000: train loss 2.4655, val loss 2.4659\n",
      "iter 38000/38010/48828: loss 2.4692, time 2665.62ms\n",
      "iter 38100/38115/48828: loss 2.4632, time 100.61ms\n",
      "iter 38200/38205/48828: loss 2.4774, time 339.94ms\n",
      "iter 38300/38310/48828: loss 2.4589, time 117.51ms\n",
      "iter 38400/38415/48828: loss 2.4596, time 112.02ms\n",
      "iter 38500/38505/48828: loss 2.4753, time 110.94ms\n",
      "iter 38600/38610/48828: loss 2.4679, time 112.34ms\n",
      "iter 38700/38715/48828: loss 2.4374, time 101.72ms\n",
      "iter 38800/38805/48828: loss 2.4435, time 106.39ms\n",
      "iter 38900/38910/48828: loss 2.4520, time 105.02ms\n",
      "step 39000: train loss 2.4423, val loss 2.4424\n",
      "iter 39000/39015/48828: loss 2.4427, time 2530.02ms\n",
      "iter 39100/39105/48828: loss 2.4405, time 106.17ms\n",
      "iter 39200/39210/48828: loss 2.4653, time 113.05ms\n",
      "iter 39300/39315/48828: loss 2.4443, time 100.21ms\n",
      "iter 39400/39405/48828: loss 2.4481, time 112.30ms\n",
      "iter 39500/39510/48828: loss 2.4421, time 112.81ms\n",
      "iter 39600/39615/48828: loss 2.4222, time 110.16ms\n",
      "iter 39700/39705/48828: loss 2.4647, time 109.83ms\n",
      "iter 39800/39810/48828: loss 2.4551, time 103.74ms\n",
      "iter 39900/39915/48828: loss 2.4614, time 110.82ms\n",
      "step 40000: train loss 2.4556, val loss 2.4556\n",
      "iter 40000/40005/48828: loss 2.4536, time 2720.15ms\n",
      "iter 40100/40110/48828: loss 2.4685, time 105.88ms\n",
      "iter 40200/40215/48828: loss 2.4695, time 103.06ms\n",
      "iter 40300/40305/48828: loss 2.4939, time 305.49ms\n",
      "iter 40400/40410/48828: loss 2.4490, time 121.54ms\n",
      "iter 40500/40515/48828: loss 2.4811, time 111.00ms\n",
      "iter 40600/40605/48828: loss 2.4631, time 103.71ms\n",
      "iter 40700/40710/48828: loss 2.5161, time 106.91ms\n",
      "iter 40800/40815/48828: loss 2.4684, time 98.46ms\n",
      "iter 40900/40905/48828: loss 2.4589, time 104.16ms\n",
      "step 41000: train loss 2.4403, val loss 2.4404\n",
      "iter 41000/41010/48828: loss 2.4506, time 2828.52ms\n",
      "iter 41100/41115/48828: loss 2.4602, time 118.55ms\n",
      "iter 41200/41205/48828: loss 2.4464, time 105.95ms\n",
      "iter 41300/41310/48828: loss 2.4548, time 104.79ms\n",
      "iter 41400/41415/48828: loss 2.4287, time 99.38ms\n",
      "iter 41500/41505/48828: loss 2.4246, time 110.22ms\n",
      "iter 41600/41610/48828: loss 2.4372, time 111.40ms\n",
      "iter 41700/41715/48828: loss 2.4129, time 99.96ms\n",
      "iter 41800/41805/48828: loss 2.4306, time 124.31ms\n",
      "iter 41900/41910/48828: loss 2.4385, time 113.61ms\n",
      "step 42000: train loss 2.4342, val loss 2.4344\n",
      "iter 42000/42015/48828: loss 2.4357, time 2608.70ms\n",
      "iter 42100/42105/48828: loss 2.4593, time 129.89ms\n",
      "iter 42200/42210/48828: loss 2.4217, time 130.99ms\n",
      "iter 42300/42315/48828: loss 2.4312, time 107.56ms\n",
      "iter 42400/42405/48828: loss 2.4221, time 534.20ms\n",
      "iter 42500/42510/48828: loss 2.4472, time 106.32ms\n",
      "iter 42600/42615/48828: loss 2.4226, time 99.82ms\n",
      "iter 42700/42705/48828: loss 2.4210, time 110.25ms\n",
      "iter 42800/42810/48828: loss 2.4376, time 106.70ms\n",
      "iter 42900/42915/48828: loss 2.4180, time 99.57ms\n",
      "step 43000: train loss 2.4112, val loss 2.4112\n",
      "iter 43000/43005/48828: loss 2.4040, time 2834.57ms\n",
      "iter 43100/43110/48828: loss 2.4060, time 108.44ms\n",
      "iter 43200/43215/48828: loss 2.4175, time 102.26ms\n",
      "iter 43300/43305/48828: loss 2.4065, time 105.91ms\n",
      "iter 43400/43410/48828: loss 2.3942, time 105.14ms\n",
      "iter 43500/43515/48828: loss 2.4349, time 106.06ms\n",
      "iter 43600/43605/48828: loss 2.3930, time 117.66ms\n",
      "iter 43700/43710/48828: loss 2.4219, time 111.72ms\n",
      "iter 43800/43815/48828: loss 2.3914, time 106.57ms\n",
      "iter 43900/43905/48828: loss 2.3982, time 111.36ms\n",
      "step 44000: train loss 2.4058, val loss 2.4055\n",
      "iter 44000/44010/48828: loss 2.4146, time 3436.93ms\n",
      "iter 44100/44115/48828: loss 2.3910, time 134.86ms\n",
      "iter 44200/44205/48828: loss 2.3945, time 107.64ms\n",
      "iter 44300/44310/48828: loss 2.3729, time 104.85ms\n",
      "iter 44400/44415/48828: loss 2.3954, time 101.58ms\n",
      "iter 44500/44505/48828: loss 2.3930, time 722.34ms\n",
      "iter 44600/44610/48828: loss 2.3689, time 108.09ms\n",
      "iter 44700/44715/48828: loss 2.4136, time 399.41ms\n",
      "iter 44800/44805/48828: loss 2.3804, time 113.85ms\n",
      "iter 44900/44910/48828: loss 2.3825, time 118.28ms\n",
      "step 45000: train loss 2.3828, val loss 2.3826\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 45000/45015/48828: loss 2.3796, time 4461.52ms\n",
      "iter 45100/45105/48828: loss 2.3780, time 114.97ms\n",
      "iter 45200/45210/48828: loss 2.3672, time 113.07ms\n",
      "iter 45300/45315/48828: loss 2.3956, time 102.07ms\n",
      "iter 45400/45405/48828: loss 2.3837, time 112.11ms\n",
      "iter 45500/45510/48828: loss 2.3715, time 106.08ms\n",
      "iter 45600/45615/48828: loss 2.3693, time 106.26ms\n",
      "iter 45700/45705/48828: loss 2.3843, time 102.98ms\n",
      "iter 45800/45810/48828: loss 2.3583, time 285.85ms\n",
      "iter 45900/45915/48828: loss 2.3626, time 112.05ms\n",
      "step 46000: train loss 2.3646, val loss 2.3650\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 46000/46005/48828: loss 2.3636, time 2492.55ms\n",
      "iter 46100/46110/48828: loss 2.3547, time 108.27ms\n",
      "iter 46200/46215/48828: loss 2.3602, time 105.21ms\n",
      "iter 46300/46305/48828: loss 2.3615, time 113.69ms\n",
      "iter 46400/46410/48828: loss 2.3572, time 113.97ms\n",
      "iter 46500/46515/48828: loss 2.3510, time 101.98ms\n",
      "iter 46600/46605/48828: loss 2.3339, time 303.76ms\n",
      "iter 46700/46710/48828: loss 2.3581, time 106.53ms\n",
      "iter 46800/46815/48828: loss 2.3322, time 108.40ms\n",
      "iter 46900/46905/48828: loss 2.3451, time 103.21ms\n",
      "step 47000: train loss 2.3466, val loss 2.3469\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 47000/47010/48828: loss 2.3436, time 2863.94ms\n",
      "iter 47100/47115/48828: loss 2.3442, time 111.09ms\n",
      "iter 47200/47205/48828: loss 2.3644, time 105.95ms\n",
      "iter 47300/47310/48828: loss 2.3683, time 109.36ms\n",
      "iter 47400/47415/48828: loss 2.3341, time 101.04ms\n",
      "iter 47500/47505/48828: loss 2.3414, time 111.84ms\n",
      "iter 47600/47610/48828: loss 2.3385, time 108.29ms\n",
      "iter 47700/47715/48828: loss 2.3193, time 103.48ms\n",
      "iter 47800/47805/48828: loss 2.3344, time 116.85ms\n",
      "iter 47900/47910/48828: loss 2.3364, time 112.81ms\n",
      "step 48000: train loss 2.3098, val loss 2.3098\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 48000/48015/48828: loss 2.3106, time 2753.73ms\n",
      "iter 48100/48105/48828: loss 2.3198, time 108.82ms\n",
      "iter 48200/48210/48828: loss 2.2982, time 119.44ms\n",
      "iter 48300/48315/48828: loss 2.2803, time 101.45ms\n",
      "iter 48400/48405/48828: loss 2.2830, time 106.61ms\n",
      "iter 48500/48510/48828: loss 2.2747, time 112.23ms\n",
      "iter 48600/48615/48828: loss 2.2742, time 102.82ms\n",
      "iter 48700/48705/48828: loss 2.2946, time 305.72ms\n",
      "iter 48800/48810/48828: loss 2.2614, time 104.49ms\n",
      "\n",
      "\n",
      "## Running generation 4 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [16:21<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=45.70% win[2]=54.30%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 39, mean: 13.94\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=139 win[1]=38.85% counts=Counter({2: 85, 1: 54}), win[2]=61.15% draw=0.00%\n",
      "  a=2: n=7686 win[1]=43.98% counts=Counter({2: 4306, 1: 3380}), win[2]=56.02% draw=0.00%\n",
      "  a=3: n=349 win[1]=42.69% counts=Counter({2: 200, 1: 149}), win[2]=57.31% draw=0.00%\n",
      "  a=4: n=882 win[1]=64.06% counts=Counter({1: 565, 2: 317}), win[2]=35.94% draw=0.00%\n",
      "  a=5: n=319 win[1]=57.68% counts=Counter({1: 184, 2: 135}), win[2]=42.32% draw=0.00%\n",
      "  a=6: n= 94 win[1]=39.36% counts=Counter({2: 57, 1: 37}), win[2]=60.64% draw=0.00%\n",
      "  a=7: n=531 win[1]=37.85% counts=Counter({2: 330, 1: 201}), win[2]=62.15% draw=0.00%\n",
      "Training model on gen-4\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.3694, val loss 2.3695\n",
      "iter 0/20/48828: loss 2.3561, time 9337.90ms\n",
      "iter 100/120/48828: loss 2.5607, time 114.96ms\n",
      "iter 200/220/48828: loss 2.5474, time 500.65ms\n",
      "iter 300/320/48828: loss 2.5579, time 333.26ms\n",
      "iter 400/420/48828: loss 2.5669, time 109.71ms\n",
      "iter 500/520/48828: loss 2.5526, time 138.76ms\n",
      "iter 600/620/48828: loss 2.5686, time 163.07ms\n",
      "iter 700/720/48828: loss 2.5319, time 151.46ms\n",
      "iter 800/820/48828: loss 2.5639, time 127.78ms\n",
      "iter 900/920/48828: loss 2.5409, time 125.10ms\n",
      "step 1000: train loss 2.5439, val loss 2.5438\n",
      "iter 1000/1020/48828: loss 2.5425, time 5578.50ms\n",
      "iter 1100/1120/48828: loss 2.5510, time 304.81ms\n",
      "iter 1200/1220/48828: loss 2.5720, time 249.14ms\n",
      "iter 1300/1320/48828: loss 2.5518, time 104.94ms\n",
      "iter 1400/1420/48828: loss 2.5974, time 163.49ms\n",
      "iter 1500/1520/48828: loss 2.5804, time 106.93ms\n",
      "iter 1600/1620/48828: loss 2.5788, time 109.91ms\n",
      "iter 1700/1720/48828: loss 2.5450, time 113.51ms\n",
      "iter 1800/1820/48828: loss 2.5534, time 404.16ms\n",
      "iter 1900/1920/48828: loss 2.5673, time 125.49ms\n",
      "step 2000: train loss 2.5446, val loss 2.5446\n",
      "iter 2000/2020/48828: loss 2.5447, time 6587.13ms\n",
      "iter 2100/2120/48828: loss 2.5443, time 126.41ms\n",
      "iter 2200/2220/48828: loss 2.5815, time 165.37ms\n",
      "iter 2300/2320/48828: loss 2.5554, time 106.72ms\n",
      "iter 2400/2420/48828: loss 2.5440, time 138.10ms\n",
      "iter 2500/2520/48828: loss 2.5596, time 236.23ms\n",
      "iter 2600/2620/48828: loss 2.5614, time 172.25ms\n",
      "iter 2700/2720/48828: loss 2.5644, time 100.03ms\n",
      "iter 2800/2820/48828: loss 2.5712, time 252.57ms\n",
      "iter 2900/2920/48828: loss 2.5572, time 114.79ms\n",
      "step 3000: train loss 2.5496, val loss 2.5494\n",
      "iter 3000/3020/48828: loss 2.5505, time 5015.01ms\n",
      "iter 3100/3120/48828: loss 2.5493, time 144.06ms\n",
      "iter 3200/3220/48828: loss 2.5533, time 118.77ms\n",
      "iter 3300/3320/48828: loss 2.5982, time 145.05ms\n",
      "iter 3400/3420/48828: loss 2.5862, time 109.07ms\n",
      "iter 3500/3520/48828: loss 2.5656, time 112.47ms\n",
      "iter 3600/3620/48828: loss 2.5660, time 116.58ms\n",
      "iter 3700/3720/48828: loss 2.5594, time 120.02ms\n",
      "iter 3800/3820/48828: loss 2.5484, time 111.30ms\n",
      "iter 3900/3920/48828: loss 2.5404, time 185.10ms\n",
      "step 4000: train loss 2.5713, val loss 2.5713\n",
      "iter 4000/4020/48828: loss 2.5717, time 5491.46ms\n",
      "iter 4100/4120/48828: loss 2.5460, time 109.44ms\n",
      "iter 4200/4220/48828: loss 2.5921, time 106.59ms\n",
      "iter 4300/4320/48828: loss 2.5740, time 112.81ms\n",
      "iter 4400/4420/48828: loss 2.5688, time 128.46ms\n",
      "iter 4500/4520/48828: loss 2.5570, time 111.31ms\n",
      "iter 4600/4620/48828: loss 2.5491, time 116.82ms\n",
      "iter 4700/4720/48828: loss 2.5453, time 106.61ms\n",
      "iter 4800/4820/48828: loss 2.5324, time 114.86ms\n",
      "iter 4900/4920/48828: loss 2.5295, time 238.62ms\n",
      "step 5000: train loss 2.5617, val loss 2.5616\n",
      "iter 5000/5020/48828: loss 2.5653, time 7304.27ms\n",
      "iter 5100/5120/48828: loss 2.5377, time 311.43ms\n",
      "iter 5200/5220/48828: loss 2.5894, time 113.70ms\n",
      "iter 5300/5320/48828: loss 2.5412, time 120.63ms\n",
      "iter 5400/5420/48828: loss 2.5344, time 117.14ms\n",
      "iter 5500/5520/48828: loss 2.5769, time 245.63ms\n",
      "iter 5600/5620/48828: loss 2.5505, time 137.27ms\n",
      "iter 5700/5720/48828: loss 2.5456, time 114.28ms\n",
      "iter 5800/5820/48828: loss 2.5578, time 154.18ms\n",
      "iter 5900/5920/48828: loss 2.5444, time 116.42ms\n",
      "step 6000: train loss 2.5459, val loss 2.5460\n",
      "iter 6000/6020/48828: loss 2.5465, time 6578.98ms\n",
      "iter 6100/6120/48828: loss 2.5388, time 151.52ms\n",
      "iter 6200/6220/48828: loss 2.5806, time 141.69ms\n",
      "iter 6300/6320/48828: loss 2.5428, time 100.65ms\n",
      "iter 6400/6420/48828: loss 2.5364, time 106.72ms\n",
      "iter 6500/6520/48828: loss 2.5590, time 120.89ms\n",
      "iter 6600/6620/48828: loss 2.5377, time 133.55ms\n",
      "iter 6700/6720/48828: loss 2.5774, time 118.43ms\n",
      "iter 6800/6820/48828: loss 2.5807, time 612.73ms\n",
      "iter 6900/6920/48828: loss 2.5394, time 99.00ms\n",
      "step 7000: train loss 2.5463, val loss 2.5463\n",
      "iter 7000/7020/48828: loss 2.5485, time 5749.40ms\n",
      "iter 7100/7120/48828: loss 2.5680, time 101.44ms\n",
      "iter 7200/7220/48828: loss 2.5452, time 215.53ms\n",
      "iter 7300/7320/48828: loss 2.5510, time 99.82ms\n",
      "iter 7400/7420/48828: loss 2.5660, time 532.14ms\n",
      "iter 7500/7520/48828: loss 2.5529, time 100.82ms\n",
      "iter 7600/7620/48828: loss 2.5849, time 243.92ms\n",
      "iter 7700/7720/48828: loss 2.5416, time 120.03ms\n",
      "iter 7800/7820/48828: loss 2.5575, time 110.26ms\n",
      "iter 7900/7920/48828: loss 2.5742, time 251.51ms\n",
      "step 8000: train loss 2.5537, val loss 2.5537\n",
      "iter 8000/8020/48828: loss 2.5557, time 3565.83ms\n",
      "iter 8100/8120/48828: loss 2.5462, time 253.81ms\n",
      "iter 8200/8220/48828: loss 2.5681, time 242.76ms\n",
      "iter 8300/8320/48828: loss 2.5812, time 104.54ms\n",
      "iter 8400/8420/48828: loss 2.5549, time 100.76ms\n",
      "iter 8500/8520/48828: loss 2.5660, time 141.95ms\n",
      "iter 8600/8620/48828: loss 2.5600, time 107.61ms\n",
      "iter 8700/8720/48828: loss 2.5568, time 101.65ms\n",
      "iter 8800/8820/48828: loss 2.5724, time 109.57ms\n",
      "iter 8900/8920/48828: loss 2.5440, time 249.19ms\n",
      "step 9000: train loss 2.5409, val loss 2.5409\n",
      "iter 9000/9020/48828: loss 2.5388, time 8639.77ms\n",
      "iter 9100/9120/48828: loss 2.5504, time 100.89ms\n",
      "iter 9200/9220/48828: loss 2.5270, time 114.71ms\n",
      "iter 9300/9320/48828: loss 2.5663, time 235.27ms\n",
      "iter 9400/9420/48828: loss 2.5479, time 126.02ms\n",
      "iter 9500/9520/48828: loss 2.5750, time 113.09ms\n",
      "iter 9600/9620/48828: loss 2.5524, time 364.88ms\n",
      "iter 9700/9720/48828: loss 2.5642, time 115.06ms\n",
      "iter 9800/9820/48828: loss 2.5544, time 159.17ms\n",
      "iter 9900/9920/48828: loss 2.5388, time 539.27ms\n",
      "step 10000: train loss 2.5623, val loss 2.5623\n",
      "iter 10000/10020/48828: loss 2.5655, time 5697.30ms\n",
      "iter 10100/10120/48828: loss 2.5860, time 103.01ms\n",
      "iter 10200/10220/48828: loss 2.5404, time 428.67ms\n",
      "iter 10300/10320/48828: loss 2.5706, time 106.11ms\n",
      "iter 10400/10420/48828: loss 2.5559, time 356.30ms\n",
      "iter 10500/10520/48828: loss 2.5380, time 122.04ms\n",
      "iter 10600/10620/48828: loss 2.5679, time 111.46ms\n",
      "iter 10700/10720/48828: loss 2.5502, time 120.27ms\n",
      "iter 10800/10820/48828: loss 2.5555, time 120.05ms\n",
      "iter 10900/10920/48828: loss 2.5497, time 125.38ms\n",
      "step 11000: train loss 2.5378, val loss 2.5378\n",
      "iter 11000/11020/48828: loss 2.5454, time 3622.26ms\n",
      "iter 11100/11120/48828: loss 2.5205, time 214.73ms\n",
      "iter 11200/11220/48828: loss 2.5197, time 114.46ms\n",
      "iter 11300/11320/48828: loss 2.5542, time 156.63ms\n",
      "iter 11400/11420/48828: loss 2.5432, time 99.11ms\n",
      "iter 11500/11520/48828: loss 2.5534, time 128.02ms\n",
      "iter 11600/11620/48828: loss 2.5463, time 116.11ms\n",
      "iter 11700/11720/48828: loss 2.5504, time 213.79ms\n",
      "iter 11800/11820/48828: loss 2.5476, time 108.54ms\n",
      "iter 11900/11920/48828: loss 2.5298, time 421.03ms\n",
      "step 12000: train loss 2.5570, val loss 2.5570\n",
      "iter 12000/12020/48828: loss 2.5581, time 5639.35ms\n",
      "iter 12100/12120/48828: loss 2.5579, time 99.87ms\n",
      "iter 12200/12220/48828: loss 2.5323, time 127.54ms\n",
      "iter 12300/12320/48828: loss 2.5448, time 449.07ms\n",
      "iter 12400/12420/48828: loss 2.5796, time 277.22ms\n",
      "iter 12500/12520/48828: loss 2.5875, time 95.03ms\n",
      "iter 12600/12620/48828: loss 2.5804, time 574.11ms\n",
      "iter 12700/12720/48828: loss 2.5870, time 122.56ms\n",
      "iter 12800/12820/48828: loss 2.5663, time 123.53ms\n",
      "iter 12900/12920/48828: loss 2.5512, time 123.89ms\n",
      "step 13000: train loss 2.5440, val loss 2.5440\n",
      "iter 13000/13020/48828: loss 2.5416, time 3995.00ms\n",
      "iter 13100/13120/48828: loss 2.5597, time 259.21ms\n",
      "iter 13200/13220/48828: loss 2.5279, time 112.02ms\n",
      "iter 13300/13320/48828: loss 2.5451, time 107.03ms\n",
      "iter 13400/13420/48828: loss 2.5639, time 102.33ms\n",
      "iter 13500/13520/48828: loss 2.5968, time 341.95ms\n",
      "iter 13600/13620/48828: loss 2.5571, time 235.74ms\n",
      "iter 13700/13720/48828: loss 2.5294, time 103.92ms\n",
      "iter 13800/13820/48828: loss 2.5151, time 114.08ms\n",
      "iter 13900/13920/48828: loss 2.5516, time 108.98ms\n",
      "step 14000: train loss 2.5853, val loss 2.5853\n",
      "iter 14000/14020/48828: loss 2.5858, time 3511.99ms\n",
      "iter 14100/14120/48828: loss 2.5867, time 99.05ms\n",
      "iter 14200/14220/48828: loss 2.5539, time 107.11ms\n",
      "iter 14300/14320/48828: loss 2.5690, time 123.02ms\n",
      "iter 14400/14420/48828: loss 2.5707, time 126.70ms\n",
      "iter 14500/14520/48828: loss 2.5652, time 110.57ms\n",
      "iter 14600/14620/48828: loss 2.5666, time 106.50ms\n",
      "iter 14700/14720/48828: loss 2.5931, time 111.72ms\n",
      "iter 14800/14820/48828: loss 2.5713, time 111.95ms\n",
      "iter 14900/14920/48828: loss 2.5485, time 515.09ms\n",
      "step 15000: train loss 2.5424, val loss 2.5424\n",
      "iter 15000/15020/48828: loss 2.5400, time 3511.88ms\n",
      "iter 15100/15120/48828: loss 2.5244, time 97.84ms\n",
      "iter 15200/15220/48828: loss 2.5457, time 107.71ms\n",
      "iter 15300/15320/48828: loss 2.5514, time 133.98ms\n",
      "iter 15400/15420/48828: loss 2.5501, time 122.63ms\n",
      "iter 15500/15520/48828: loss 2.5603, time 100.31ms\n",
      "iter 15600/15620/48828: loss 2.5422, time 468.22ms\n",
      "iter 15700/15720/48828: loss 2.5429, time 105.14ms\n",
      "iter 15800/15820/48828: loss 2.5413, time 488.57ms\n",
      "iter 15900/15920/48828: loss 2.5876, time 116.95ms\n",
      "step 16000: train loss 2.5483, val loss 2.5483\n",
      "iter 16000/16020/48828: loss 2.5482, time 4511.33ms\n",
      "iter 16100/16120/48828: loss 2.5574, time 95.38ms\n",
      "iter 16200/16220/48828: loss 2.5737, time 134.58ms\n",
      "iter 16300/16320/48828: loss 2.5781, time 109.90ms\n",
      "iter 16400/16420/48828: loss 2.5509, time 109.20ms\n",
      "iter 16500/16520/48828: loss 2.5687, time 189.32ms\n",
      "iter 16600/16620/48828: loss 2.5744, time 113.08ms\n",
      "iter 16700/16720/48828: loss 2.5563, time 137.90ms\n",
      "iter 16800/16820/48828: loss 2.5551, time 125.10ms\n",
      "iter 16900/16920/48828: loss 2.5523, time 117.73ms\n",
      "step 17000: train loss 2.5568, val loss 2.5570\n",
      "iter 17000/17020/48828: loss 2.5582, time 4171.14ms\n",
      "iter 17100/17120/48828: loss 2.5382, time 114.23ms\n",
      "iter 17200/17220/48828: loss 2.5417, time 108.21ms\n",
      "iter 17300/17320/48828: loss 2.5563, time 329.80ms\n",
      "iter 17400/17420/48828: loss 2.5506, time 125.57ms\n",
      "iter 17500/17520/48828: loss 2.5418, time 102.00ms\n",
      "iter 17600/17620/48828: loss 2.5529, time 107.32ms\n",
      "iter 17700/17720/48828: loss 2.5402, time 111.90ms\n",
      "iter 17800/17820/48828: loss 2.5446, time 110.86ms\n",
      "iter 17900/17920/48828: loss 2.5740, time 128.27ms\n",
      "step 18000: train loss 2.5411, val loss 2.5413\n",
      "iter 18000/18020/48828: loss 2.5409, time 3878.93ms\n",
      "iter 18100/18120/48828: loss 2.5308, time 107.47ms\n",
      "iter 18200/18220/48828: loss 2.5326, time 457.52ms\n",
      "iter 18300/18320/48828: loss 2.5563, time 122.79ms\n",
      "iter 18400/18420/48828: loss 2.5673, time 129.55ms\n",
      "iter 18500/18520/48828: loss 2.5603, time 111.28ms\n",
      "iter 18600/18620/48828: loss 2.5344, time 110.96ms\n",
      "iter 18700/18720/48828: loss 2.5427, time 115.62ms\n",
      "iter 18800/18820/48828: loss 2.5461, time 174.36ms\n",
      "iter 18900/18920/48828: loss 2.5449, time 106.86ms\n",
      "step 19000: train loss 2.5535, val loss 2.5534\n",
      "iter 19000/19020/48828: loss 2.5462, time 4045.42ms\n",
      "iter 19100/19120/48828: loss 2.5458, time 99.89ms\n",
      "iter 19200/19220/48828: loss 2.5432, time 146.93ms\n",
      "iter 19300/19320/48828: loss 2.5344, time 113.01ms\n",
      "iter 19400/19420/48828: loss 2.5464, time 242.56ms\n",
      "iter 19500/19520/48828: loss 2.5458, time 111.24ms\n",
      "iter 19600/19620/48828: loss 2.5343, time 109.79ms\n",
      "iter 19700/19720/48828: loss 2.5776, time 123.54ms\n",
      "iter 19800/19820/48828: loss 2.5241, time 99.23ms\n",
      "iter 19900/19920/48828: loss 2.5403, time 114.53ms\n",
      "step 20000: train loss 2.5604, val loss 2.5606\n",
      "iter 20000/20020/48828: loss 2.5586, time 4299.94ms\n",
      "iter 20100/20120/48828: loss 2.5273, time 457.70ms\n",
      "iter 20200/20220/48828: loss 2.5455, time 150.18ms\n",
      "iter 20300/20320/48828: loss 2.5656, time 130.52ms\n",
      "iter 20400/20420/48828: loss 2.5288, time 107.01ms\n",
      "iter 20500/20520/48828: loss 2.5626, time 120.42ms\n",
      "iter 20600/20620/48828: loss 2.5426, time 125.70ms\n",
      "iter 20700/20720/48828: loss 2.5677, time 108.90ms\n",
      "iter 20800/20820/48828: loss 2.5368, time 255.36ms\n",
      "iter 20900/20920/48828: loss 2.5431, time 102.31ms\n",
      "step 21000: train loss 2.5264, val loss 2.5266\n",
      "iter 21000/21020/48828: loss 2.5197, time 3531.40ms\n",
      "iter 21100/21120/48828: loss 2.5477, time 100.82ms\n",
      "iter 21200/21220/48828: loss 2.5386, time 122.95ms\n",
      "iter 21300/21320/48828: loss 2.5510, time 542.27ms\n",
      "iter 21400/21420/48828: loss 2.5477, time 106.88ms\n",
      "iter 21500/21520/48828: loss 2.5455, time 110.04ms\n",
      "iter 21600/21620/48828: loss 2.5459, time 109.77ms\n",
      "iter 21700/21720/48828: loss 2.5455, time 97.85ms\n",
      "iter 21800/21820/48828: loss 2.5840, time 108.72ms\n",
      "iter 21900/21920/48828: loss 2.5238, time 115.61ms\n",
      "step 22000: train loss 2.5501, val loss 2.5500\n",
      "iter 22000/22020/48828: loss 2.5485, time 3901.05ms\n",
      "iter 22100/22120/48828: loss 2.5421, time 114.34ms\n",
      "iter 22200/22220/48828: loss 2.5278, time 119.87ms\n",
      "iter 22300/22320/48828: loss 2.5551, time 102.20ms\n",
      "iter 22400/22420/48828: loss 2.5372, time 145.82ms\n",
      "iter 22500/22520/48828: loss 2.5428, time 108.75ms\n",
      "iter 22600/22620/48828: loss 2.5231, time 106.48ms\n",
      "iter 22700/22720/48828: loss 2.5473, time 105.60ms\n",
      "iter 22800/22820/48828: loss 2.5315, time 137.12ms\n",
      "iter 22900/22920/48828: loss 2.5125, time 110.78ms\n",
      "step 23000: train loss 2.5167, val loss 2.5166\n",
      "iter 23000/23020/48828: loss 2.5186, time 4077.02ms\n",
      "iter 23100/23120/48828: loss 2.5297, time 104.27ms\n",
      "iter 23200/23220/48828: loss 2.5366, time 110.75ms\n",
      "iter 23300/23320/48828: loss 2.5367, time 112.09ms\n",
      "iter 23400/23420/48828: loss 2.5130, time 107.23ms\n",
      "iter 23500/23520/48828: loss 2.5392, time 113.74ms\n",
      "iter 23600/23620/48828: loss 2.5134, time 110.46ms\n",
      "iter 23700/23720/48828: loss 2.5248, time 113.39ms\n",
      "iter 23800/23820/48828: loss 2.5350, time 167.48ms\n",
      "iter 23900/23920/48828: loss 2.4968, time 510.84ms\n",
      "step 24000: train loss 2.5266, val loss 2.5267\n",
      "iter 24000/24020/48828: loss 2.5249, time 3346.82ms\n",
      "iter 24100/24120/48828: loss 2.5182, time 107.71ms\n",
      "iter 24200/24220/48828: loss 2.5063, time 112.69ms\n",
      "iter 24300/24320/48828: loss 2.5080, time 100.61ms\n",
      "iter 24400/24420/48828: loss 2.5488, time 110.83ms\n",
      "iter 24500/24520/48828: loss 2.5000, time 108.46ms\n",
      "iter 24600/24620/48828: loss 2.5219, time 120.64ms\n",
      "iter 24700/24720/48828: loss 2.5214, time 98.18ms\n",
      "iter 24800/24820/48828: loss 2.5196, time 104.70ms\n",
      "iter 24900/24920/48828: loss 2.4973, time 234.67ms\n",
      "step 25000: train loss 2.5260, val loss 2.5259\n",
      "iter 25000/25020/48828: loss 2.5253, time 3800.86ms\n",
      "iter 25100/25120/48828: loss 2.5252, time 105.35ms\n",
      "iter 25200/25220/48828: loss 2.5295, time 108.85ms\n",
      "iter 25300/25320/48828: loss 2.5789, time 114.94ms\n",
      "iter 25400/25420/48828: loss 2.5280, time 101.61ms\n",
      "iter 25500/25520/48828: loss 2.5444, time 127.00ms\n",
      "iter 25600/25620/48828: loss 2.5228, time 107.74ms\n",
      "iter 25700/25720/48828: loss 2.5385, time 106.40ms\n",
      "iter 25800/25820/48828: loss 2.5262, time 102.47ms\n",
      "iter 25900/25920/48828: loss 2.5299, time 100.08ms\n",
      "step 26000: train loss 2.5151, val loss 2.5152\n",
      "iter 26000/26020/48828: loss 2.5193, time 4178.98ms\n",
      "iter 26100/26120/48828: loss 2.5267, time 244.70ms\n",
      "iter 26200/26220/48828: loss 2.5260, time 99.49ms\n",
      "iter 26300/26320/48828: loss 2.5107, time 101.79ms\n",
      "iter 26400/26420/48828: loss 2.5093, time 107.99ms\n",
      "iter 26500/26520/48828: loss 2.5253, time 253.28ms\n",
      "iter 26600/26620/48828: loss 2.5147, time 114.52ms\n",
      "iter 26700/26720/48828: loss 2.5265, time 102.13ms\n",
      "iter 26800/26820/48828: loss 2.5434, time 98.00ms\n",
      "iter 26900/26920/48828: loss 2.5102, time 99.12ms\n",
      "step 27000: train loss 2.5151, val loss 2.5149\n",
      "iter 27000/27020/48828: loss 2.5178, time 7610.95ms\n",
      "iter 27100/27120/48828: loss 2.5238, time 109.69ms\n",
      "iter 27200/27220/48828: loss 2.5517, time 100.44ms\n",
      "iter 27300/27320/48828: loss 2.5392, time 113.07ms\n",
      "iter 27400/27420/48828: loss 2.5449, time 107.54ms\n",
      "iter 27500/27520/48828: loss 2.5730, time 112.08ms\n",
      "iter 27600/27620/48828: loss 2.5576, time 116.08ms\n",
      "iter 27700/27720/48828: loss 2.5352, time 109.34ms\n",
      "iter 27800/27820/48828: loss 2.5435, time 98.54ms\n",
      "iter 27900/27920/48828: loss 2.5361, time 103.50ms\n",
      "step 28000: train loss 2.5189, val loss 2.5189\n",
      "iter 28000/28020/48828: loss 2.5127, time 3913.40ms\n",
      "iter 28100/28120/48828: loss 2.5512, time 104.48ms\n",
      "iter 28200/28220/48828: loss 2.5203, time 107.48ms\n",
      "iter 28300/28320/48828: loss 2.5170, time 108.80ms\n",
      "iter 28400/28420/48828: loss 2.5090, time 105.75ms\n",
      "iter 28500/28520/48828: loss 2.5420, time 100.40ms\n",
      "iter 28600/28620/48828: loss 2.5780, time 99.16ms\n",
      "iter 28700/28720/48828: loss 2.5744, time 120.40ms\n",
      "iter 28800/28820/48828: loss 2.5283, time 99.27ms\n",
      "iter 28900/28920/48828: loss 2.5496, time 383.77ms\n",
      "step 29000: train loss 2.5321, val loss 2.5321\n",
      "iter 29000/29020/48828: loss 2.5275, time 9919.93ms\n",
      "iter 29100/29120/48828: loss 2.5196, time 112.27ms\n",
      "iter 29200/29220/48828: loss 2.5395, time 101.13ms\n",
      "iter 29300/29320/48828: loss 2.5420, time 101.91ms\n",
      "iter 29400/29420/48828: loss 2.5156, time 107.23ms\n",
      "iter 29500/29520/48828: loss 2.5258, time 95.33ms\n",
      "iter 29600/29620/48828: loss 2.5168, time 100.64ms\n",
      "iter 29700/29720/48828: loss 2.4886, time 125.23ms\n",
      "iter 29800/29820/48828: loss 2.5030, time 109.62ms\n",
      "iter 29900/29920/48828: loss 2.5144, time 115.54ms\n",
      "step 30000: train loss 2.5088, val loss 2.5090\n",
      "iter 30000/30020/48828: loss 2.5075, time 4039.65ms\n",
      "iter 30100/30120/48828: loss 2.5031, time 110.25ms\n",
      "iter 30200/30220/48828: loss 2.5017, time 117.95ms\n",
      "iter 30300/30320/48828: loss 2.4988, time 105.08ms\n",
      "iter 30400/30420/48828: loss 2.5141, time 105.98ms\n",
      "iter 30500/30520/48828: loss 2.4972, time 105.02ms\n",
      "iter 30600/30620/48828: loss 2.4937, time 101.18ms\n",
      "iter 30700/30720/48828: loss 2.5184, time 100.53ms\n",
      "iter 30800/30820/48828: loss 2.4801, time 118.50ms\n",
      "iter 30900/30920/48828: loss 2.4921, time 110.67ms\n",
      "step 31000: train loss 2.4702, val loss 2.4702\n",
      "iter 31000/31020/48828: loss 2.4752, time 3240.82ms\n",
      "iter 31100/31120/48828: loss 2.4466, time 334.47ms\n",
      "iter 31200/31220/48828: loss 2.4694, time 109.39ms\n",
      "iter 31300/31320/48828: loss 2.4478, time 376.68ms\n",
      "iter 31400/31420/48828: loss 2.4612, time 110.02ms\n",
      "iter 31500/31520/48828: loss 2.4635, time 108.01ms\n",
      "iter 31600/31620/48828: loss 2.4612, time 109.86ms\n",
      "iter 31700/31720/48828: loss 2.4472, time 117.87ms\n",
      "iter 31800/31820/48828: loss 2.4813, time 108.73ms\n",
      "iter 31900/31920/48828: loss 2.4582, time 104.17ms\n",
      "step 32000: train loss 2.4837, val loss 2.4834\n",
      "iter 32000/32020/48828: loss 2.4799, time 5525.46ms\n",
      "iter 32100/32120/48828: loss 2.5664, time 102.15ms\n",
      "iter 32200/32220/48828: loss 2.4939, time 104.67ms\n",
      "iter 32300/32320/48828: loss 2.4759, time 99.21ms\n",
      "iter 32400/32420/48828: loss 2.5345, time 101.06ms\n",
      "iter 32500/32520/48828: loss 2.5178, time 113.29ms\n",
      "iter 32600/32620/48828: loss 2.5504, time 135.77ms\n",
      "iter 32700/32720/48828: loss 2.5168, time 229.41ms\n",
      "iter 32800/32820/48828: loss 2.5261, time 103.46ms\n",
      "iter 32900/32920/48828: loss 2.5011, time 97.72ms\n",
      "step 33000: train loss 2.5105, val loss 2.5104\n",
      "iter 33000/33020/48828: loss 2.5100, time 4115.20ms\n",
      "iter 33100/33120/48828: loss 2.5110, time 114.63ms\n",
      "iter 33200/33220/48828: loss 2.4844, time 112.70ms\n",
      "iter 33300/33320/48828: loss 2.4970, time 104.97ms\n",
      "iter 33400/33420/48828: loss 2.5172, time 98.67ms\n",
      "iter 33500/33520/48828: loss 2.5361, time 507.86ms\n",
      "iter 33600/33620/48828: loss 2.5393, time 105.69ms\n",
      "iter 33700/33720/48828: loss 2.5052, time 407.16ms\n",
      "iter 33800/33820/48828: loss 2.4863, time 106.27ms\n",
      "iter 33900/33920/48828: loss 2.5167, time 102.27ms\n",
      "step 34000: train loss 2.5051, val loss 2.5049\n",
      "iter 34000/34020/48828: loss 2.5049, time 3685.80ms\n",
      "iter 34100/34120/48828: loss 2.5066, time 111.13ms\n",
      "iter 34200/34220/48828: loss 2.4806, time 110.16ms\n",
      "iter 34300/34320/48828: loss 2.4838, time 106.29ms\n",
      "iter 34400/34420/48828: loss 2.4934, time 99.13ms\n",
      "iter 34500/34520/48828: loss 2.4895, time 118.43ms\n",
      "iter 34600/34620/48828: loss 2.4697, time 105.13ms\n",
      "iter 34700/34720/48828: loss 2.4923, time 118.19ms\n",
      "iter 34800/34820/48828: loss 2.4791, time 109.12ms\n",
      "iter 34900/34920/48828: loss 2.4647, time 112.05ms\n",
      "step 35000: train loss 2.4508, val loss 2.4506\n",
      "iter 35000/35020/48828: loss 2.4603, time 3496.38ms\n",
      "iter 35100/35120/48828: loss 2.4435, time 105.23ms\n",
      "iter 35200/35220/48828: loss 2.4687, time 125.63ms\n",
      "iter 35300/35320/48828: loss 2.4852, time 109.97ms\n",
      "iter 35400/35420/48828: loss 2.4548, time 238.97ms\n",
      "iter 35500/35520/48828: loss 2.4535, time 111.55ms\n",
      "iter 35600/35620/48828: loss 2.4782, time 100.90ms\n",
      "iter 35700/35720/48828: loss 2.5340, time 134.43ms\n",
      "iter 35800/35820/48828: loss 2.4712, time 106.64ms\n",
      "iter 35900/35920/48828: loss 2.4670, time 107.19ms\n",
      "step 36000: train loss 2.4692, val loss 2.4692\n",
      "iter 36000/36020/48828: loss 2.4695, time 3901.27ms\n",
      "iter 36100/36120/48828: loss 2.4671, time 127.55ms\n",
      "iter 36200/36220/48828: loss 2.4633, time 113.05ms\n",
      "iter 36300/36320/48828: loss 2.4803, time 106.95ms\n",
      "iter 36400/36420/48828: loss 2.4835, time 99.59ms\n",
      "iter 36500/36520/48828: loss 2.4482, time 124.65ms\n",
      "iter 36600/36620/48828: loss 2.4894, time 118.97ms\n",
      "iter 36700/36720/48828: loss 2.4367, time 106.26ms\n",
      "iter 36800/36820/48828: loss 2.4472, time 126.90ms\n",
      "iter 36900/36920/48828: loss 2.4457, time 107.15ms\n",
      "step 37000: train loss 2.4874, val loss 2.4875\n",
      "iter 37000/37020/48828: loss 2.4900, time 3837.74ms\n",
      "iter 37100/37120/48828: loss 2.4753, time 103.96ms\n",
      "iter 37200/37220/48828: loss 2.4769, time 100.99ms\n",
      "iter 37300/37320/48828: loss 2.4804, time 113.29ms\n",
      "iter 37400/37420/48828: loss 2.4580, time 107.71ms\n",
      "iter 37500/37520/48828: loss 2.4814, time 101.59ms\n",
      "iter 37600/37620/48828: loss 2.4669, time 396.12ms\n",
      "iter 37700/37720/48828: loss 2.4836, time 117.63ms\n",
      "iter 37800/37820/48828: loss 2.4858, time 103.96ms\n",
      "iter 37900/37920/48828: loss 2.4443, time 103.23ms\n",
      "step 38000: train loss 2.5342, val loss 2.5337\n",
      "iter 38000/38020/48828: loss 2.5326, time 3650.50ms\n",
      "iter 38100/38120/48828: loss 2.4760, time 118.69ms\n",
      "iter 38200/38220/48828: loss 2.4959, time 111.43ms\n",
      "iter 38300/38320/48828: loss 2.4568, time 111.87ms\n",
      "iter 38400/38420/48828: loss 2.5183, time 106.52ms\n",
      "iter 38500/38520/48828: loss 2.4598, time 119.85ms\n",
      "iter 38600/38620/48828: loss 2.4361, time 109.32ms\n",
      "iter 38700/38720/48828: loss 2.4321, time 102.28ms\n",
      "iter 38800/38820/48828: loss 2.4233, time 101.39ms\n",
      "iter 38900/38920/48828: loss 2.4145, time 100.41ms\n",
      "step 39000: train loss 2.4117, val loss 2.4114\n",
      "iter 39000/39020/48828: loss 2.4115, time 3727.88ms\n",
      "iter 39100/39120/48828: loss 2.4197, time 200.30ms\n",
      "iter 39200/39220/48828: loss 2.4397, time 129.67ms\n",
      "iter 39300/39320/48828: loss 2.4315, time 100.79ms\n",
      "iter 39400/39420/48828: loss 2.4669, time 101.26ms\n",
      "iter 39500/39520/48828: loss 2.4174, time 102.99ms\n",
      "iter 39600/39620/48828: loss 2.4195, time 111.51ms\n",
      "iter 39700/39720/48828: loss 2.4096, time 108.88ms\n",
      "iter 39800/39820/48828: loss 2.3939, time 109.93ms\n",
      "iter 39900/39920/48828: loss 2.3905, time 110.63ms\n",
      "step 40000: train loss 2.3809, val loss 2.3808\n",
      "iter 40000/40020/48828: loss 2.3837, time 4041.08ms\n",
      "iter 40100/40120/48828: loss 2.4138, time 118.03ms\n",
      "iter 40200/40220/48828: loss 2.4378, time 100.90ms\n",
      "iter 40300/40320/48828: loss 2.3928, time 107.61ms\n",
      "iter 40400/40420/48828: loss 2.3809, time 113.80ms\n",
      "iter 40500/40520/48828: loss 2.3859, time 123.21ms\n",
      "iter 40600/40620/48828: loss 2.4289, time 112.01ms\n",
      "iter 40700/40720/48828: loss 2.4040, time 112.94ms\n",
      "iter 40800/40820/48828: loss 2.3991, time 102.37ms\n",
      "iter 40900/40920/48828: loss 2.3764, time 102.73ms\n",
      "step 41000: train loss 2.3938, val loss 2.3937\n",
      "iter 41000/41020/48828: loss 2.3944, time 3690.62ms\n",
      "iter 41100/41120/48828: loss 2.3558, time 124.65ms\n",
      "iter 41200/41220/48828: loss 2.3781, time 108.43ms\n",
      "iter 41300/41320/48828: loss 2.4118, time 126.71ms\n",
      "iter 41400/41420/48828: loss 2.3800, time 109.00ms\n",
      "iter 41500/41520/48828: loss 2.3574, time 114.32ms\n",
      "iter 41600/41620/48828: loss 2.3737, time 107.01ms\n",
      "iter 41700/41720/48828: loss 2.3469, time 102.21ms\n",
      "iter 41800/41820/48828: loss 2.3679, time 107.30ms\n",
      "iter 41900/41920/48828: loss 2.3460, time 101.06ms\n",
      "step 42000: train loss 2.3455, val loss 2.3456\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 42000/42020/48828: loss 2.3391, time 3782.93ms\n",
      "iter 42100/42120/48828: loss 2.3350, time 110.59ms\n",
      "iter 42200/42220/48828: loss 2.3313, time 106.65ms\n",
      "iter 42300/42320/48828: loss 2.3512, time 111.30ms\n",
      "iter 42400/42420/48828: loss 2.3245, time 106.62ms\n",
      "iter 42500/42520/48828: loss 2.3247, time 110.53ms\n",
      "iter 42600/42620/48828: loss 2.3119, time 241.80ms\n",
      "iter 42700/42720/48828: loss 2.3275, time 105.63ms\n",
      "iter 42800/42820/48828: loss 2.2935, time 100.95ms\n",
      "iter 42900/42920/48828: loss 2.3159, time 116.07ms\n",
      "step 43000: train loss 2.3035, val loss 2.3040\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 43000/43020/48828: loss 2.3182, time 3783.12ms\n",
      "iter 43100/43120/48828: loss 2.3056, time 136.58ms\n",
      "iter 43200/43220/48828: loss 2.3099, time 103.71ms\n",
      "iter 43300/43320/48828: loss 2.3150, time 102.39ms\n",
      "iter 43400/43420/48828: loss 2.3001, time 101.46ms\n",
      "iter 43500/43520/48828: loss 2.3044, time 114.19ms\n",
      "iter 43600/43620/48828: loss 2.2848, time 319.36ms\n",
      "iter 43700/43720/48828: loss 2.2805, time 108.13ms\n",
      "iter 43800/43820/48828: loss 2.2839, time 109.75ms\n",
      "iter 43900/43920/48828: loss 2.2952, time 113.05ms\n",
      "step 44000: train loss 2.2723, val loss 2.2726\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 44000/44020/48828: loss 2.2780, time 4044.13ms\n",
      "iter 44100/44120/48828: loss 2.2719, time 101.51ms\n",
      "iter 44200/44220/48828: loss 2.2586, time 111.79ms\n",
      "iter 44300/44320/48828: loss 2.2533, time 106.95ms\n",
      "iter 44400/44420/48828: loss 2.2622, time 109.56ms\n",
      "iter 44500/44520/48828: loss 2.2646, time 113.27ms\n",
      "iter 44600/44620/48828: loss 2.2531, time 111.67ms\n",
      "iter 44700/44720/48828: loss 2.2445, time 102.69ms\n",
      "iter 44800/44820/48828: loss 2.2174, time 101.07ms\n",
      "iter 44900/44920/48828: loss 2.2264, time 113.10ms\n",
      "step 45000: train loss 2.2278, val loss 2.2281\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 45000/45020/48828: loss 2.2248, time 3309.59ms\n",
      "iter 45100/45120/48828: loss 2.2457, time 114.07ms\n",
      "iter 45200/45220/48828: loss 2.2134, time 106.77ms\n",
      "iter 45300/45320/48828: loss 2.2341, time 113.40ms\n",
      "iter 45400/45420/48828: loss 2.2086, time 107.05ms\n",
      "iter 45500/45520/48828: loss 2.2129, time 106.71ms\n",
      "iter 45600/45620/48828: loss 2.2380, time 121.44ms\n",
      "iter 45700/45720/48828: loss 2.2088, time 119.95ms\n",
      "iter 45800/45820/48828: loss 2.2129, time 237.43ms\n",
      "iter 45900/45920/48828: loss 2.1980, time 113.43ms\n",
      "step 46000: train loss 2.1958, val loss 2.1955\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 46000/46020/48828: loss 2.1923, time 4014.75ms\n",
      "iter 46100/46120/48828: loss 2.1781, time 100.04ms\n",
      "iter 46200/46220/48828: loss 2.1928, time 496.72ms\n",
      "iter 46300/46320/48828: loss 2.1854, time 105.89ms\n",
      "iter 46400/46420/48828: loss 2.1976, time 109.93ms\n",
      "iter 46500/46520/48828: loss 2.1922, time 100.74ms\n",
      "iter 46600/46620/48828: loss 2.1745, time 110.38ms\n",
      "iter 46700/46720/48828: loss 2.1864, time 121.90ms\n",
      "iter 46800/46820/48828: loss 2.1551, time 111.74ms\n",
      "iter 46900/46920/48828: loss 2.1881, time 107.64ms\n",
      "step 47000: train loss 2.1738, val loss 2.1736\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 47000/47020/48828: loss 2.1744, time 3962.34ms\n",
      "iter 47100/47120/48828: loss 2.1638, time 100.15ms\n",
      "iter 47200/47220/48828: loss 2.1584, time 99.89ms\n",
      "iter 47300/47320/48828: loss 2.1670, time 100.30ms\n",
      "iter 47400/47420/48828: loss 2.1572, time 130.41ms\n",
      "iter 47500/47520/48828: loss 2.1596, time 100.25ms\n",
      "iter 47600/47620/48828: loss 2.1550, time 115.25ms\n",
      "iter 47700/47720/48828: loss 2.1454, time 122.84ms\n",
      "iter 47800/47820/48828: loss 2.1452, time 105.73ms\n",
      "iter 47900/47920/48828: loss 2.1282, time 110.77ms\n",
      "step 48000: train loss 2.1488, val loss 2.1483\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 48000/48020/48828: loss 2.1600, time 3241.97ms\n",
      "iter 48100/48120/48828: loss 2.1453, time 112.58ms\n",
      "iter 48200/48220/48828: loss 2.1425, time 109.06ms\n",
      "iter 48300/48320/48828: loss 2.1661, time 125.43ms\n",
      "iter 48400/48420/48828: loss 2.1347, time 115.59ms\n",
      "iter 48500/48520/48828: loss 2.1218, time 103.34ms\n",
      "iter 48600/48620/48828: loss 2.1657, time 104.47ms\n",
      "iter 48700/48720/48828: loss 2.1433, time 102.75ms\n",
      "iter 48800/48820/48828: loss 2.1207, time 105.88ms\n",
      "\n",
      "\n",
      "## Running generation 5 for config_alias=trajectory_sims-200_games-10000_size-tiny_train-488_x1\n",
      "Playing 10000 games, simulations=200, model_size=small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [16:39<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner Stats:\n",
      "Winner counts: win[1]=58.39% win[2]=41.61%, n=10000, draw=0.00%\n",
      "Game Length min: 7, max: 42, mean: 13.73\n",
      "Winner Stats by initial move:\n",
      "  a=1: n=462 win[1]=37.66% counts=Counter({2: 288, 1: 174}), win[2]=62.34% draw=0.00%\n",
      "  a=2: n=1005 win[1]=54.63% counts=Counter({1: 549, 2: 456}), win[2]=45.37% draw=0.00%\n",
      "  a=3: n=1499 win[1]=48.10% counts=Counter({2: 778, 1: 721}), win[2]=51.90% draw=0.00%\n",
      "  a=4: n=5503 win[1]=67.85% counts=Counter({1: 3734, 2: 1769}), win[2]=32.15% draw=0.00%\n",
      "  a=5: n=864 win[1]=41.32% counts=Counter({2: 507, 1: 357}), win[2]=58.68% draw=0.00%\n",
      "  a=6: n=191 win[1]=65.45% counts=Counter({1: 125, 2: 66}), win[2]=34.55% draw=0.00%\n",
      "  a=7: n=476 win[1]=37.61% counts=Counter({2: 297, 1: 179}), win[2]=62.39% draw=0.00%\n",
      "Training model on gen-5\n",
      "TrainConfig(model_name='connect4-e2e', model_version='v1', eval_interval=1000, log_interval=100, eval_iters=20, eval_only=False, always_save_checkpoint=False, wandb_log=False, gradient_accumulation_steps=1, batch_size=2048, learning_rate=0.05, max_epochs=10000, max_iters=48828, weight_decay=0.1, beta1=0.9, beta2=0.99, grad_clip=1.0, decay_lr=True, warmup_iters=0, lr_decay_iters=48828, min_lr=0.005, device='cuda', dtype='bfloat16', compile=False)\n",
      "num decayed parameter tensors: 19, with 50,880 parameters\n",
      "num non-decayed parameter tensors: 11, with 298 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.2267, val loss 2.2269\n",
      "iter 0/25/48828: loss 2.2098, time 4936.42ms\n",
      "iter 100/125/48828: loss 2.5307, time 153.13ms\n",
      "iter 200/225/48828: loss 2.5216, time 133.08ms\n",
      "iter 300/325/48828: loss 2.5363, time 152.68ms\n",
      "iter 400/425/48828: loss 2.5301, time 120.97ms\n",
      "iter 500/525/48828: loss 2.5263, time 624.70ms\n",
      "iter 600/625/48828: loss 2.5578, time 133.83ms\n",
      "iter 700/725/48828: loss 2.5753, time 283.54ms\n",
      "iter 800/825/48828: loss 2.5905, time 239.17ms\n",
      "iter 900/925/48828: loss 2.5617, time 158.25ms\n",
      "step 1000: train loss 2.5511, val loss 2.5510\n",
      "iter 1000/1025/48828: loss 2.5517, time 4366.26ms\n",
      "iter 1100/1125/48828: loss 2.5408, time 138.87ms\n",
      "iter 1200/1225/48828: loss 2.5295, time 114.05ms\n",
      "iter 1300/1325/48828: loss 2.5502, time 123.31ms\n",
      "iter 1400/1425/48828: loss 2.5550, time 108.34ms\n",
      "iter 1500/1525/48828: loss 2.5296, time 501.06ms\n",
      "iter 1600/1625/48828: loss 2.5521, time 1346.90ms\n",
      "iter 1700/1725/48828: loss 2.5383, time 105.23ms\n",
      "iter 1800/1825/48828: loss 2.5426, time 232.07ms\n",
      "iter 1900/1925/48828: loss 2.5193, time 447.99ms\n",
      "step 2000: train loss 2.5200, val loss 2.5211\n",
      "iter 2000/2025/48828: loss 2.5241, time 4270.57ms\n",
      "iter 2100/2125/48828: loss 2.5415, time 148.53ms\n",
      "iter 2200/2225/48828: loss 2.5446, time 121.97ms\n",
      "iter 2300/2325/48828: loss 2.5592, time 527.17ms\n",
      "iter 2400/2425/48828: loss 2.5353, time 98.18ms\n",
      "iter 2500/2525/48828: loss 2.5398, time 160.91ms\n",
      "iter 2600/2625/48828: loss 2.5366, time 99.81ms\n",
      "iter 2700/2725/48828: loss 2.5189, time 226.44ms\n",
      "iter 2800/2825/48828: loss 2.5298, time 434.65ms\n",
      "iter 2900/2925/48828: loss 2.5437, time 389.52ms\n",
      "step 3000: train loss 2.5226, val loss 2.5228\n",
      "iter 3000/3025/48828: loss 2.5221, time 9162.73ms\n",
      "iter 3100/3125/48828: loss 2.5274, time 128.87ms\n",
      "iter 3200/3225/48828: loss 2.5371, time 567.79ms\n",
      "iter 3300/3325/48828: loss 2.5411, time 130.53ms\n",
      "iter 3400/3425/48828: loss 2.5468, time 323.86ms\n",
      "iter 3500/3525/48828: loss 2.5509, time 99.35ms\n",
      "iter 3600/3625/48828: loss 2.5586, time 372.44ms\n",
      "iter 3700/3725/48828: loss 2.5497, time 128.58ms\n",
      "iter 3800/3825/48828: loss 2.5551, time 207.94ms\n",
      "iter 3900/3925/48828: loss 2.5450, time 1942.43ms\n",
      "step 4000: train loss 2.5443, val loss 2.5444\n",
      "iter 4000/4025/48828: loss 2.5428, time 4463.15ms\n",
      "iter 4100/4125/48828: loss 2.5331, time 129.74ms\n",
      "iter 4200/4225/48828: loss 2.5326, time 215.23ms\n",
      "iter 4300/4325/48828: loss 2.5208, time 121.64ms\n",
      "iter 4400/4425/48828: loss 2.5489, time 202.62ms\n",
      "iter 4500/4525/48828: loss 2.5388, time 230.24ms\n",
      "iter 4600/4625/48828: loss 2.5678, time 295.61ms\n",
      "iter 4700/4725/48828: loss 2.5294, time 128.28ms\n",
      "iter 4800/4825/48828: loss 2.5408, time 293.20ms\n",
      "iter 4900/4925/48828: loss 2.5441, time 110.16ms\n",
      "step 5000: train loss 2.5358, val loss 2.5357\n",
      "iter 5000/5025/48828: loss 2.5362, time 4306.44ms\n",
      "iter 5100/5125/48828: loss 2.5380, time 108.40ms\n",
      "iter 5200/5225/48828: loss 2.5620, time 107.77ms\n",
      "iter 5300/5325/48828: loss 2.5174, time 495.01ms\n",
      "iter 5400/5425/48828: loss 2.5450, time 779.29ms\n",
      "iter 5500/5525/48828: loss 2.5541, time 117.73ms\n",
      "iter 5600/5625/48828: loss 2.5591, time 180.61ms\n",
      "iter 5700/5725/48828: loss 2.5274, time 209.40ms\n",
      "iter 5800/5825/48828: loss 2.5195, time 205.03ms\n",
      "iter 5900/5925/48828: loss 2.5265, time 118.08ms\n",
      "step 6000: train loss 2.5348, val loss 2.5348\n",
      "iter 6000/6025/48828: loss 2.5383, time 3654.66ms\n",
      "iter 6100/6125/48828: loss 2.5407, time 110.10ms\n",
      "iter 6200/6225/48828: loss 2.5351, time 1824.19ms\n",
      "iter 6300/6325/48828: loss 2.5459, time 226.36ms\n",
      "iter 6400/6425/48828: loss 2.5357, time 184.04ms\n",
      "iter 6500/6525/48828: loss 2.5522, time 135.43ms\n",
      "iter 6600/6625/48828: loss 2.5559, time 221.26ms\n",
      "iter 6700/6725/48828: loss 2.5724, time 106.22ms\n",
      "iter 6800/6825/48828: loss 2.5454, time 108.14ms\n",
      "iter 6900/6925/48828: loss 2.5572, time 106.45ms\n",
      "step 7000: train loss 2.5376, val loss 2.5368\n",
      "iter 7000/7025/48828: loss 2.5422, time 3607.27ms\n",
      "iter 7100/7125/48828: loss 2.5328, time 114.36ms\n",
      "iter 7200/7225/48828: loss 2.5440, time 214.90ms\n",
      "iter 7300/7325/48828: loss 2.5345, time 105.02ms\n",
      "iter 7400/7425/48828: loss 2.5465, time 132.93ms\n",
      "iter 7500/7525/48828: loss 2.5576, time 445.53ms\n",
      "iter 7600/7625/48828: loss 2.5614, time 118.19ms\n",
      "iter 7700/7725/48828: loss 2.5442, time 102.06ms\n",
      "iter 7800/7825/48828: loss 2.5513, time 200.10ms\n",
      "iter 7900/7925/48828: loss 2.5670, time 225.17ms\n",
      "step 8000: train loss 2.5363, val loss 2.5357\n",
      "iter 8000/8025/48828: loss 2.5383, time 4174.03ms\n",
      "iter 8100/8125/48828: loss 2.5429, time 204.94ms\n",
      "iter 8200/8225/48828: loss 2.5498, time 107.47ms\n",
      "iter 8300/8325/48828: loss 2.5572, time 148.42ms\n",
      "iter 8400/8425/48828: loss 2.5253, time 542.89ms\n",
      "iter 8500/8525/48828: loss 2.5483, time 481.21ms\n",
      "iter 8600/8625/48828: loss 2.5412, time 218.29ms\n",
      "iter 8700/8725/48828: loss 2.5080, time 318.32ms\n",
      "iter 8800/8825/48828: loss 2.5345, time 204.90ms\n",
      "iter 8900/8925/48828: loss 2.5265, time 214.91ms\n",
      "step 9000: train loss 2.5533, val loss 2.5532\n",
      "iter 9000/9025/48828: loss 2.5520, time 7228.29ms\n",
      "iter 9100/9125/48828: loss 2.5428, time 109.72ms\n",
      "iter 9200/9225/48828: loss 2.5258, time 511.35ms\n",
      "iter 9300/9325/48828: loss 2.5590, time 211.06ms\n",
      "iter 9400/9425/48828: loss 2.5459, time 125.26ms\n",
      "iter 9500/9525/48828: loss 2.5303, time 197.61ms\n",
      "iter 9600/9625/48828: loss 2.5083, time 212.73ms\n",
      "iter 9700/9725/48828: loss 2.5362, time 563.67ms\n",
      "iter 9800/9825/48828: loss 2.5336, time 184.56ms\n",
      "iter 9900/9925/48828: loss 2.5499, time 100.09ms\n",
      "step 10000: train loss 2.5332, val loss 2.5333\n",
      "iter 10000/10025/48828: loss 2.5328, time 4124.96ms\n",
      "iter 10100/10125/48828: loss 2.5357, time 106.80ms\n",
      "iter 10200/10225/48828: loss 2.5451, time 109.45ms\n",
      "iter 10300/10325/48828: loss 2.5192, time 445.93ms\n",
      "iter 10400/10425/48828: loss 2.5323, time 126.90ms\n",
      "iter 10500/10525/48828: loss 2.6044, time 109.35ms\n",
      "iter 10600/10625/48828: loss 2.5143, time 518.27ms\n",
      "iter 10700/10725/48828: loss 2.5423, time 123.85ms\n",
      "iter 10800/10825/48828: loss 2.5315, time 109.86ms\n",
      "iter 10900/10925/48828: loss 2.5280, time 212.90ms\n",
      "step 11000: train loss 2.5590, val loss 2.5585\n",
      "iter 11000/11025/48828: loss 2.5573, time 4515.65ms\n",
      "iter 11100/11125/48828: loss 2.5437, time 108.77ms\n",
      "iter 11200/11225/48828: loss 2.5322, time 120.33ms\n",
      "iter 11300/11325/48828: loss 2.5695, time 119.70ms\n",
      "iter 11400/11425/48828: loss 2.5334, time 110.47ms\n",
      "iter 11500/11525/48828: loss 2.5305, time 111.01ms\n",
      "iter 11600/11625/48828: loss 2.5520, time 123.69ms\n",
      "iter 11700/11725/48828: loss 2.5333, time 212.38ms\n",
      "iter 11800/11825/48828: loss 2.5328, time 201.26ms\n",
      "iter 11900/11925/48828: loss 2.5644, time 129.58ms\n",
      "step 12000: train loss 2.5432, val loss 2.5438\n",
      "iter 12000/12025/48828: loss 2.5405, time 4430.84ms\n",
      "iter 12100/12125/48828: loss 2.5514, time 109.70ms\n",
      "iter 12200/12225/48828: loss 2.5391, time 129.61ms\n",
      "iter 12300/12325/48828: loss 2.5440, time 150.96ms\n",
      "iter 12400/12425/48828: loss 2.5181, time 832.11ms\n",
      "iter 12500/12525/48828: loss 2.5326, time 126.53ms\n",
      "iter 12600/12625/48828: loss 2.5167, time 142.45ms\n",
      "iter 12700/12725/48828: loss 2.5463, time 121.54ms\n",
      "iter 12800/12825/48828: loss 2.5051, time 455.11ms\n",
      "iter 12900/12925/48828: loss 2.5285, time 139.94ms\n",
      "step 13000: train loss 2.5318, val loss 2.5319\n",
      "iter 13000/13025/48828: loss 2.5336, time 3978.61ms\n",
      "iter 13100/13125/48828: loss 2.5385, time 212.58ms\n",
      "iter 13200/13225/48828: loss 2.5240, time 104.96ms\n",
      "iter 13300/13325/48828: loss 2.5412, time 118.27ms\n",
      "iter 13400/13425/48828: loss 2.5214, time 141.60ms\n",
      "iter 13500/13525/48828: loss 2.5445, time 498.43ms\n",
      "iter 13600/13625/48828: loss 2.5569, time 124.45ms\n",
      "iter 13700/13725/48828: loss 2.5468, time 227.68ms\n",
      "iter 13800/13825/48828: loss 2.5555, time 233.62ms\n",
      "iter 13900/13925/48828: loss 2.5640, time 100.41ms\n",
      "step 14000: train loss 2.5259, val loss 2.5258\n",
      "iter 14000/14025/48828: loss 2.5218, time 3796.07ms\n",
      "iter 14100/14125/48828: loss 2.5789, time 113.20ms\n",
      "iter 14200/14225/48828: loss 2.5602, time 121.40ms\n",
      "iter 14300/14325/48828: loss 2.5447, time 131.18ms\n",
      "iter 14400/14425/48828: loss 2.5421, time 292.58ms\n",
      "iter 14500/14525/48828: loss 2.5362, time 118.14ms\n",
      "iter 14600/14625/48828: loss 2.5145, time 119.56ms\n",
      "iter 14700/14725/48828: loss 2.5470, time 107.72ms\n",
      "iter 14800/14825/48828: loss 2.5535, time 112.45ms\n",
      "iter 14900/14925/48828: loss 2.5360, time 115.00ms\n",
      "step 15000: train loss 2.5389, val loss 2.5388\n",
      "iter 15000/15025/48828: loss 2.5367, time 4519.73ms\n",
      "iter 15100/15125/48828: loss 2.5529, time 389.11ms\n",
      "iter 15200/15225/48828: loss 2.5445, time 108.21ms\n",
      "iter 15300/15325/48828: loss 2.5337, time 232.61ms\n",
      "iter 15400/15425/48828: loss 2.5256, time 100.36ms\n",
      "iter 15500/15525/48828: loss 2.5254, time 664.96ms\n",
      "iter 15600/15625/48828: loss 2.5600, time 110.71ms\n",
      "iter 15700/15725/48828: loss 2.5468, time 200.71ms\n",
      "iter 15800/15825/48828: loss 2.5257, time 296.68ms\n",
      "iter 15900/15925/48828: loss 2.5553, time 149.76ms\n",
      "step 16000: train loss 2.5578, val loss 2.5577\n",
      "iter 16000/16025/48828: loss 2.5558, time 4030.41ms\n",
      "iter 16100/16125/48828: loss 2.5535, time 114.04ms\n",
      "iter 16200/16225/48828: loss 2.5319, time 113.38ms\n",
      "iter 16300/16325/48828: loss 2.5426, time 112.14ms\n",
      "iter 16400/16425/48828: loss 2.5544, time 100.78ms\n",
      "iter 16500/16525/48828: loss 2.5723, time 103.32ms\n",
      "iter 16600/16625/48828: loss 2.5435, time 108.04ms\n",
      "iter 16700/16725/48828: loss 2.5654, time 415.41ms\n",
      "iter 16800/16825/48828: loss 2.5451, time 108.94ms\n",
      "iter 16900/16925/48828: loss 2.5638, time 217.60ms\n",
      "step 17000: train loss 2.5678, val loss 2.5678\n",
      "iter 17000/17025/48828: loss 2.5660, time 3760.74ms\n",
      "iter 17100/17125/48828: loss 2.5354, time 121.57ms\n",
      "iter 17200/17225/48828: loss 2.5756, time 109.08ms\n",
      "iter 17300/17325/48828: loss 2.5250, time 105.97ms\n",
      "iter 17400/17425/48828: loss 2.5359, time 124.71ms\n",
      "iter 17500/17525/48828: loss 2.5121, time 280.96ms\n",
      "iter 17600/17625/48828: loss 2.5022, time 208.04ms\n",
      "iter 17700/17725/48828: loss 2.5514, time 107.35ms\n",
      "iter 17800/17825/48828: loss 2.5396, time 103.99ms\n",
      "iter 17900/17925/48828: loss 2.5438, time 109.11ms\n",
      "step 18000: train loss 2.5432, val loss 2.5428\n",
      "iter 18000/18025/48828: loss 2.5434, time 4527.40ms\n",
      "iter 18100/18125/48828: loss 2.5411, time 123.63ms\n",
      "iter 18200/18225/48828: loss 2.5271, time 107.40ms\n",
      "iter 18300/18325/48828: loss 2.5200, time 105.29ms\n",
      "iter 18400/18425/48828: loss 2.5451, time 117.23ms\n",
      "iter 18500/18525/48828: loss 2.5572, time 107.26ms\n",
      "iter 18600/18625/48828: loss 2.5361, time 131.16ms\n",
      "iter 18700/18725/48828: loss 2.5150, time 204.06ms\n",
      "iter 18800/18825/48828: loss 2.5496, time 124.06ms\n",
      "iter 18900/18925/48828: loss 2.5303, time 117.90ms\n",
      "step 19000: train loss 2.5307, val loss 2.5317\n",
      "iter 19000/19025/48828: loss 2.5338, time 5261.13ms\n",
      "iter 19100/19125/48828: loss 2.5273, time 116.26ms\n",
      "iter 19200/19225/48828: loss 2.5102, time 106.29ms\n",
      "iter 19300/19325/48828: loss 2.5234, time 111.00ms\n",
      "iter 19400/19425/48828: loss 2.5392, time 361.15ms\n",
      "iter 19500/19525/48828: loss 2.5501, time 111.90ms\n",
      "iter 19600/19625/48828: loss 2.5519, time 202.34ms\n",
      "iter 19700/19725/48828: loss 2.5329, time 113.61ms\n",
      "iter 19800/19825/48828: loss 2.5141, time 341.22ms\n",
      "iter 19900/19925/48828: loss 2.5398, time 195.50ms\n",
      "step 20000: train loss 2.5350, val loss 2.5345\n",
      "iter 20000/20025/48828: loss 2.5262, time 5098.81ms\n",
      "iter 20100/20125/48828: loss 2.5395, time 125.66ms\n",
      "iter 20200/20225/48828: loss 2.5266, time 113.19ms\n",
      "iter 20300/20325/48828: loss 2.5552, time 114.42ms\n",
      "iter 20400/20425/48828: loss 2.5236, time 98.85ms\n",
      "iter 20500/20525/48828: loss 2.5543, time 115.07ms\n",
      "iter 20600/20625/48828: loss 2.5258, time 106.80ms\n",
      "iter 20700/20725/48828: loss 2.5284, time 153.37ms\n",
      "iter 20800/20825/48828: loss 2.5066, time 131.95ms\n",
      "iter 20900/20925/48828: loss 2.5264, time 97.83ms\n",
      "step 21000: train loss 2.5190, val loss 2.5192\n",
      "iter 21000/21025/48828: loss 2.5231, time 5688.79ms\n",
      "iter 21100/21125/48828: loss 2.5177, time 98.99ms\n",
      "iter 21200/21225/48828: loss 2.5215, time 108.79ms\n",
      "iter 21300/21325/48828: loss 2.5238, time 207.33ms\n",
      "iter 21400/21425/48828: loss 2.5405, time 500.71ms\n",
      "iter 21500/21525/48828: loss 2.5294, time 224.99ms\n",
      "iter 21600/21625/48828: loss 2.5571, time 116.85ms\n",
      "iter 21700/21725/48828: loss 2.5170, time 108.10ms\n",
      "iter 21800/21825/48828: loss 2.5083, time 170.35ms\n",
      "iter 21900/21925/48828: loss 2.5426, time 120.55ms\n",
      "step 22000: train loss 2.5392, val loss 2.5389\n",
      "iter 22000/22025/48828: loss 2.5425, time 5427.50ms\n",
      "iter 22100/22125/48828: loss 2.5495, time 499.74ms\n",
      "iter 22200/22225/48828: loss 2.5135, time 112.23ms\n",
      "iter 22300/22325/48828: loss 2.5222, time 206.46ms\n",
      "iter 22400/22425/48828: loss 2.5149, time 111.94ms\n",
      "iter 22500/22525/48828: loss 2.5180, time 497.99ms\n",
      "iter 22600/22625/48828: loss 2.5203, time 102.81ms\n",
      "iter 22700/22725/48828: loss 2.5066, time 102.89ms\n",
      "iter 22800/22825/48828: loss 2.5249, time 132.72ms\n",
      "iter 22900/22925/48828: loss 2.5372, time 127.73ms\n",
      "step 23000: train loss 2.5327, val loss 2.5322\n",
      "iter 23000/23025/48828: loss 2.5277, time 6804.25ms\n",
      "iter 23100/23125/48828: loss 2.5167, time 104.00ms\n",
      "iter 23200/23225/48828: loss 2.5110, time 100.20ms\n",
      "iter 23300/23325/48828: loss 2.5074, time 106.92ms\n",
      "iter 23400/23425/48828: loss 2.5515, time 140.00ms\n",
      "iter 23500/23525/48828: loss 2.5269, time 201.57ms\n",
      "iter 23600/23625/48828: loss 2.5364, time 120.85ms\n",
      "iter 23700/23725/48828: loss 2.5082, time 200.83ms\n",
      "iter 23800/23825/48828: loss 2.5279, time 132.84ms\n",
      "iter 23900/23925/48828: loss 2.4986, time 124.71ms\n",
      "step 24000: train loss 2.4952, val loss 2.4953\n",
      "iter 24000/24025/48828: loss 2.4907, time 3917.85ms\n",
      "iter 24100/24125/48828: loss 2.4755, time 101.56ms\n",
      "iter 24200/24225/48828: loss 2.5201, time 102.80ms\n",
      "iter 24300/24325/48828: loss 2.4971, time 103.28ms\n",
      "iter 24400/24425/48828: loss 2.5185, time 104.53ms\n",
      "iter 24500/24525/48828: loss 2.4991, time 101.99ms\n",
      "iter 24600/24625/48828: loss 2.5034, time 206.04ms\n",
      "iter 24700/24725/48828: loss 2.4969, time 179.64ms\n",
      "iter 24800/24825/48828: loss 2.5244, time 118.84ms\n",
      "iter 24900/24925/48828: loss 2.4976, time 128.35ms\n",
      "step 25000: train loss 2.5522, val loss 2.5520\n",
      "iter 25000/25025/48828: loss 2.5503, time 7137.83ms\n",
      "iter 25100/25125/48828: loss 2.5110, time 127.72ms\n",
      "iter 25200/25225/48828: loss 2.5284, time 112.18ms\n",
      "iter 25300/25325/48828: loss 2.4976, time 118.99ms\n",
      "iter 25400/25425/48828: loss 2.5170, time 112.52ms\n",
      "iter 25500/25525/48828: loss 2.5071, time 115.76ms\n",
      "iter 25600/25625/48828: loss 2.5304, time 99.31ms\n",
      "iter 25700/25725/48828: loss 2.4995, time 113.01ms\n",
      "iter 25800/25825/48828: loss 2.5486, time 117.37ms\n",
      "iter 25900/25925/48828: loss 2.5158, time 107.38ms\n",
      "step 26000: train loss 2.5062, val loss 2.5069\n",
      "iter 26000/26025/48828: loss 2.5034, time 4538.34ms\n",
      "iter 26100/26125/48828: loss 2.5013, time 112.98ms\n",
      "iter 26200/26225/48828: loss 2.5172, time 189.12ms\n",
      "iter 26300/26325/48828: loss 2.4870, time 105.20ms\n",
      "iter 26400/26425/48828: loss 2.4886, time 611.53ms\n",
      "iter 26500/26525/48828: loss 2.5285, time 107.33ms\n",
      "iter 26600/26625/48828: loss 2.5105, time 119.14ms\n",
      "iter 26700/26725/48828: loss 2.5111, time 107.67ms\n",
      "iter 26800/26825/48828: loss 2.5027, time 109.96ms\n",
      "iter 26900/26925/48828: loss 2.5271, time 113.78ms\n",
      "step 27000: train loss 2.4862, val loss 2.4853\n",
      "iter 27000/27025/48828: loss 2.4838, time 3349.95ms\n",
      "iter 27100/27125/48828: loss 2.5071, time 101.97ms\n",
      "iter 27200/27225/48828: loss 2.5153, time 112.83ms\n",
      "iter 27300/27325/48828: loss 2.5074, time 205.75ms\n",
      "iter 27400/27425/48828: loss 2.5162, time 102.70ms\n",
      "iter 27500/27525/48828: loss 2.5112, time 105.01ms\n",
      "iter 27600/27625/48828: loss 2.5269, time 473.67ms\n",
      "iter 27700/27725/48828: loss 2.5186, time 126.16ms\n",
      "iter 27800/27825/48828: loss 2.5477, time 100.29ms\n",
      "iter 27900/27925/48828: loss 2.5018, time 107.28ms\n",
      "step 28000: train loss 2.5012, val loss 2.5010\n",
      "iter 28000/28025/48828: loss 2.4980, time 3095.48ms\n",
      "iter 28100/28125/48828: loss 2.5239, time 98.04ms\n",
      "iter 28200/28225/48828: loss 2.5069, time 111.36ms\n",
      "iter 28300/28325/48828: loss 2.5207, time 108.37ms\n",
      "iter 28400/28425/48828: loss 2.5361, time 112.81ms\n",
      "iter 28500/28525/48828: loss 2.5294, time 104.36ms\n",
      "iter 28600/28625/48828: loss 2.5232, time 98.06ms\n",
      "iter 28700/28725/48828: loss 2.5060, time 4629.15ms\n",
      "iter 28800/28825/48828: loss 2.5190, time 118.02ms\n",
      "iter 28900/28925/48828: loss 2.5373, time 121.92ms\n",
      "step 29000: train loss 2.5150, val loss 2.5144\n",
      "iter 29000/29025/48828: loss 2.5160, time 3847.21ms\n",
      "iter 29100/29125/48828: loss 2.4809, time 106.27ms\n",
      "iter 29200/29225/48828: loss 2.4886, time 108.06ms\n",
      "iter 29300/29325/48828: loss 2.5126, time 105.12ms\n",
      "iter 29400/29425/48828: loss 2.5055, time 689.87ms\n",
      "iter 29500/29525/48828: loss 2.4780, time 670.00ms\n",
      "iter 29600/29625/48828: loss 2.5162, time 186.92ms\n",
      "iter 29700/29725/48828: loss 2.4886, time 110.87ms\n",
      "iter 29800/29825/48828: loss 2.4941, time 105.88ms\n",
      "iter 29900/29925/48828: loss 2.4855, time 107.14ms\n",
      "step 30000: train loss 2.4863, val loss 2.4851\n",
      "iter 30000/30025/48828: loss 2.4823, time 4477.93ms\n",
      "iter 30100/30125/48828: loss 2.4699, time 112.52ms\n",
      "iter 30200/30225/48828: loss 2.4678, time 107.44ms\n",
      "iter 30300/30325/48828: loss 2.4712, time 99.78ms\n",
      "iter 30400/30425/48828: loss 2.4662, time 109.79ms\n",
      "iter 30500/30525/48828: loss 2.4627, time 117.96ms\n",
      "iter 30600/30625/48828: loss 2.4567, time 2125.18ms\n",
      "iter 30700/30725/48828: loss 2.4936, time 125.25ms\n",
      "iter 30800/30825/48828: loss 2.4781, time 98.20ms\n",
      "iter 30900/30925/48828: loss 2.5246, time 108.72ms\n",
      "step 31000: train loss 2.4999, val loss 2.5000\n",
      "iter 31000/31025/48828: loss 2.4974, time 3644.61ms\n",
      "iter 31100/31125/48828: loss 2.4853, time 101.27ms\n",
      "iter 31200/31225/48828: loss 2.4793, time 99.55ms\n",
      "iter 31300/31325/48828: loss 2.4786, time 99.88ms\n",
      "iter 31400/31425/48828: loss 2.4853, time 143.40ms\n",
      "iter 31500/31525/48828: loss 2.4756, time 195.49ms\n",
      "iter 31600/31625/48828: loss 2.4772, time 114.37ms\n",
      "iter 31700/31725/48828: loss 2.4936, time 107.34ms\n",
      "iter 31800/31825/48828: loss 2.4574, time 111.42ms\n",
      "iter 31900/31925/48828: loss 2.4825, time 115.75ms\n",
      "step 32000: train loss 2.4918, val loss 2.4921\n",
      "iter 32000/32025/48828: loss 2.4915, time 3480.18ms\n",
      "iter 32100/32125/48828: loss 2.4815, time 108.49ms\n",
      "iter 32200/32225/48828: loss 2.4596, time 109.17ms\n",
      "iter 32300/32325/48828: loss 2.4761, time 213.84ms\n",
      "iter 32400/32425/48828: loss 2.4934, time 117.58ms\n",
      "iter 32500/32525/48828: loss 2.5090, time 122.61ms\n",
      "iter 32600/32625/48828: loss 2.4674, time 215.82ms\n",
      "iter 32700/32725/48828: loss 2.4685, time 119.13ms\n",
      "iter 32800/32825/48828: loss 2.4641, time 108.20ms\n",
      "iter 32900/32925/48828: loss 2.4611, time 108.45ms\n",
      "step 33000: train loss 2.4722, val loss 2.4718\n",
      "iter 33000/33025/48828: loss 2.4756, time 6417.46ms\n",
      "iter 33100/33125/48828: loss 2.4793, time 112.26ms\n",
      "iter 33200/33225/48828: loss 2.4924, time 208.14ms\n",
      "iter 33300/33325/48828: loss 2.4615, time 101.82ms\n",
      "iter 33400/33425/48828: loss 2.4777, time 637.14ms\n",
      "iter 33500/33525/48828: loss 2.4570, time 110.10ms\n",
      "iter 33600/33625/48828: loss 2.4759, time 104.81ms\n",
      "iter 33700/33725/48828: loss 2.4584, time 112.65ms\n",
      "iter 33800/33825/48828: loss 2.4667, time 108.94ms\n",
      "iter 33900/33925/48828: loss 2.4529, time 519.92ms\n",
      "step 34000: train loss 2.4707, val loss 2.4714\n",
      "iter 34000/34025/48828: loss 2.4738, time 7347.28ms\n",
      "iter 34100/34125/48828: loss 2.4798, time 107.88ms\n",
      "iter 34200/34225/48828: loss 2.4889, time 121.79ms\n",
      "iter 34300/34325/48828: loss 2.4863, time 107.64ms\n",
      "iter 34400/34425/48828: loss 2.4830, time 111.86ms\n",
      "iter 34500/34525/48828: loss 2.4834, time 109.80ms\n",
      "iter 34600/34625/48828: loss 2.4901, time 102.05ms\n",
      "iter 34700/34725/48828: loss 2.4644, time 100.74ms\n",
      "iter 34800/34825/48828: loss 2.4560, time 442.38ms\n",
      "iter 34900/34925/48828: loss 2.4843, time 111.21ms\n",
      "step 35000: train loss 2.4470, val loss 2.4474\n",
      "iter 35000/35025/48828: loss 2.4409, time 3125.86ms\n",
      "iter 35100/35125/48828: loss 2.4573, time 116.42ms\n",
      "iter 35200/35225/48828: loss 2.4574, time 104.92ms\n",
      "iter 35300/35325/48828: loss 2.4278, time 112.62ms\n",
      "iter 35400/35425/48828: loss 2.4583, time 204.87ms\n",
      "iter 35500/35525/48828: loss 2.4534, time 102.49ms\n",
      "iter 35600/35625/48828: loss 2.4514, time 109.78ms\n",
      "iter 35700/35725/48828: loss 2.4455, time 121.47ms\n",
      "iter 35800/35825/48828: loss 2.4469, time 106.98ms\n",
      "iter 35900/35925/48828: loss 2.4260, time 108.11ms\n",
      "step 36000: train loss 2.4477, val loss 2.4479\n",
      "iter 36000/36025/48828: loss 2.4488, time 8439.85ms\n",
      "iter 36100/36125/48828: loss 2.4523, time 107.26ms\n",
      "iter 36200/36225/48828: loss 2.4902, time 111.66ms\n",
      "iter 36300/36325/48828: loss 2.4506, time 108.03ms\n",
      "iter 36400/36425/48828: loss 2.4730, time 108.53ms\n",
      "iter 36500/36525/48828: loss 2.4693, time 526.79ms\n",
      "iter 36600/36625/48828: loss 2.4484, time 118.94ms\n",
      "iter 36700/36725/48828: loss 2.4419, time 101.27ms\n",
      "iter 36800/36825/48828: loss 2.4463, time 100.19ms\n",
      "iter 36900/36925/48828: loss 2.4492, time 106.71ms\n",
      "step 37000: train loss 2.4351, val loss 2.4360\n",
      "iter 37000/37025/48828: loss 2.4376, time 6504.53ms\n",
      "iter 37100/37125/48828: loss 2.4670, time 100.30ms\n",
      "iter 37200/37225/48828: loss 2.4446, time 507.11ms\n",
      "iter 37300/37325/48828: loss 2.4377, time 117.97ms\n",
      "iter 37400/37425/48828: loss 2.4298, time 116.72ms\n",
      "iter 37500/37525/48828: loss 2.4284, time 105.90ms\n",
      "iter 37600/37625/48828: loss 2.4354, time 103.49ms\n",
      "iter 37700/37725/48828: loss 2.4193, time 229.50ms\n",
      "iter 37800/37825/48828: loss 2.4624, time 235.13ms\n",
      "iter 37900/37925/48828: loss 2.4200, time 114.57ms\n",
      "step 38000: train loss 2.4088, val loss 2.4097\n",
      "iter 38000/38025/48828: loss 2.4110, time 3664.23ms\n",
      "iter 38100/38125/48828: loss 2.4448, time 110.28ms\n",
      "iter 38200/38225/48828: loss 2.4428, time 98.80ms\n",
      "iter 38300/38325/48828: loss 2.4653, time 105.37ms\n",
      "iter 38400/38425/48828: loss 2.4399, time 108.97ms\n",
      "iter 38500/38525/48828: loss 2.4174, time 101.35ms\n",
      "iter 38600/38625/48828: loss 2.4183, time 106.31ms\n",
      "iter 38700/38725/48828: loss 2.4179, time 107.43ms\n",
      "iter 38800/38825/48828: loss 2.4122, time 111.24ms\n",
      "iter 38900/38925/48828: loss 2.4108, time 103.91ms\n",
      "step 39000: train loss 2.4197, val loss 2.4201\n",
      "iter 39000/39025/48828: loss 2.4201, time 3639.72ms\n",
      "iter 39100/39125/48828: loss 2.4182, time 105.64ms\n",
      "iter 39200/39225/48828: loss 2.4090, time 108.88ms\n",
      "iter 39300/39325/48828: loss 2.4210, time 113.19ms\n",
      "iter 39400/39425/48828: loss 2.4197, time 112.34ms\n",
      "iter 39500/39525/48828: loss 2.4251, time 108.91ms\n",
      "iter 39600/39625/48828: loss 2.4406, time 109.50ms\n",
      "iter 39700/39725/48828: loss 2.3976, time 104.20ms\n",
      "iter 39800/39825/48828: loss 2.4122, time 114.76ms\n",
      "iter 39900/39925/48828: loss 2.4172, time 123.73ms\n",
      "step 40000: train loss 2.4135, val loss 2.4164\n",
      "iter 40000/40025/48828: loss 2.4215, time 4119.61ms\n",
      "iter 40100/40125/48828: loss 2.3859, time 104.36ms\n",
      "iter 40200/40225/48828: loss 2.3939, time 106.18ms\n",
      "iter 40300/40325/48828: loss 2.3966, time 109.44ms\n",
      "iter 40400/40425/48828: loss 2.3691, time 516.12ms\n",
      "iter 40500/40525/48828: loss 2.3855, time 106.20ms\n",
      "iter 40600/40625/48828: loss 2.4094, time 100.71ms\n",
      "iter 40700/40725/48828: loss 2.3871, time 211.34ms\n",
      "iter 40800/40825/48828: loss 2.3842, time 111.94ms\n",
      "iter 40900/40925/48828: loss 2.3740, time 108.65ms\n",
      "step 41000: train loss 2.3788, val loss 2.3795\n",
      "iter 41000/41025/48828: loss 2.3896, time 3623.69ms\n",
      "iter 41100/41125/48828: loss 2.3671, time 117.77ms\n",
      "iter 41200/41225/48828: loss 2.3759, time 108.41ms\n",
      "iter 41300/41325/48828: loss 2.4068, time 107.55ms\n",
      "iter 41400/41425/48828: loss 2.3947, time 112.06ms\n",
      "iter 41500/41525/48828: loss 2.3926, time 106.27ms\n",
      "iter 41600/41625/48828: loss 2.4096, time 112.43ms\n",
      "iter 41700/41725/48828: loss 2.3803, time 99.50ms\n",
      "iter 41800/41825/48828: loss 2.3841, time 119.16ms\n",
      "iter 41900/41925/48828: loss 2.3858, time 109.62ms\n",
      "step 42000: train loss 2.3881, val loss 2.3874\n",
      "iter 42000/42025/48828: loss 2.3799, time 3933.26ms\n",
      "iter 42100/42125/48828: loss 2.3755, time 199.85ms\n",
      "iter 42200/42225/48828: loss 2.3971, time 136.65ms\n",
      "iter 42300/42325/48828: loss 2.3752, time 107.52ms\n",
      "iter 42400/42425/48828: loss 2.4073, time 251.74ms\n",
      "iter 42500/42525/48828: loss 2.3691, time 108.29ms\n",
      "iter 42600/42625/48828: loss 2.3461, time 116.00ms\n",
      "iter 42700/42725/48828: loss 2.3453, time 108.39ms\n",
      "iter 42800/42825/48828: loss 2.3587, time 116.68ms\n",
      "iter 42900/42925/48828: loss 2.3369, time 264.99ms\n",
      "step 43000: train loss 2.3353, val loss 2.3368\n",
      "iter 43000/43025/48828: loss 2.3355, time 6064.14ms\n",
      "iter 43100/43125/48828: loss 2.3412, time 99.27ms\n",
      "iter 43200/43225/48828: loss 2.3649, time 111.82ms\n",
      "iter 43300/43325/48828: loss 2.3411, time 109.92ms\n",
      "iter 43400/43425/48828: loss 2.3236, time 139.66ms\n",
      "iter 43500/43525/48828: loss 2.3193, time 490.67ms\n",
      "iter 43600/43625/48828: loss 2.3421, time 115.80ms\n",
      "iter 43700/43725/48828: loss 2.3228, time 112.89ms\n",
      "iter 43800/43825/48828: loss 2.3001, time 207.34ms\n",
      "iter 43900/43925/48828: loss 2.3071, time 105.07ms\n",
      "step 44000: train loss 2.2991, val loss 2.2989\n",
      "iter 44000/44025/48828: loss 2.3087, time 4114.07ms\n",
      "iter 44100/44125/48828: loss 2.2924, time 111.64ms\n",
      "iter 44200/44225/48828: loss 2.2983, time 108.52ms\n",
      "iter 44300/44325/48828: loss 2.2990, time 110.45ms\n",
      "iter 44400/44425/48828: loss 2.2951, time 109.50ms\n",
      "iter 44500/44525/48828: loss 2.2787, time 110.70ms\n",
      "iter 44600/44625/48828: loss 2.2855, time 99.47ms\n",
      "iter 44700/44725/48828: loss 2.2840, time 225.83ms\n",
      "iter 44800/44825/48828: loss 2.2711, time 108.63ms\n",
      "iter 44900/44925/48828: loss 2.2891, time 102.34ms\n",
      "step 45000: train loss 2.2710, val loss 2.2706\n",
      "iter 45000/45025/48828: loss 2.2856, time 3175.00ms\n",
      "iter 45100/45125/48828: loss 2.2510, time 107.15ms\n",
      "iter 45200/45225/48828: loss 2.2732, time 109.74ms\n",
      "iter 45300/45325/48828: loss 2.2815, time 114.62ms\n",
      "iter 45400/45425/48828: loss 2.2689, time 106.63ms\n",
      "iter 45500/45525/48828: loss 2.2525, time 111.39ms\n",
      "iter 45600/45625/48828: loss 2.2546, time 112.34ms\n",
      "iter 45700/45725/48828: loss 2.2572, time 109.75ms\n",
      "iter 45800/45825/48828: loss 2.2444, time 105.55ms\n",
      "iter 45900/45925/48828: loss 2.2568, time 130.17ms\n",
      "step 46000: train loss 2.2585, val loss 2.2573\n",
      "iter 46000/46025/48828: loss 2.2497, time 4818.68ms\n",
      "iter 46100/46125/48828: loss 2.2367, time 125.76ms\n",
      "iter 46200/46225/48828: loss 2.2785, time 111.41ms\n",
      "iter 46300/46325/48828: loss 2.2409, time 2900.19ms\n",
      "iter 46400/46425/48828: loss 2.2303, time 100.61ms\n",
      "iter 46500/46525/48828: loss 2.2535, time 101.24ms\n",
      "iter 46600/46625/48828: loss 2.2429, time 102.92ms\n",
      "iter 46700/46725/48828: loss 2.2459, time 127.27ms\n",
      "iter 46800/46825/48828: loss 2.2352, time 109.29ms\n",
      "iter 46900/46925/48828: loss 2.2309, time 109.96ms\n",
      "step 47000: train loss 2.2340, val loss 2.2326\n",
      "iter 47000/47025/48828: loss 2.2253, time 3957.52ms\n",
      "iter 47100/47125/48828: loss 2.2393, time 99.32ms\n",
      "iter 47200/47225/48828: loss 2.2446, time 117.49ms\n",
      "iter 47300/47325/48828: loss 2.2331, time 106.25ms\n",
      "iter 47400/47425/48828: loss 2.2094, time 494.80ms\n",
      "iter 47500/47525/48828: loss 2.2305, time 100.88ms\n",
      "iter 47600/47625/48828: loss 2.2196, time 124.27ms\n",
      "iter 47700/47725/48828: loss 2.2212, time 99.35ms\n",
      "iter 47800/47825/48828: loss 2.2138, time 111.37ms\n",
      "iter 47900/47925/48828: loss 2.2306, time 108.25ms\n",
      "step 48000: train loss 2.2247, val loss 2.2260\n",
      "saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
      "iter 48000/48025/48828: loss 2.2309, time 7740.62ms\n",
      "iter 48100/48125/48828: loss 2.2101, time 107.68ms\n",
      "iter 48200/48225/48828: loss 2.2159, time 108.28ms\n",
      "iter 48300/48325/48828: loss 2.2190, time 110.28ms\n",
      "iter 48400/48425/48828: loss 2.1886, time 113.87ms\n",
      "iter 48500/48525/48828: loss 2.2077, time 107.32ms\n",
      "iter 48600/48625/48828: loss 2.1952, time 108.13ms\n",
      "iter 48700/48725/48828: loss 2.1891, time 109.94ms\n",
      "iter 48800/48825/48828: loss 2.1841, time 111.82ms\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, NUM_GENERATIONS+1):\n",
    "        current_model = model_dict[generation_id-1]\n",
    "        results_i, trajectory_path_i, model_i = await run_generation(current_model, num_games=NUM_GAMES, num_simulations=NUM_SIMULATIONS, generation_id=generation_id)\n",
    "        results_dict[generation_id] = results_i\n",
    "        trajectory_paths_dict[generation_id] = trajectory_path_i\n",
    "        model_dict[generation_id] = model_i\n",
    "\n",
    "## refactor, learning_rate = 0.05, warmup_iters=0\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/488: loss 2.7801, time 611.02ms\n",
    "# iter 100/105/488: loss 2.5840, time 63.73ms\n",
    "# iter 200/205/488: loss 2.5958, time 62.98ms\n",
    "# iter 300/305/488: loss 2.5835, time 60.15ms\n",
    "# iter 400/405/488: loss 2.5793, time 63.62ms\n",
    "\n",
    "# ## model = small\n",
    "# step 0: train loss 2.7741, val loss 2.7741\n",
    "# iter 0/5/488: loss 2.7743, time 1624.89ms\n",
    "# iter 100/105/488: loss 2.6157, time 141.39ms\n",
    "# iter 200/205/488: loss 2.6120, time 161.22ms\n",
    "# iter 300/305/488: loss 2.5983, time 203.82ms\n",
    "\n",
    "\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7801, val loss 2.7801\n",
    "# iter 0/5/4882: loss 2.7801, time 1422.53ms\n",
    "# iter 100/105/4882: loss 2.5970, time 110.71ms\n",
    "# iter 200/205/4882: loss 2.5962, time 116.96ms\n",
    "# iter 300/305/4882: loss 2.5917, time 160.95ms\n",
    "# iter 400/405/4882: loss 2.5885, time 63.37ms\n",
    "# iter 500/505/4882: loss 2.5912, time 65.25ms\n",
    "# iter 600/605/4882: loss 2.6000, time 67.49ms\n",
    "# iter 700/705/4882: loss 2.5780, time 61.43ms\n",
    "# iter 800/805/4882: loss 2.5864, time 265.56ms\n",
    "# iter 900/905/4882: loss 2.5857, time 263.09ms\n",
    "# step 1000: train loss 2.5849, val loss 2.5847\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 1000/1005/4882: loss 2.5844, time 1812.58ms\n",
    "# iter 1100/1105/4882: loss 2.5832, time 62.89ms\n",
    "# iter 1200/1205/4882: loss 2.5743, time 95.25ms\n",
    "# iter 1300/1305/4882: loss 2.5720, time 324.18ms\n",
    "# iter 1400/1405/4882: loss 2.5880, time 73.66ms\n",
    "# iter 1500/1505/4882: loss 2.5745, time 295.39ms\n",
    "# iter 1600/1605/4882: loss 2.5726, time 76.05ms\n",
    "# iter 1700/1705/4882: loss 2.5670, time 63.20ms\n",
    "# iter 1800/1805/4882: loss 2.5720, time 62.66ms\n",
    "# iter 1900/1905/4882: loss 2.5694, time 449.06ms\n",
    "# step 2000: train loss 2.5806, val loss 2.5806\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 2000/2005/4882: loss 2.5893, time 920.12ms\n",
    "# iter 2100/2105/4882: loss 2.5686, time 430.53ms\n",
    "# iter 2200/2205/4882: loss 2.5741, time 63.05ms\n",
    "# iter 2300/2305/4882: loss 2.5679, time 60.90ms\n",
    "# iter 2400/2405/4882: loss 2.5754, time 69.07ms\n",
    "# iter 2500/2505/4882: loss 2.5673, time 68.33ms\n",
    "# iter 2600/2605/4882: loss 2.5648, time 66.26ms\n",
    "# iter 2700/2705/4882: loss 2.5622, time 69.76ms\n",
    "# iter 2800/2805/4882: loss 2.5541, time 143.65ms\n",
    "# iter 2900/2905/4882: loss 2.5634, time 66.40ms\n",
    "# step 3000: train loss 2.5550, val loss 2.5547\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 3000/3005/4882: loss 2.5545, time 975.15ms\n",
    "# iter 3100/3105/4882: loss 2.5594, time 63.55ms\n",
    "# iter 3200/3205/4882: loss 2.5499, time 64.17ms\n",
    "# iter 3300/3305/4882: loss 2.5481, time 70.28ms\n",
    "# iter 3400/3405/4882: loss 2.5565, time 73.58ms\n",
    "# iter 3500/3505/4882: loss 2.5602, time 72.22ms\n",
    "# iter 3600/3605/4882: loss 2.5429, time 88.68ms\n",
    "# iter 3700/3705/4882: loss 2.5259, time 63.15ms\n",
    "# iter 3800/3805/4882: loss 2.5346, time 66.07ms\n",
    "# iter 3900/3905/4882: loss 2.5386, time 73.50ms\n",
    "# step 4000: train loss 2.5350, val loss 2.5345\n",
    "# saving checkpoint to /Users/rodo/src/rgi3/models/connect4-e2e/v1\n",
    "# iter 4000/4005/4882: loss 2.5424, time 1217.41ms\n",
    "# iter 4100/4105/4882: loss 2.5290, time 101.01ms\n",
    "# iter 4200/4205/4882: loss 2.5323, time 61.94ms\n",
    "# iter 4300/4305/4882: loss 2.5250, time 72.57ms\n",
    "# iter 4400/4405/4882: loss 2.5243, time 68.38ms\n",
    "# iter 4500/4505/4882: loss 2.5331, time 73.33ms\n",
    "# iter 4600/4605/4882: loss 2.5246, time 101.00ms\n",
    "# iter 4700/4705/4882: loss 2.5336, time 67.27ms\n",
    "# iter 4800/4805/4882: loss 2.5170, time 79.40ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Play single game\n",
    "result = await play_generation_async(current_model, num_games=1, simulations=NUM_SIMULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "td_array = [TrajectoryDataset(DATA_DIR, f\"gen-{generation_id}\", block_size=n_max_context) for generation_id in range(1, NUM_GENERATIONS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dd) = 401\n"
     ]
    }
   ],
   "source": [
    "# [td for td in td_array]\n",
    "unrolled = [(generation+1, d) for generation, td in enumerate(td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "        dd[tuple(tuple(d.action[:4].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(dd) = {len(dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prefix(model, game, prefix):\n",
    "    serial_evaluator = ActionHistoryTransformerEvaluator(model, device=device, block_size=block_size, vocab=action_vocab)\n",
    "    state = game.initial_state()\n",
    "    for action in prefix:\n",
    "        state = game.next_state(state, action)\n",
    "    legal_actions = game.legal_actions(state)\n",
    "    result = serial_evaluator.evaluate(game, state, legal_actions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([26450., 23550.]), win_pct=52.90%, sum=50000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "player_probs=[0.6513413 0.3486587]\n",
      "\n",
      "prefix=(0, 1)\n",
      "gen=*: tensor([1400., 1792.]), win_pct=43.86%, sum=3192.0\n",
      "gen=1: tensor([812., 752.]), win_pct=51.92%, sum=1564.0\n",
      "gen=2: tensor([319., 614.]), win_pct=34.19%, sum=933.0\n",
      "gen=3: tensor([41., 53.]), win_pct=43.62%, sum=94.0\n",
      "gen=4: tensor([54., 85.]), win_pct=38.85%, sum=139.0\n",
      "gen=5: tensor([174., 288.]), win_pct=37.66%, sum=462.0\n",
      "player_probs=[0.5495784 0.4504216]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([4801., 5671.]), win_pct=45.85%, sum=10472.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "player_probs=[0.68400687 0.31599316]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([2462., 2433.]), win_pct=50.30%, sum=4895.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "player_probs=[0.9123545  0.08764553]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([12812.,  8544.]), win_pct=59.99%, sum=21356.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "player_probs=[0.71874875 0.28125125]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([2250.5000, 2097.5000]), win_pct=51.76%, sum=4348.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "player_probs=[0.6177454  0.38225457]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([1168.,  996.]), win_pct=53.97%, sum=2164.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "player_probs=[0.69149137 0.30850863]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([1556.5000, 2016.5000]), win_pct=43.56%, sum=3573.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "player_probs=[0.552222   0.44777802]\n",
      "\n",
      "prefix=(0, 1, 1)\n",
      "gen=*: tensor([165., 120.]), win_pct=57.89%, sum=285.0\n",
      "gen=1: tensor([140., 105.]), win_pct=57.14%, sum=245.0\n",
      "gen=2: tensor([15.,  9.]), win_pct=62.50%, sum=24.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=5: tensor([8., 4.]), win_pct=66.67%, sum=12.0\n",
      "player_probs=[0.8551968  0.14480329]\n",
      "\n",
      "prefix=(0, 1, 2)\n",
      "gen=*: tensor([130., 104.]), win_pct=55.56%, sum=234.0\n",
      "gen=1: tensor([118.,  95.]), win_pct=55.40%, sum=213.0\n",
      "gen=2: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=3: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([3., 1.]), win_pct=75.00%, sum=4.0\n",
      "player_probs=[0.5910691 0.4089309]\n",
      "\n",
      "prefix=(0, 1, 3)\n",
      "gen=*: tensor([246., 255.]), win_pct=49.10%, sum=501.0\n",
      "gen=1: tensor([100., 104.]), win_pct=49.02%, sum=204.0\n",
      "gen=2: tensor([133., 138.]), win_pct=49.08%, sum=271.0\n",
      "gen=3: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([ 8., 10.]), win_pct=44.44%, sum=18.0\n",
      "player_probs=[0.6942675  0.30573246]\n",
      "\n",
      "prefix=(0, 1, 4)\n",
      "gen=*: tensor([385., 883.]), win_pct=30.36%, sum=1268.0\n",
      "gen=1: tensor([ 84., 150.]), win_pct=35.90%, sum=234.0\n",
      "gen=2: tensor([ 80., 355.]), win_pct=18.39%, sum=435.0\n",
      "gen=3: tensor([25., 38.]), win_pct=39.68%, sum=63.0\n",
      "gen=4: tensor([48., 81.]), win_pct=37.21%, sum=129.0\n",
      "gen=5: tensor([148., 259.]), win_pct=36.36%, sum=407.0\n",
      "player_probs=[0.62978816 0.3702118 ]\n",
      "\n",
      "prefix=(0, 1, 5)\n",
      "gen=*: tensor([180., 213.]), win_pct=45.80%, sum=393.0\n",
      "gen=1: tensor([105., 108.]), win_pct=49.30%, sum=213.0\n",
      "gen=2: tensor([63., 84.]), win_pct=42.86%, sum=147.0\n",
      "gen=3: tensor([8., 8.]), win_pct=50.00%, sum=16.0\n",
      "gen=4: tensor([2., 3.]), win_pct=40.00%, sum=5.0\n",
      "gen=5: tensor([ 2., 10.]), win_pct=16.67%, sum=12.0\n",
      "player_probs=[0.5877163  0.41228372]\n",
      "\n",
      "prefix=(0, 1, 6)\n",
      "gen=*: tensor([157., 116.]), win_pct=57.51%, sum=273.0\n",
      "gen=1: tensor([143., 100.]), win_pct=58.85%, sum=243.0\n",
      "gen=2: tensor([ 9., 14.]), win_pct=39.13%, sum=23.0\n",
      "gen=3: tensor([1., 2.]), win_pct=33.33%, sum=3.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([3., 0.]), win_pct=100.00%, sum=3.0\n",
      "player_probs=[0.60609543 0.3939046 ]\n",
      "\n",
      "prefix=(0, 1, 7)\n",
      "gen=*: tensor([137., 101.]), win_pct=57.56%, sum=238.0\n",
      "gen=1: tensor([122.,  90.]), win_pct=57.55%, sum=212.0\n",
      "gen=2: tensor([12.,  7.]), win_pct=63.16%, sum=19.0\n",
      "gen=3: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([2., 4.]), win_pct=33.33%, sum=6.0\n",
      "player_probs=[0.73934084 0.26065916]\n",
      "\n",
      "prefix=(0, 4, 1)\n",
      "gen=*: tensor([930., 311.]), win_pct=74.94%, sum=1241.0\n",
      "gen=1: tensor([183.,  44.]), win_pct=80.62%, sum=227.0\n",
      "gen=2: tensor([41.,  8.]), win_pct=83.67%, sum=49.0\n",
      "gen=3: tensor([129.,  23.]), win_pct=84.87%, sum=152.0\n",
      "gen=4: tensor([304., 210.]), win_pct=59.14%, sum=514.0\n",
      "gen=5: tensor([273.,  26.]), win_pct=91.30%, sum=299.0\n",
      "player_probs=[0.97509074 0.02490929]\n",
      "\n",
      "prefix=(0, 4, 2)\n",
      "gen=*: tensor([453., 143.]), win_pct=76.01%, sum=596.0\n",
      "gen=1: tensor([154.,  42.]), win_pct=78.57%, sum=196.0\n",
      "gen=2: tensor([47., 12.]), win_pct=79.66%, sum=59.0\n",
      "gen=3: tensor([67., 29.]), win_pct=69.79%, sum=96.0\n",
      "gen=4: tensor([109.,  46.]), win_pct=70.32%, sum=155.0\n",
      "gen=5: tensor([76., 14.]), win_pct=84.44%, sum=90.0\n",
      "player_probs=[0.57822275 0.42177722]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([529., 223.]), win_pct=70.35%, sum=752.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "player_probs=[0.6543235  0.34567645]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([9689., 7366.]), win_pct=56.81%, sum=17055.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "player_probs=[0.94337124 0.05662876]\n",
      "\n",
      "prefix=(0, 4, 5)\n",
      "gen=*: tensor([501., 260.]), win_pct=65.83%, sum=761.0\n",
      "gen=1: tensor([165.,  66.]), win_pct=71.43%, sum=231.0\n",
      "gen=2: tensor([50., 41.]), win_pct=54.95%, sum=91.0\n",
      "gen=3: tensor([82., 54.]), win_pct=60.29%, sum=136.0\n",
      "gen=4: tensor([42., 26.]), win_pct=61.76%, sum=68.0\n",
      "gen=5: tensor([162.,  73.]), win_pct=68.94%, sum=235.0\n",
      "player_probs=[0.87765527 0.12234473]\n",
      "\n",
      "prefix=(0, 4, 6)\n",
      "gen=*: tensor([312., 154.]), win_pct=66.95%, sum=466.0\n",
      "gen=1: tensor([155.,  45.]), win_pct=77.50%, sum=200.0\n",
      "gen=2: tensor([30., 34.]), win_pct=46.88%, sum=64.0\n",
      "gen=3: tensor([78., 54.]), win_pct=59.09%, sum=132.0\n",
      "gen=4: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=5: tensor([35., 15.]), win_pct=70.00%, sum=50.0\n",
      "player_probs=[0.76051176 0.23948824]\n",
      "\n",
      "prefix=(0, 4, 7)\n",
      "gen=*: tensor([398.,  87.]), win_pct=82.06%, sum=485.0\n",
      "gen=1: tensor([162.,  43.]), win_pct=79.02%, sum=205.0\n",
      "gen=2: tensor([32., 11.]), win_pct=74.42%, sum=43.0\n",
      "gen=3: tensor([125.,  22.]), win_pct=85.03%, sum=147.0\n",
      "gen=4: tensor([30.,  4.]), win_pct=88.24%, sum=34.0\n",
      "gen=5: tensor([49.,  7.]), win_pct=87.50%, sum=56.0\n",
      "player_probs=[0.84268564 0.15731436]\n"
     ]
    }
   ],
   "source": [
    "## Someting is borked? Player1 win percent should be much higher??\n",
    "def compare_model_vs_data(model, game, dd):\n",
    "    prefix_list = [\n",
    "        (0,), \n",
    "        (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7),\n",
    "        (0,1,1), (0,1,2), (0,1,3), (0,1,4), (0,1,5), (0,1,6), (0,1,7),\n",
    "        (0,4,1), (0,4,2), (0,4,3), (0,4,4), (0,4,5), (0,4,6), (0,4,7)]\n",
    "    for prefix in prefix_list:\n",
    "        print(f\"\\nprefix={prefix}\")\n",
    "        for gen, counts in dd[prefix].items():\n",
    "            print(f\"gen={gen}: {counts}, win_pct={100*counts[0]/sum(counts):.2f}%, sum={sum(counts)}\")\n",
    "        assert prefix[0] == 0\n",
    "        actions = prefix[1:]\n",
    "        eval_result = eval_prefix(model, game, actions)\n",
    "        # print(f'legal_policy={eval_result.legal_policy}')\n",
    "        # print(f'player_values={eval_result.player_values}')\n",
    "        print(f'player_probs={(eval_result.player_values+1)/2}')\n",
    "\n",
    "compare_model_vs_data(current_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model\n",
    "model_0 = create_random_model(model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "if RUN_GENERATIONS:\n",
    "    model_1 = load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Model 0\n",
      "Parameter containing:\n",
      "tensor([[-0.0504,  0.0093, -0.0226,  0.0165,  0.0072, -0.0277,  0.0177, -0.0237,\n",
      "          0.0188, -0.0143,  0.0269, -0.0013, -0.0198,  0.0144, -0.0243,  0.0237,\n",
      "         -0.0140,  0.0181,  0.0318, -0.0157,  0.0148, -0.0034, -0.0324,  0.0208,\n",
      "         -0.0291, -0.0305,  0.0342, -0.0172,  0.0260, -0.0208, -0.0124,  0.0096],\n",
      "        [-0.0126,  0.0072,  0.0104, -0.0769, -0.0078, -0.0041, -0.0216,  0.0517,\n",
      "          0.0385, -0.0091, -0.0166,  0.0048,  0.0219, -0.0037, -0.0307,  0.0049,\n",
      "         -0.0108,  0.0062,  0.0002, -0.0093, -0.0116, -0.0281, -0.0046, -0.0189,\n",
      "         -0.0464, -0.0034, -0.0101, -0.0152, -0.0383,  0.0229, -0.0058, -0.0105],\n",
      "        [-0.0208, -0.0252,  0.0038, -0.0199, -0.0087, -0.0136, -0.0072,  0.0004,\n",
      "         -0.0083,  0.0022, -0.0049,  0.0290,  0.0091, -0.0186, -0.0067,  0.0133,\n",
      "         -0.0194, -0.0004,  0.0097,  0.0346, -0.0028, -0.0301,  0.0015,  0.0175,\n",
      "         -0.0117,  0.0087,  0.0185, -0.0033,  0.0177,  0.0148,  0.0138,  0.0008],\n",
      "        [ 0.0108,  0.0246,  0.0061,  0.0229,  0.0009, -0.0005, -0.0093,  0.0311,\n",
      "          0.0120,  0.0404, -0.0377, -0.0150,  0.0187, -0.0209,  0.0130, -0.0071,\n",
      "         -0.0059, -0.0070,  0.0242,  0.0173,  0.0340,  0.0052, -0.0227,  0.0163,\n",
      "          0.0290, -0.0299,  0.0063,  0.0577, -0.0285,  0.0299, -0.0042,  0.0182],\n",
      "        [ 0.0079,  0.0110,  0.0027, -0.0343, -0.0018, -0.0030,  0.0095, -0.0341,\n",
      "         -0.0042, -0.0264,  0.0266,  0.0052,  0.0007, -0.0033, -0.0015, -0.0278,\n",
      "          0.0115, -0.0010,  0.0064, -0.0044, -0.0115,  0.0057, -0.0438, -0.0170,\n",
      "          0.0007,  0.0555, -0.0064, -0.0045,  0.0276, -0.0094, -0.0307, -0.0514],\n",
      "        [-0.0523, -0.0152, -0.0220,  0.0133, -0.0118,  0.0202,  0.0215, -0.0226,\n",
      "         -0.0149, -0.0153, -0.0252, -0.0042, -0.0080,  0.0028, -0.0132, -0.0119,\n",
      "         -0.0156, -0.0223,  0.0076,  0.0080, -0.0226, -0.0042, -0.0134, -0.0196,\n",
      "         -0.0076,  0.0077, -0.0115,  0.0192, -0.0142, -0.0291, -0.0335, -0.0355],\n",
      "        [ 0.0327, -0.0035, -0.0089, -0.0022,  0.0108, -0.0225,  0.0273,  0.0042,\n",
      "          0.0252, -0.0067,  0.0078, -0.0050, -0.0320,  0.0053, -0.0220,  0.0217,\n",
      "         -0.0161, -0.0067,  0.0006, -0.0307, -0.0253, -0.0320, -0.0005,  0.0118,\n",
      "          0.0088, -0.0154, -0.0236,  0.0061, -0.0021, -0.0011, -0.0321,  0.0027],\n",
      "        [-0.0200,  0.0198,  0.0210, -0.0304, -0.0015, -0.0245,  0.0166,  0.0256,\n",
      "         -0.0027,  0.0169, -0.0032, -0.0262,  0.0021,  0.0221,  0.0110, -0.0057,\n",
      "          0.0008, -0.0422,  0.0184, -0.0111,  0.0309,  0.0086, -0.0171,  0.0331,\n",
      "         -0.0144, -0.0149,  0.0199, -0.0022,  0.0037, -0.0067, -0.0106, -0.0266]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([26450., 23550.]), win_pct=52.90%, sum=50000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "player_probs=[0.55144787 0.4485522 ]\n",
      "\n",
      "prefix=(0, 1)\n",
      "gen=*: tensor([1400., 1792.]), win_pct=43.86%, sum=3192.0\n",
      "gen=1: tensor([812., 752.]), win_pct=51.92%, sum=1564.0\n",
      "gen=2: tensor([319., 614.]), win_pct=34.19%, sum=933.0\n",
      "gen=3: tensor([41., 53.]), win_pct=43.62%, sum=94.0\n",
      "gen=4: tensor([54., 85.]), win_pct=38.85%, sum=139.0\n",
      "gen=5: tensor([174., 288.]), win_pct=37.66%, sum=462.0\n",
      "player_probs=[0.55200404 0.4479959 ]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([4801., 5671.]), win_pct=45.85%, sum=10472.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "player_probs=[0.5518051 0.4481949]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([2462., 2433.]), win_pct=50.30%, sum=4895.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "player_probs=[0.5510513 0.4489487]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([12812.,  8544.]), win_pct=59.99%, sum=21356.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "player_probs=[0.5517535 0.4482465]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([2250.5000, 2097.5000]), win_pct=51.76%, sum=4348.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "player_probs=[0.55149066 0.44850934]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([1168.,  996.]), win_pct=53.97%, sum=2164.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "player_probs=[0.551355 0.448645]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([1556.5000, 2016.5000]), win_pct=43.56%, sum=3573.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "player_probs=[0.5521995  0.44780052]\n",
      "\n",
      "prefix=(0, 1, 1)\n",
      "gen=*: tensor([165., 120.]), win_pct=57.89%, sum=285.0\n",
      "gen=1: tensor([140., 105.]), win_pct=57.14%, sum=245.0\n",
      "gen=2: tensor([15.,  9.]), win_pct=62.50%, sum=24.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=5: tensor([8., 4.]), win_pct=66.67%, sum=12.0\n",
      "player_probs=[0.5526118  0.44738814]\n",
      "\n",
      "prefix=(0, 1, 2)\n",
      "gen=*: tensor([130., 104.]), win_pct=55.56%, sum=234.0\n",
      "gen=1: tensor([118.,  95.]), win_pct=55.40%, sum=213.0\n",
      "gen=2: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=3: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([3., 1.]), win_pct=75.00%, sum=4.0\n",
      "player_probs=[0.55249685 0.44750315]\n",
      "\n",
      "prefix=(0, 1, 3)\n",
      "gen=*: tensor([246., 255.]), win_pct=49.10%, sum=501.0\n",
      "gen=1: tensor([100., 104.]), win_pct=49.02%, sum=204.0\n",
      "gen=2: tensor([133., 138.]), win_pct=49.08%, sum=271.0\n",
      "gen=3: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([ 8., 10.]), win_pct=44.44%, sum=18.0\n",
      "player_probs=[0.55172354 0.4482765 ]\n",
      "\n",
      "prefix=(0, 1, 4)\n",
      "gen=*: tensor([385., 883.]), win_pct=30.36%, sum=1268.0\n",
      "gen=1: tensor([ 84., 150.]), win_pct=35.90%, sum=234.0\n",
      "gen=2: tensor([ 80., 355.]), win_pct=18.39%, sum=435.0\n",
      "gen=3: tensor([25., 38.]), win_pct=39.68%, sum=63.0\n",
      "gen=4: tensor([48., 81.]), win_pct=37.21%, sum=129.0\n",
      "gen=5: tensor([148., 259.]), win_pct=36.36%, sum=407.0\n",
      "player_probs=[0.55244166 0.44755837]\n",
      "\n",
      "prefix=(0, 1, 5)\n",
      "gen=*: tensor([180., 213.]), win_pct=45.80%, sum=393.0\n",
      "gen=1: tensor([105., 108.]), win_pct=49.30%, sum=213.0\n",
      "gen=2: tensor([63., 84.]), win_pct=42.86%, sum=147.0\n",
      "gen=3: tensor([8., 8.]), win_pct=50.00%, sum=16.0\n",
      "gen=4: tensor([2., 3.]), win_pct=40.00%, sum=5.0\n",
      "gen=5: tensor([ 2., 10.]), win_pct=16.67%, sum=12.0\n",
      "player_probs=[0.552145   0.44785497]\n",
      "\n",
      "prefix=(0, 1, 6)\n",
      "gen=*: tensor([157., 116.]), win_pct=57.51%, sum=273.0\n",
      "gen=1: tensor([143., 100.]), win_pct=58.85%, sum=243.0\n",
      "gen=2: tensor([ 9., 14.]), win_pct=39.13%, sum=23.0\n",
      "gen=3: tensor([1., 2.]), win_pct=33.33%, sum=3.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([3., 0.]), win_pct=100.00%, sum=3.0\n",
      "player_probs=[0.55202335 0.44797665]\n",
      "\n",
      "prefix=(0, 1, 7)\n",
      "gen=*: tensor([137., 101.]), win_pct=57.56%, sum=238.0\n",
      "gen=1: tensor([122.,  90.]), win_pct=57.55%, sum=212.0\n",
      "gen=2: tensor([12.,  7.]), win_pct=63.16%, sum=19.0\n",
      "gen=3: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([2., 4.]), win_pct=33.33%, sum=6.0\n",
      "player_probs=[0.5529134  0.44708654]\n",
      "\n",
      "prefix=(0, 4, 1)\n",
      "gen=*: tensor([930., 311.]), win_pct=74.94%, sum=1241.0\n",
      "gen=1: tensor([183.,  44.]), win_pct=80.62%, sum=227.0\n",
      "gen=2: tensor([41.,  8.]), win_pct=83.67%, sum=49.0\n",
      "gen=3: tensor([129.,  23.]), win_pct=84.87%, sum=152.0\n",
      "gen=4: tensor([304., 210.]), win_pct=59.14%, sum=514.0\n",
      "gen=5: tensor([273.,  26.]), win_pct=91.30%, sum=299.0\n",
      "player_probs=[0.55234253 0.44765753]\n",
      "\n",
      "prefix=(0, 4, 2)\n",
      "gen=*: tensor([453., 143.]), win_pct=76.01%, sum=596.0\n",
      "gen=1: tensor([154.,  42.]), win_pct=78.57%, sum=196.0\n",
      "gen=2: tensor([47., 12.]), win_pct=79.66%, sum=59.0\n",
      "gen=3: tensor([67., 29.]), win_pct=69.79%, sum=96.0\n",
      "gen=4: tensor([109.,  46.]), win_pct=70.32%, sum=155.0\n",
      "gen=5: tensor([76., 14.]), win_pct=84.44%, sum=90.0\n",
      "player_probs=[0.5522752 0.4477248]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([529., 223.]), win_pct=70.35%, sum=752.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "player_probs=[0.5514836  0.44851646]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([9689., 7366.]), win_pct=56.81%, sum=17055.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "player_probs=[0.5521419 0.4478581]\n",
      "\n",
      "prefix=(0, 4, 5)\n",
      "gen=*: tensor([501., 260.]), win_pct=65.83%, sum=761.0\n",
      "gen=1: tensor([165.,  66.]), win_pct=71.43%, sum=231.0\n",
      "gen=2: tensor([50., 41.]), win_pct=54.95%, sum=91.0\n",
      "gen=3: tensor([82., 54.]), win_pct=60.29%, sum=136.0\n",
      "gen=4: tensor([42., 26.]), win_pct=61.76%, sum=68.0\n",
      "gen=5: tensor([162.,  73.]), win_pct=68.94%, sum=235.0\n",
      "player_probs=[0.55190337 0.44809657]\n",
      "\n",
      "prefix=(0, 4, 6)\n",
      "gen=*: tensor([312., 154.]), win_pct=66.95%, sum=466.0\n",
      "gen=1: tensor([155.,  45.]), win_pct=77.50%, sum=200.0\n",
      "gen=2: tensor([30., 34.]), win_pct=46.88%, sum=64.0\n",
      "gen=3: tensor([78., 54.]), win_pct=59.09%, sum=132.0\n",
      "gen=4: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=5: tensor([35., 15.]), win_pct=70.00%, sum=50.0\n",
      "player_probs=[0.5517769 0.4482231]\n",
      "\n",
      "prefix=(0, 4, 7)\n",
      "gen=*: tensor([398.,  87.]), win_pct=82.06%, sum=485.0\n",
      "gen=1: tensor([162.,  43.]), win_pct=79.02%, sum=205.0\n",
      "gen=2: tensor([32., 11.]), win_pct=74.42%, sum=43.0\n",
      "gen=3: tensor([125.,  22.]), win_pct=85.03%, sum=147.0\n",
      "gen=4: tensor([30.,  4.]), win_pct=88.24%, sum=34.0\n",
      "gen=5: tensor([49.,  7.]), win_pct=87.50%, sum=56.0\n",
      "player_probs=[0.5526777 0.4473223]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n### Model 0\")\n",
    "print(model_0.action_embedding.weight)\n",
    "compare_model_vs_data(model_0, game, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Model 1\n",
      "Parameter containing:\n",
      "tensor([[ 0.1855, -0.1236,  0.0325, -0.2279, -0.1312,  0.2044, -0.0552,  0.0015,\n",
      "          0.0456, -0.0645, -0.2379,  0.0170,  0.0728,  0.1388, -0.0804,  0.0047,\n",
      "          0.2121, -0.1895,  0.0768,  0.0613,  0.1133, -0.3043,  0.0052, -0.1532,\n",
      "          0.2247,  0.2399,  0.0458,  0.2320,  0.2496, -0.0537, -0.2677, -0.1787],\n",
      "        [ 0.0808, -0.2869,  0.0688, -0.1646,  0.3144,  0.0958,  0.0340, -0.1392,\n",
      "         -0.2065,  0.4270,  0.2191, -0.1682,  0.0057, -0.0339, -0.0352,  0.3426,\n",
      "          0.0628,  0.1943, -0.1348, -0.0467, -0.1249, -0.0522,  0.3732,  0.0999,\n",
      "         -0.0559, -0.1346, -0.1585, -0.2288, -0.0584, -0.0580,  0.0284, -0.2018],\n",
      "        [ 0.3760, -0.0341, -0.0524, -0.1812, -0.2408,  0.1764, -0.0913, -0.1694,\n",
      "         -0.2916, -0.1422, -0.1412, -0.0781,  0.1165,  0.0633,  0.1347, -0.4096,\n",
      "          0.2124, -0.2153, -0.0055, -0.1609, -0.2353,  0.0733, -0.1883, -0.4946,\n",
      "          0.1866,  0.2968,  0.3776,  0.3554,  0.0164, -0.1493, -0.3089, -0.0679],\n",
      "        [ 0.0016, -0.0088, -0.1069,  0.1094, -0.0922,  0.3032,  0.2390,  0.1223,\n",
      "         -0.1586, -0.1466, -0.1003,  0.1502, -0.1688,  0.4107,  0.1677, -0.2314,\n",
      "          0.1650,  0.0888,  0.0102, -0.0915,  0.3839, -0.1758, -0.1703, -0.0917,\n",
      "          0.3358, -0.1317,  0.0204, -0.1294,  0.0792, -0.1334,  0.0147,  0.3009],\n",
      "        [-0.0675, -0.0137,  0.1669,  0.1069,  0.0721,  0.0373, -0.0502,  0.1817,\n",
      "          0.2545,  0.0166,  0.0391,  0.1686, -0.1655, -0.0686, -0.3643,  0.0639,\n",
      "         -0.0690,  0.0546,  0.1317,  0.0649,  0.0386,  0.0211,  0.1857,  0.0489,\n",
      "          0.0365, -0.0658, -0.2632, -0.1984, -0.0332,  0.0578,  0.0674,  0.0343],\n",
      "        [-0.1813, -0.1040,  0.0812, -0.1426,  0.0893,  0.0409, -0.1936,  0.0635,\n",
      "         -0.0032, -0.0580,  0.1889, -0.0017,  0.0574,  0.2297, -0.0126, -0.0326,\n",
      "          0.1255,  0.2326, -0.0110, -0.2326,  0.2620, -0.1879, -0.2292, -0.0136,\n",
      "         -0.0285, -0.1812, -0.0721, -0.2002,  0.0099, -0.0103, -0.1667,  0.1926],\n",
      "        [ 0.1855, -0.1004,  0.1512,  0.0503, -0.1972, -0.1217,  0.1700,  0.2461,\n",
      "         -0.1227,  0.4531, -0.0483,  0.2191, -0.1108,  0.1343, -0.1850,  0.1624,\n",
      "          0.1734,  0.2414, -0.1392,  0.2010,  0.0255, -0.0515,  0.0643,  0.0467,\n",
      "         -0.0635, -0.0821, -0.0450, -0.2129, -0.0438, -0.1449,  0.1395, -0.0261],\n",
      "        [-0.0468, -0.0246, -0.1006, -0.0404, -0.0264, -0.1215,  0.0276, -0.1783,\n",
      "         -0.0440,  0.0440, -0.0851, -0.2108,  0.0533, -0.1030,  0.3216,  0.0568,\n",
      "         -0.1618, -0.0998, -0.1731, -0.0034, -0.0830,  0.1363,  0.0584,  0.1357,\n",
      "         -0.1118,  0.0241,  0.0370,  0.0427, -0.0733, -0.0439,  0.0222, -0.1090]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "\n",
      "prefix=(0,)\n",
      "gen=*: tensor([26450., 23550.]), win_pct=52.90%, sum=50000.0\n",
      "gen=1: tensor([6193.5000, 3806.5000]), win_pct=61.94%, sum=10000.0\n",
      "gen=2: tensor([4645., 5355.]), win_pct=46.45%, sum=10000.0\n",
      "gen=3: tensor([5202.5000, 4797.5000]), win_pct=52.03%, sum=10000.0\n",
      "gen=4: tensor([4570., 5430.]), win_pct=45.70%, sum=10000.0\n",
      "gen=5: tensor([5839., 4161.]), win_pct=58.39%, sum=10000.0\n",
      "player_probs=[0.62489843 0.3751015 ]\n",
      "\n",
      "prefix=(0, 1)\n",
      "gen=*: tensor([1400., 1792.]), win_pct=43.86%, sum=3192.0\n",
      "gen=1: tensor([812., 752.]), win_pct=51.92%, sum=1564.0\n",
      "gen=2: tensor([319., 614.]), win_pct=34.19%, sum=933.0\n",
      "gen=3: tensor([41., 53.]), win_pct=43.62%, sum=94.0\n",
      "gen=4: tensor([54., 85.]), win_pct=38.85%, sum=139.0\n",
      "gen=5: tensor([174., 288.]), win_pct=37.66%, sum=462.0\n",
      "player_probs=[0.6101264  0.38987362]\n",
      "\n",
      "prefix=(0, 2)\n",
      "gen=*: tensor([4801., 5671.]), win_pct=45.85%, sum=10472.0\n",
      "gen=1: tensor([771., 573.]), win_pct=57.37%, sum=1344.0\n",
      "gen=2: tensor([ 66., 262.]), win_pct=20.12%, sum=328.0\n",
      "gen=3: tensor([35., 74.]), win_pct=32.11%, sum=109.0\n",
      "gen=4: tensor([3380., 4306.]), win_pct=43.98%, sum=7686.0\n",
      "gen=5: tensor([549., 456.]), win_pct=54.63%, sum=1005.0\n",
      "player_probs=[0.6239609 0.3760391]\n",
      "\n",
      "prefix=(0, 3)\n",
      "gen=*: tensor([2462., 2433.]), win_pct=50.30%, sum=4895.0\n",
      "gen=1: tensor([846., 451.]), win_pct=65.23%, sum=1297.0\n",
      "gen=2: tensor([723., 924.]), win_pct=43.90%, sum=1647.0\n",
      "gen=3: tensor([23., 80.]), win_pct=22.33%, sum=103.0\n",
      "gen=4: tensor([149., 200.]), win_pct=42.69%, sum=349.0\n",
      "gen=5: tensor([721., 778.]), win_pct=48.10%, sum=1499.0\n",
      "player_probs=[0.6251088  0.37489122]\n",
      "\n",
      "prefix=(0, 4)\n",
      "gen=*: tensor([12812.,  8544.]), win_pct=59.99%, sum=21356.0\n",
      "gen=1: tensor([1140.,  353.]), win_pct=76.36%, sum=1493.0\n",
      "gen=2: tensor([2451., 1809.]), win_pct=57.54%, sum=4260.0\n",
      "gen=3: tensor([4922., 4296.]), win_pct=53.40%, sum=9218.0\n",
      "gen=4: tensor([565., 317.]), win_pct=64.06%, sum=882.0\n",
      "gen=5: tensor([3734., 1769.]), win_pct=67.85%, sum=5503.0\n",
      "player_probs=[0.72290945 0.27709052]\n",
      "\n",
      "prefix=(0, 5)\n",
      "gen=*: tensor([2250.5000, 2097.5000]), win_pct=51.76%, sum=4348.0\n",
      "gen=1: tensor([1014.,  511.]), win_pct=66.49%, sum=1525.0\n",
      "gen=2: tensor([629., 879.]), win_pct=41.71%, sum=1508.0\n",
      "gen=3: tensor([66.5000, 65.5000]), win_pct=50.38%, sum=132.0\n",
      "gen=4: tensor([184., 135.]), win_pct=57.68%, sum=319.0\n",
      "gen=5: tensor([357., 507.]), win_pct=41.32%, sum=864.0\n",
      "player_probs=[0.6570532 0.3429469]\n",
      "\n",
      "prefix=(0, 6)\n",
      "gen=*: tensor([1168.,  996.]), win_pct=53.97%, sum=2164.0\n",
      "gen=1: tensor([869., 532.]), win_pct=62.03%, sum=1401.0\n",
      "gen=2: tensor([ 86., 244.]), win_pct=26.06%, sum=330.0\n",
      "gen=3: tensor([51., 97.]), win_pct=34.46%, sum=148.0\n",
      "gen=4: tensor([37., 57.]), win_pct=39.36%, sum=94.0\n",
      "gen=5: tensor([125.,  66.]), win_pct=65.45%, sum=191.0\n",
      "player_probs=[0.60984075 0.39015922]\n",
      "\n",
      "prefix=(0, 7)\n",
      "gen=*: tensor([1556.5000, 2016.5000]), win_pct=43.56%, sum=3573.0\n",
      "gen=1: tensor([741.5000, 634.5000]), win_pct=53.89%, sum=1376.0\n",
      "gen=2: tensor([371., 623.]), win_pct=37.32%, sum=994.0\n",
      "gen=3: tensor([ 64., 132.]), win_pct=32.65%, sum=196.0\n",
      "gen=4: tensor([201., 330.]), win_pct=37.85%, sum=531.0\n",
      "gen=5: tensor([179., 297.]), win_pct=37.61%, sum=476.0\n",
      "player_probs=[0.5766261 0.4233739]\n",
      "\n",
      "prefix=(0, 1, 1)\n",
      "gen=*: tensor([165., 120.]), win_pct=57.89%, sum=285.0\n",
      "gen=1: tensor([140., 105.]), win_pct=57.14%, sum=245.0\n",
      "gen=2: tensor([15.,  9.]), win_pct=62.50%, sum=24.0\n",
      "gen=3: tensor([2., 1.]), win_pct=66.67%, sum=3.0\n",
      "gen=4: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=5: tensor([8., 4.]), win_pct=66.67%, sum=12.0\n",
      "player_probs=[0.60820323 0.3917968 ]\n",
      "\n",
      "prefix=(0, 1, 2)\n",
      "gen=*: tensor([130., 104.]), win_pct=55.56%, sum=234.0\n",
      "gen=1: tensor([118.,  95.]), win_pct=55.40%, sum=213.0\n",
      "gen=2: tensor([7., 7.]), win_pct=50.00%, sum=14.0\n",
      "gen=3: tensor([0., 1.]), win_pct=0.00%, sum=1.0\n",
      "gen=4: tensor([2., 0.]), win_pct=100.00%, sum=2.0\n",
      "gen=5: tensor([3., 1.]), win_pct=75.00%, sum=4.0\n",
      "player_probs=[0.67654574 0.32345432]\n",
      "\n",
      "prefix=(0, 1, 3)\n",
      "gen=*: tensor([246., 255.]), win_pct=49.10%, sum=501.0\n",
      "gen=1: tensor([100., 104.]), win_pct=49.02%, sum=204.0\n",
      "gen=2: tensor([133., 138.]), win_pct=49.08%, sum=271.0\n",
      "gen=3: tensor([4., 3.]), win_pct=57.14%, sum=7.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([ 8., 10.]), win_pct=44.44%, sum=18.0\n",
      "player_probs=[0.44363832 0.5563617 ]\n",
      "\n",
      "prefix=(0, 1, 4)\n",
      "gen=*: tensor([385., 883.]), win_pct=30.36%, sum=1268.0\n",
      "gen=1: tensor([ 84., 150.]), win_pct=35.90%, sum=234.0\n",
      "gen=2: tensor([ 80., 355.]), win_pct=18.39%, sum=435.0\n",
      "gen=3: tensor([25., 38.]), win_pct=39.68%, sum=63.0\n",
      "gen=4: tensor([48., 81.]), win_pct=37.21%, sum=129.0\n",
      "gen=5: tensor([148., 259.]), win_pct=36.36%, sum=407.0\n",
      "player_probs=[0.36332536 0.6366746 ]\n",
      "\n",
      "prefix=(0, 1, 5)\n",
      "gen=*: tensor([180., 213.]), win_pct=45.80%, sum=393.0\n",
      "gen=1: tensor([105., 108.]), win_pct=49.30%, sum=213.0\n",
      "gen=2: tensor([63., 84.]), win_pct=42.86%, sum=147.0\n",
      "gen=3: tensor([8., 8.]), win_pct=50.00%, sum=16.0\n",
      "gen=4: tensor([2., 3.]), win_pct=40.00%, sum=5.0\n",
      "gen=5: tensor([ 2., 10.]), win_pct=16.67%, sum=12.0\n",
      "player_probs=[0.5398498  0.46015018]\n",
      "\n",
      "prefix=(0, 1, 6)\n",
      "gen=*: tensor([157., 116.]), win_pct=57.51%, sum=273.0\n",
      "gen=1: tensor([143., 100.]), win_pct=58.85%, sum=243.0\n",
      "gen=2: tensor([ 9., 14.]), win_pct=39.13%, sum=23.0\n",
      "gen=3: tensor([1., 2.]), win_pct=33.33%, sum=3.0\n",
      "gen=4: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([3., 0.]), win_pct=100.00%, sum=3.0\n",
      "player_probs=[0.6149815 0.3850185]\n",
      "\n",
      "prefix=(0, 1, 7)\n",
      "gen=*: tensor([137., 101.]), win_pct=57.56%, sum=238.0\n",
      "gen=1: tensor([122.,  90.]), win_pct=57.55%, sum=212.0\n",
      "gen=2: tensor([12.,  7.]), win_pct=63.16%, sum=19.0\n",
      "gen=3: tensor([1., 0.]), win_pct=100.00%, sum=1.0\n",
      "gen=5: tensor([2., 4.]), win_pct=33.33%, sum=6.0\n",
      "player_probs=[0.6397085  0.36029142]\n",
      "\n",
      "prefix=(0, 4, 1)\n",
      "gen=*: tensor([930., 311.]), win_pct=74.94%, sum=1241.0\n",
      "gen=1: tensor([183.,  44.]), win_pct=80.62%, sum=227.0\n",
      "gen=2: tensor([41.,  8.]), win_pct=83.67%, sum=49.0\n",
      "gen=3: tensor([129.,  23.]), win_pct=84.87%, sum=152.0\n",
      "gen=4: tensor([304., 210.]), win_pct=59.14%, sum=514.0\n",
      "gen=5: tensor([273.,  26.]), win_pct=91.30%, sum=299.0\n",
      "player_probs=[0.8271125 0.1728875]\n",
      "\n",
      "prefix=(0, 4, 2)\n",
      "gen=*: tensor([453., 143.]), win_pct=76.01%, sum=596.0\n",
      "gen=1: tensor([154.,  42.]), win_pct=78.57%, sum=196.0\n",
      "gen=2: tensor([47., 12.]), win_pct=79.66%, sum=59.0\n",
      "gen=3: tensor([67., 29.]), win_pct=69.79%, sum=96.0\n",
      "gen=4: tensor([109.,  46.]), win_pct=70.32%, sum=155.0\n",
      "gen=5: tensor([76., 14.]), win_pct=84.44%, sum=90.0\n",
      "player_probs=[0.7618813  0.23811871]\n",
      "\n",
      "prefix=(0, 4, 3)\n",
      "gen=*: tensor([529., 223.]), win_pct=70.35%, sum=752.0\n",
      "gen=1: tensor([140.,  58.]), win_pct=70.71%, sum=198.0\n",
      "gen=2: tensor([75., 40.]), win_pct=65.22%, sum=115.0\n",
      "gen=3: tensor([67., 75.]), win_pct=47.18%, sum=142.0\n",
      "gen=4: tensor([33.,  7.]), win_pct=82.50%, sum=40.0\n",
      "gen=5: tensor([214.,  43.]), win_pct=83.27%, sum=257.0\n",
      "player_probs=[0.68034434 0.31965563]\n",
      "\n",
      "prefix=(0, 4, 4)\n",
      "gen=*: tensor([9689., 7366.]), win_pct=56.81%, sum=17055.0\n",
      "gen=1: tensor([181.,  55.]), win_pct=76.69%, sum=236.0\n",
      "gen=2: tensor([2176., 1663.]), win_pct=56.68%, sum=3839.0\n",
      "gen=3: tensor([4374., 4039.]), win_pct=51.99%, sum=8413.0\n",
      "gen=4: tensor([33., 18.]), win_pct=64.71%, sum=51.0\n",
      "gen=5: tensor([2925., 1591.]), win_pct=64.77%, sum=4516.0\n",
      "player_probs=[0.6434554 0.3565446]\n",
      "\n",
      "prefix=(0, 4, 5)\n",
      "gen=*: tensor([501., 260.]), win_pct=65.83%, sum=761.0\n",
      "gen=1: tensor([165.,  66.]), win_pct=71.43%, sum=231.0\n",
      "gen=2: tensor([50., 41.]), win_pct=54.95%, sum=91.0\n",
      "gen=3: tensor([82., 54.]), win_pct=60.29%, sum=136.0\n",
      "gen=4: tensor([42., 26.]), win_pct=61.76%, sum=68.0\n",
      "gen=5: tensor([162.,  73.]), win_pct=68.94%, sum=235.0\n",
      "player_probs=[0.6791567  0.32084334]\n",
      "\n",
      "prefix=(0, 4, 6)\n",
      "gen=*: tensor([312., 154.]), win_pct=66.95%, sum=466.0\n",
      "gen=1: tensor([155.,  45.]), win_pct=77.50%, sum=200.0\n",
      "gen=2: tensor([30., 34.]), win_pct=46.88%, sum=64.0\n",
      "gen=3: tensor([78., 54.]), win_pct=59.09%, sum=132.0\n",
      "gen=4: tensor([14.,  6.]), win_pct=70.00%, sum=20.0\n",
      "gen=5: tensor([35., 15.]), win_pct=70.00%, sum=50.0\n",
      "player_probs=[0.76274836 0.23725167]\n",
      "\n",
      "prefix=(0, 4, 7)\n",
      "gen=*: tensor([398.,  87.]), win_pct=82.06%, sum=485.0\n",
      "gen=1: tensor([162.,  43.]), win_pct=79.02%, sum=205.0\n",
      "gen=2: tensor([32., 11.]), win_pct=74.42%, sum=43.0\n",
      "gen=3: tensor([125.,  22.]), win_pct=85.03%, sum=147.0\n",
      "gen=4: tensor([30.,  4.]), win_pct=88.24%, sum=34.0\n",
      "gen=5: tensor([49.,  7.]), win_pct=87.50%, sum=56.0\n",
      "player_probs=[0.81357634 0.18642372]\n"
     ]
    }
   ],
   "source": [
    "if RUN_GENERATIONS:\n",
    "    print(\"\\n\\n### Model 1\")\n",
    "    print(model_1.action_embedding.weight)\n",
    "    compare_model_vs_data(model_1, game, dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Convergence\n",
    "\n",
    "Synthetic sanity-check: train on a toy 2-step game where the first action strongly determines the winner. This verifies the value head and training loop can learn simple patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3998300578.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mxxx STOP HERE xxx\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "xxx STOP HERE xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = game.initial_state()\n",
    "all_actions_0 = game.all_actions()\n",
    "\n",
    "print(all_actions_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def play_random_game_with_fake_reward(game, max_actions) -> dict:\n",
    "    state = game.initial_state()\n",
    "    action_history = []\n",
    "    legal_policies = []\n",
    "    legal_action_idx_list = []\n",
    "\n",
    "    all_actions = game.all_actions()\n",
    "    all_action_idx_map = {action: idx for idx, action in enumerate(all_actions)}\n",
    "\n",
    "    num_actions = 0\n",
    "    while not game.is_terminal(state) and num_actions < max_actions:\n",
    "        current_player = game.current_player_id(state)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "        action_idx = random.randrange(len(legal_actions))\n",
    "        action = legal_actions[action_idx]\n",
    "\n",
    "        action_history.append(action)\n",
    "        legal_policies.append(np.ones(len(legal_actions))/len(legal_actions))\n",
    "        legal_action_idx = np.array([all_action_idx_map[action] for action in legal_actions])\n",
    "        legal_action_idx_list.append(legal_action_idx)\n",
    "\n",
    "        state = game.next_state(state, action)\n",
    "        num_actions += 1\n",
    "\n",
    "    # Determine outcome\n",
    "    fake_reward = np.mean(action_history) / len(legal_actions)\n",
    "    rewards = np.array([fake_reward, 1.0-fake_reward])\n",
    "    if fake_reward >= 0.5:\n",
    "        winner = 1\n",
    "    else:\n",
    "        winner = 2\n",
    "\n",
    "    return {\n",
    "        \"winner\": winner,\n",
    "        \"rewards\": rewards,\n",
    "        \"action_history\": action_history,\n",
    "        \"legal_policies\": legal_policies,\n",
    "        \"final_state\": state,\n",
    "        \"legal_action_idx\": legal_action_idx_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_game_with_fake_reward(game, max_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [play_random_game_with_fake_reward(game, max_actions=2) for _ in range(100_000)]\n",
    "print_game_stats(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gen_name = \"fake-0\"\n",
    "trajectory_path = write_trajectory_dataset(results, action_vocab, fake_gen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_model_config = model_config_dict[MODEL_SIZE]\n",
    "fake_model_config = model_config_dict[\"large\"]\n",
    "fake_model = create_random_model(fake_model_config, action_vocab_size=action_vocab.vocab_size, num_players=game.num_players(state_0), seed=42)\n",
    "\n",
    "training_splits = [f'gen-{fake_gen_name}']\n",
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "save_model(fake_model, fake_trainer, fake_gen_name)\n",
    "\n",
    "## model_size=tiny\n",
    "# num decayed parameter tensors: 11, with 1,968 parameters\n",
    "# num non-decayed parameter tensors: 7, with 50 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.7817, val loss 2.7816\n",
    "# iter 0/49/488: loss 2.7821, time 2537.56ms\n",
    "# iter 100/147/488: loss 2.6890, time 53.61ms\n",
    "# iter 200/245/488: loss 2.6342, time 63.05ms\n",
    "# iter 300/343/488: loss 2.6187, time 55.31ms\n",
    "# iter 400/441/488: loss 2.6147, time 61.11ms\n",
    "\n",
    "## model_size=large\n",
    "# num decayed parameter tensors: 35, with 1,579,776 parameters\n",
    "# num non-decayed parameter tensors: 19, with 2,186 parameters\n",
    "# using fused AdamW: False\n",
    "# step 0: train loss 2.8087, val loss 2.8088\n",
    "# iter 0/49/488: loss 2.8099, time 11225.20ms\n",
    "# iter 100/147/488: loss 2.6065, time 596.91ms\n",
    "# iter 200/245/488: loss 2.6075, time 618.00ms\n",
    "# iter 300/343/488: loss 2.6080, time 613.63ms\n",
    "# iter 400/441/488: loss 2.6051, time 616.39ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rerun in range(10):\n",
    "#     print(f\"Re-running training for {fake_gen_name} {rerun+1} of 10\")\n",
    "#     fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n",
    "#     save_model(fake_model, fake_trainer, fake_gen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [td for td in td_array]\n",
    "fake_td_array = [TrajectoryDataset(DATA_DIR, split, block_size=n_max_context) for split in training_splits]\n",
    "fake_unrolled = [(generation+1, d) for generation, td in enumerate(fake_td_array) for d in td]\n",
    "\n",
    "# gen, d = unrolled[0], \n",
    "# d.action[:2]\n",
    "# d.value[0]\n",
    "\n",
    "# Inspect training data\n",
    "fake_dd = defaultdict(lambda: defaultdict(lambda: torch.tensor([0., 0.])))\n",
    "\n",
    "for gen, d in fake_unrolled:\n",
    "    for g in ['*', gen]:    \n",
    "        fake_dd[tuple(tuple(d.action[:0].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:1].tolist()))][g] += d.value[0]\n",
    "        fake_dd[tuple(tuple(d.action[:2].tolist()))][g] += d.value[0]\n",
    "        # fake_dd[tuple(tuple(d.action[:3].tolist()))][g] += d.value[0]\n",
    "\n",
    "print(f\"len(fake_dd) = {len(fake_dd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model = load_model(fake_gen_name)\n",
    "compare_model_vs_data(fake_model, game, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_model, fake_trainer = train_model(fake_model, training_splits, train_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
