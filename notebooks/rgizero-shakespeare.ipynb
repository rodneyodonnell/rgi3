{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LLM on shakespeare data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "import requests\n",
    "from rgi.rgizero import common\n",
    "\n",
    "DATA_DIR = common.data_dir(\"shakespeare-char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_path = DATA_DIR / \"raw_text.txt\"\n",
    "if not os.path.exists(raw_text_path):\n",
    "    data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    with open(raw_text_path, \"w\") as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def split_dataset(dataset: Dataset, train_split: float = 0.9) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"Split dataset into train and validation sets.\"\"\"\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    train_dataset = torch.utils.data.Subset(dataset, list(range(train_size)))\n",
    "    validation_dataset = torch.utils.data.Subset(dataset, list(range(train_size, len(dataset))))\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.text_dataset import SimpleTextDataset\n",
    "\n",
    "BLOCK_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_DATALOADER_WORKERS = 0\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "with open(raw_text_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "text_dataset = SimpleTextDataset(raw_text, BLOCK_SIZE, device=DEVICE)\n",
    "vocab_size = text_dataset.vocab_size\n",
    "train_dataset, val_dataset = split_dataset(text_dataset, 0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_DATALOADER_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_DATALOADER_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.train import Trainer, TrainConfig\n",
    "from rgi.rgizero.models.transformer import TransformerConfig\n",
    "from rgi.rgizero.models.token_transformer import TokenTransformer\n",
    "\n",
    "\n",
    "# model_config = TransformerConfig(n_max_context=BLOCK_SIZE, n_embd=128, n_layer=4, n_head=4, dropout=0.2)  # femto\n",
    "model_config = TransformerConfig(n_max_context=BLOCK_SIZE, n_embd=384, n_layer=6, n_head=6, dropout=0.2)  # baby\n",
    "model = TokenTransformer(model_config, vocab_size)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainConfig with overrides based on train_shakespeare_char.py\n",
    "base_shakespeare_train_config = TrainConfig(\n",
    "    model_name=\"shakespeare-gpt\",\n",
    "    model_version=\"v1\",\n",
    "\n",
    "    eval_interval = 250,  # keep frequent because we'll overfit\n",
    "    eval_iters = 200,\n",
    "    log_interval = 10,  # don't print too too often\n",
    "\n",
    "    # we expect to overfit on this small dataset, so only save when val improves\n",
    "    always_save_checkpoint = False,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    batch_size = 64,\n",
    "\n",
    "    learning_rate = 1e-3,  # with baby networks can afford to go a bit higher\n",
    "    max_iters = 5000,\n",
    "    lr_decay_iters = 5000,  # make equal to max_iters usually\n",
    "    min_lr = 1e-4,  # learning_rate / 10 usually\n",
    "    beta2 = 0.99,  # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    warmup_iters = 100,  # not super necessary potentially\n",
    ")\n",
    "\n",
    "baby_shakespeare_train_config = base_shakespeare_train_config.__replace__(\n",
    "    max_iters=5000,\n",
    "    eval_interval=50,\n",
    "    eval_iters=10,\n",
    "    log_interval=10,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "femto_shakespeare_train_config = baby_shakespeare_train_config.__replace__(\n",
    "    # femto GPT model, much smaller than baby model.\n",
    "    warmup_iters=10,\n",
    "    max_iters=100,\n",
    "    compile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "train_config = femto_shakespeare_train_config\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "PROFILER = None # \"prun\"  # None, \"prun\", \"torch\"\n",
    "\n",
    "if PROFILER is None:\n",
    "    trainer.train()\n",
    "elif PROFILER == \"prun\":\n",
    "    _pstats = %prun -r -l 30 -s cumulative trainer.train()\n",
    "    # _pstats.print_stats('_nn')\n",
    "elif PROFILER == \"torch\":\n",
    "    from torch.profiler import profile, record_function, ProfilerActivity\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        with record_function(\"training_step\"):\n",
    "            trainer.train()\n",
    "    \n",
    "    print(\"\\nPreparing profiler output ...\")  # why is this so slow?\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"\\n\"\n",
    "    x = torch.tensor([text_dataset.encode(context)], dtype=torch.long, device=DEVICE)\n",
    "    y = model.generate(x, max_new_tokens=50)\n",
    "    print(text_dataset.decode(y[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
