{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step run of alphazero self-play & training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Game and players\n",
    "from rgi.rgizero.experiment import ExperimentRunner, ExperimentConfig\n",
    "from rgi.rgizero.data.trajectory_dataset import Vocab, print_dataset_stats, TrajectoryDataset\n",
    "from rgi.rgizero.evaluators import ActionHistoryTransformerEvaluator, AsyncNetworkEvaluator\n",
    "from rgi.rgizero.models.tuner import create_random_model\n",
    "\n",
    "import notebook_utils\n",
    "from notebook_utils import reload_local_modules\n",
    "\n",
    "device = notebook_utils.detect_device()\n",
    "\n",
    "## Disable for debugger stability?\n",
    "# # Allow asyncio to work with jupyter notebook\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# Increase numpy print width\n",
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GENERATIONS = True\n",
    "\n",
    "\n",
    "# Create Experiment Config\n",
    "experiment_config = ExperimentConfig(\n",
    "    experiment_name='smoketest-e2e-v2',\n",
    "    parent_experiment_name='smoketest-e2e',\n",
    "    game_name='connect4',\n",
    "    num_generations=20,\n",
    "    num_games_per_gen=10_000,\n",
    "    num_simulations=50,\n",
    "    model_size=\"tiny\",\n",
    "    train_batch_size=10,\n",
    "    max_training_epochs=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Tuned params from connect4 with 23k training games.\n",
    "tuned_params = {\n",
    "    'batch_size': 256,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'bias': True,\n",
    "    'decay_lr': True,\n",
    "    'dropout': 0.0,\n",
    "    'dtype': 'float16',\n",
    "    'grad_clip': 1.0,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 0.002,\n",
    "    'lr_decay_iters': 1000,\n",
    "    'max_epochs': 1000000,\n",
    "    'max_iters': 1000,\n",
    "    'min_lr': 0.0002,\n",
    "    'n_embd': 256,\n",
    "    'n_head': 2,\n",
    "    'n_layer': 3,\n",
    "    'n_max_context': 44,\n",
    "    'warmup_iters': 100,  # TODO: Tuner says this should be 500? That seems high...\n",
    "    'weight_decay': 0.1,\n",
    "    # 'model_name': 'c4-smoketest',\n",
    "    # 'model_version': '0.1',\n",
    "    # 'num_players': 2,\n",
    "    # 'vocab_size': 8,\n",
    "    # 'dataset_paths': (\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'),\n",
    "    #     PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5')\n",
    "    #     ),\n",
    "    'eval_iters': 200,\n",
    "    'log_interval': 1000,\n",
    "    'eval_interval': 10000,\n",
    "    # 'device': 'mps',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up game and experiment runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.rgizero.data.trajectory_dataset import Vocab\n",
    "from rgi.rgizero.common import TOKENS\n",
    "\n",
    "# Initialize Experiment Runner\n",
    "experiment_base_dir = Path.cwd().parent / 'experiments'\n",
    "experiment_runner = ExperimentRunner(experiment_config, experiment_base_dir, training_args=tuned_params)\n",
    "game = experiment_runner.game\n",
    "action_vocab = experiment_runner.action_vocab\n",
    "n_max_context = experiment_runner.n_max_context\n",
    "\n",
    "DATA_DIR = experiment_runner.data_dir\n",
    "MODEL_DIR = experiment_runner.models_dir\n",
    "\n",
    "print('âœ… Runner initialized')\n",
    "print(f'Game: {experiment_runner.config.game_name}, Players: {experiment_runner.num_players}, Actions: {list(game.base_game.all_actions())}')\n",
    "print('Data dir: ', DATA_DIR)\n",
    "print('Model dir: ', MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize (creates Random Gen 0 if needed)\n",
    "model_0 = experiment_runner.initialize()\n",
    "current_model = model_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GENERATIONS = False\n",
    "results_dict = {}\n",
    "trajectory_paths_dict = {}\n",
    "model_dict = {0: model_0}\n",
    "\n",
    "current_model = model_dict[0]\n",
    "if RUN_GENERATIONS:\n",
    "    for generation_id in range(1, experiment_config.num_generations+1):\n",
    "        current_model = await experiment_runner.run_generation_step_async(generation_id, current_model)\n",
    "        dataset_paths = experiment_runner.get_trajectory_paths(generation_id)\n",
    "        \n",
    "        # print stats for visibility\n",
    "        print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=current_model, game=game)\n",
    "        \n",
    "        model_dict[generation_id] = current_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model (initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "state_0 = game.initial_state()\n",
    "NUM_GENERATIONS = 5\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Parameters which will never be used for tuning.\n",
    "fixed_params = dict(\n",
    "    model_name='c4-smoketest',\n",
    "    model_version='0.1',\n",
    "    num_players = game.num_players(state_0),\n",
    "    vocab_size = action_vocab.vocab_size,\n",
    "    dataset_paths = tuple(experiment_runner.get_trajectory_paths(experiment_config.num_generations)),\n",
    "\n",
    "\n",
    "    eval_iters = 200,\n",
    "    log_interval = 1000,\n",
    "    eval_interval = 10_000,\n",
    "\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "initial_params = dict(\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    n_embd=8,  # tiny model\n",
    "\n",
    "    n_max_context=n_max_context,\n",
    "    batch_size = 32,\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    max_iters=100,\n",
    "    max_epochs=1_000_000, # Make max_epoch high, rely on max_iters to stop.\n",
    "        \n",
    "    learning_rate = LEARNING_RATE,    \n",
    "    decay_lr = True,  # whether to decay the learning rate\n",
    "    lr_decay_iters = 100,  # make equal to max_iters usually\n",
    "    min_lr = LEARNING_RATE / 10,  # learning_rate / 10 usually\n",
    "    warmup_iters = 0,  # not super necessary potentially\n",
    "\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    grad_clip = 1.0,  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = \"float16\",\n",
    "\n",
    "    dropout = 0.0,\n",
    "    bias = False,  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    last_file = None,   # Used in tuning key only.\n",
    ")\n",
    "\n",
    "tune_options = dict(\n",
    "    n_layer = [1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 32],\n",
    "    # n_head = [1, 2, 4, 8, 16, 32],   # Needs to be calcualted to ensure n_embed % n_head == 0\n",
    "    n_embd = [8, 16, 32, 64, 128, 256, 512, 1024, 2048],\n",
    "\n",
    "    n_max_context = [initial_params['n_max_context']],\n",
    "    batch_size = [16, 32, 64, 128, 256, 512, 1024],\n",
    "    gradient_accumulation_steps = [1],  # TODO: We only support 1 for now. This fails is we don't have an exact multiple of the batch size per epoch.\n",
    "\n",
    "    max_iters = [100, 300, 1_000, 3_000, 5_000, 10_000, 30_000, 100_000, 300_000],\n",
    "    max_epochs = [1_000_000], # Make max_epoch high, rely on max_iters to stop.\n",
    " \n",
    "    learning_rate = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "    decay_lr = [False, True],\n",
    "\n",
    "    # TODO: What is a sensible range here?\n",
    "    beta1 = [0.90, 0.95, 0.99],\n",
    "    beta2 = [0.95, 0.98, 0.99],\n",
    "\n",
    "    weight_decay = [0.01, 0.05, 0.1, 0.2],\n",
    "    grad_clip = [0,0, 1.0],  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "    dtype = [\"bfloat16\", \"float16\"],\n",
    "    dropout = [0.0, 0.01, 0.02, 0.05, 0.1],\n",
    "    bias = [True, False],    \n",
    ")\n",
    "\n",
    "_n_head_options = [1, 2, 4, 8, 16, 32]\n",
    "computed_tune_options = dict(\n",
    "    min_lr = lambda opt: [opt['learning_rate'] / 10],\n",
    "    lr_decay_iters = lambda opt: [opt['max_iters']],\n",
    "    warmup_iters = lambda opt: [x for x in [0, 100, 500, 1000] if x < opt['lr_decay_iters']] if opt['decay_lr'] else [0],\n",
    "    n_head = lambda opt: [n for n in _n_head_options if opt['n_embd'] % n == 0],\n",
    "    last_file = lambda opt: [str(opt['dataset_paths'][-1])],\n",
    ")\n",
    "\n",
    "TUNER_VERSION = \"0.0.4-smoketest\"\n",
    "\n",
    "from rgi.rgizero.models.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "_ = tuner.autotune_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "tuner = Tuner(\n",
    "    fixed_params=fixed_params.copy(),\n",
    "    initial_params=initial_params.copy(),\n",
    "    tune_options=tune_options.copy(), \n",
    "    computed_tune_options=computed_tune_options.copy(),\n",
    "    cache_version=TUNER_VERSION,\n",
    "    target_improvement_per_minute=0.001)\n",
    "\n",
    "tuner_result = tuner.autotune_smart()\n",
    "# print(f'tuner_result={tuner_result}')\n",
    "\n",
    "best_params = tuner.best_params.copy()\n",
    "## Recalculating with best_params = {'batch_size': 512, 'beta1': 0.9, 'beta2': 0.99, 'bias': False, 'decay_lr': True, 'dropout': 0.0, 'dtype': 'float16', 'grad_clip': 1.0, 'gradient_accumulation_steps': 1, 'last_file': '/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20', 'learning_rate': 0.01, 'lr_decay_iters': 5000, 'max_epochs': 1000000, 'max_iters': 30000, 'min_lr': 0.001, 'n_embd': 64, 'n_head': 8, 'n_layer': 4, 'n_max_context': 44, 'warmup_iters': 1000, 'weight_decay': 0.2, 'model_name': 'c4-smoketest', 'model_version': '0.1', 'num_players': 2, 'vocab_size': 8, 'dataset_paths': (PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-4'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-5'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-6'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-7'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-8'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-9'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-10'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-11'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-12'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-13'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-14'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-15'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-16'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-17'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-18'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-19'), PosixPath('/Users/rodo/src/rgi3-sync/experiments/smoketest-e2e-v2/data/gen-20')), 'eval_iters': 200, 'log_interval': 1000, 'eval_interval': 10000, 'device': 'mps'}\n",
    "## {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "\n",
    "# best_params['max_iters'] = 30_000 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "# best_params['max_iters'] = 10_000 #{'train': 2.2972692108154296, 'train_policy_loss': 1.7478227978944778, 'train_value_loss': 0.5494464221596718, 'val': 2.3170768583522126, 'val_policy_loss': 1.7486604136579178, 'val_value_loss': 0.5684164271635168, 'elapsed': 529.6186480522156, 'param_hash': 'c81f68b9b00650b83a229a76b93d77d51f7bd1af43148f6c1af75e8f562f2670'}\n",
    "# best_params['max_iters'] = 5000 # {'train': 2.3059648656845093, 'train_policy_loss': 1.749609624147415, 'train_value_loss': 0.5563552376627922, 'val': 2.319357395172119, 'val_policy_loss': 1.7505432332263273, 'val_value_loss': 0.5688141549334806, 'elapsed': 267.94705629348755, 'param_hash': 'a1194e1f289d35c51e5a0f01692109358f947530b50dd54364f01baefdb6bedf'}\n",
    "# best_params['max_iters'] = 3000 # {'train': 2.3192384707927705, 'train_policy_loss': 1.752758464217186, 'train_value_loss': 0.5664800041913987, 'val': 2.326388120651245, 'val_policy_loss': 1.7533490412375505, 'val_value_loss': 0.5730390969444724, 'elapsed': 172.16889691352844, 'param_hash': '9e0204edd0b23fd54025b0c4de66c870a268b95197e0e47c4460ec03875b3dcb'}\n",
    "# best_params['max_iters'] = 1000\n",
    "best_params['max_iters'] = 3000\n",
    "\n",
    "# best_params['learning_rate'] = 0.01 # {'train': 2.2821048521995544, 'train_policy_loss': 1.746874241232872, 'train_value_loss': 0.5352306108176709, 'val': 2.324920549112208, 'val_policy_loss': 1.7480337900273941, 'val_value_loss': 0.5768867538255804, 'elapsed': 1651.836928844452, 'param_hash': '03189f0f4cb118a1ec142e488fe3fac12e97c118f60661796f8a5a64fa871d44'}\n",
    "# best_params['learning_rate'] = 0.0005\n",
    "best_params['learning_rate'] = 0.0001\n",
    "\n",
    "print(f'## Recalculating with best_params = {best_params}')\n",
    "best_params = tuner._recalculate_tunable_params(best_params)\n",
    "best_model = tuner.get_model_for_params(best_params)\n",
    "print(tuner.train_and_compute_loss(best_params, reload_model=True)[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_local_modules(verbose=False)\n",
    "\n",
    "generation_id = 20\n",
    "dataset_paths = experiment_runner.get_trajectory_paths(generation_id)\n",
    "print_dataset_stats(dataset_paths, n_max_context, action_vocab, model=best_model, game=game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 iter training\n",
    "transform_config_fields: {'n_max_context', 'n_layer', 'dropout', 'n_head', 'bias', 'n_embd'}\n",
    "train_config_fields: {'decay_lr', 'wandb_log', 'warmup_iters', 'dtype', 'compile', 'batch_size', 'model_version', 'lr_decay_iters', 'eval_iters', 'log_interval', 'model_name', 'device', 'gradient_accumulation_steps', 'weight_decay', 'eval_interval', 'grad_clip', 'always_save_checkpoint', 'beta1', 'min_lr', 'eval_only', 'max_epochs', 'max_iters', 'learning_rate', 'beta2'}\n",
    "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
    "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
    "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
    "Dataset Stats:\n",
    "  Trajectories: 173000\n",
    "  Total actions: 2754358\n",
    "  Avg trajectory length: 15.92\n",
    "Prefix Stats:\n",
    "actions=(): 173000 win=104006 loss=68957 draw=37 win1%=60.12 model-win1%=59.44\n",
    "actions=(1,): 46966 win=26530 loss=20423 draw=13 win1%=56.49 model-win1%=55.45\n",
    "actions=(1, 1): 13319 win=7577 loss=5739 draw=3 win1%=56.89 model-win1%=58.33\n",
    "actions=(2,): 8666 win=4963 loss=3698 draw=5 win1%=57.27 model-win1%=58.16\n",
    "actions=(3,): 16091 win=10062 loss=6024 draw=5 win1%=62.53 model-win1%=61.10\n",
    "actions=(4,): 19604 win=14349 loss=5254 draw=1 win1%=73.19 model-win1%=72.11\n",
    "actions=(5,): 18147 win=11351 loss=6793 draw=3 win1%=62.55 model-win1%=62.14\n",
    "actions=(6,): 17995 win=10412 loss=7581 draw=2 win1%=57.86 model-win1%=57.84\n",
    "actions=(7,): 45531 win=26339 loss=19184 draw=8 win1%=57.85 model-win1%=57.81\n",
    "actions=(7, 1): 11328 win=7096 loss=4230 draw=2 win1%=62.64 model-win1%=61.39\n",
    "actions=(7, 7): 8785 win=5147 loss=3637 draw=1 win1%=58.59 model-win1%=60.27\n",
    "\n",
    "# 3000 iter training\n",
    "transform_config_fields: {'n_max_context', 'n_layer', 'dropout', 'n_head', 'bias', 'n_embd'}\n",
    "train_config_fields: {'decay_lr', 'wandb_log', 'warmup_iters', 'dtype', 'compile', 'batch_size', 'model_version', 'lr_decay_iters', 'eval_iters', 'log_interval', 'model_name', 'device', 'gradient_accumulation_steps', 'weight_decay', 'eval_interval', 'grad_clip', 'always_save_checkpoint', 'beta1', 'min_lr', 'eval_only', 'max_epochs', 'max_iters', 'learning_rate', 'beta2'}\n",
    "Using forked data for gen 1 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-1\n",
    "Using forked data for gen 2 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-2\n",
    "Using forked data for gen 3 from /Users/rodo/src/rgi3-sync/experiments/smoketest-e2e/data/gen-3\n",
    "Dataset Stats:\n",
    "  Trajectories: 173000\n",
    "  Total actions: 2754358\n",
    "  Avg trajectory length: 15.92\n",
    "Prefix Stats:\n",
    "actions=(): 173000 win=104006 loss=68957 draw=37 win1%=60.12 model-win1%=59.36\n",
    "actions=(1,): 46966 win=26530 loss=20423 draw=13 win1%=56.49 model-win1%=55.66\n",
    "actions=(1, 1): 13319 win=7577 loss=5739 draw=3 win1%=56.89 model-win1%=58.03\n",
    "actions=(2,): 8666 win=4963 loss=3698 draw=5 win1%=57.27 model-win1%=58.40\n",
    "actions=(3,): 16091 win=10062 loss=6024 draw=5 win1%=62.53 model-win1%=61.81\n",
    "actions=(4,): 19604 win=14349 loss=5254 draw=1 win1%=73.19 model-win1%=73.78\n",
    "actions=(5,): 18147 win=11351 loss=6793 draw=3 win1%=62.55 model-win1%=61.63\n",
    "actions=(6,): 17995 win=10412 loss=7581 draw=2 win1%=57.86 model-win1%=58.78\n",
    "actions=(7,): 45531 win=26339 loss=19184 draw=8 win1%=57.85 model-win1%=57.19\n",
    "actions=(7, 1): 11328 win=7096 loss=4230 draw=2 win1%=62.64 model-win1%=63.81\n",
    "actions=(7, 7): 8785 win=5147 loss=3637 draw=1 win1%=58.59 model-win1%=59.49"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
